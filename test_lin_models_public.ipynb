{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up_YEB0Qfrn6"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXEEvSr7cv_3",
        "outputId": "e4f54c73-ce10-4351-eedb-5e42644974cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CudaDevice(id=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Basic Imports\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except ModuleNotFoundError:\n",
        "    %pip install --quiet tqdm\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    from tokenizers import Tokenizer\n",
        "    from tokenizers.models import BPE\n",
        "    from tokenizers.trainers import BpeTrainer\n",
        "    from tokenizers.pre_tokenizers import Whitespace\n",
        "except ModuleNotFoundError:\n",
        "    %pip install --quiet datasets\n",
        "    %pip install --quiet tokenizers\n",
        "    from datasets import load_dataset\n",
        "    from tokenizers import Tokenizer\n",
        "    from tokenizers.models import BPE\n",
        "    from tokenizers.trainers import BpeTrainer\n",
        "    from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "try:\n",
        "    import flax\n",
        "    from flax import linen as nn\n",
        "    from flax.training import train_state\n",
        "    from flax.core import freeze, unfreeze\n",
        "    from flax.linen.initializers import zeros, normal\n",
        "except ModuleNotFoundError:\n",
        "    %pip install --quiet flax\n",
        "    import flax\n",
        "    from flax import linen as nn\n",
        "    from flax.training import train_state\n",
        "    from flax.core import freeze, unfreeze\n",
        "    from flax.linen.initializers import zeros, normal\n",
        "\n",
        "try:\n",
        "    import optax\n",
        "except ModuleNotFoundError:\n",
        "    %pip install --quiet optax\n",
        "    import optax\n",
        "\n",
        "try:\n",
        "    from pythomata import SimpleDFA\n",
        "except ModuleNotFoundError:\n",
        "    %pip install --quiet pythomata\n",
        "    from pythomata import SimpleDFA\n",
        "try:\n",
        "    from ml_collections import config_dict\n",
        "except ModuleNotFoundError:\n",
        "    %pip install --quiet ml_collections\n",
        "    from ml_collections import config_dict\n",
        "\n",
        "\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "import abc\n",
        "import re\n",
        "from functools import partial\n",
        "import jax\n",
        "import torch\n",
        "from collections import Counter\n",
        "from jax import  random, numpy as jnp, lax, vmap, jit, grad, value_and_grad, lib\n",
        "from jax import random, vmap, pmap, custom_vjp, config as cfg, nn as jax_nn\n",
        "from typing import List, Optional, Callable, Tuple, Dict, Any, Set, overload\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "from typing import Dict, Tuple, Any, Callable, NamedTuple, Optional, Literal\n",
        "import chex\n",
        "\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRVlLYdafvPj"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "obHOLIucfxA-"
      },
      "outputs": [],
      "source": [
        "def get_experiment_config(seeds:List[int]) -> config_dict.ConfigDict:\n",
        "    '''Returns the configuration for the model and the experiment'''\n",
        "    experiment_config = config_dict.ConfigDict()\n",
        "    experiment_config.seeds = seeds\n",
        "    experiment_config.data = data_configurator() # Data Configurations\n",
        "    experiment_config.optim = optim_configurator() # Optimizer Configurations\n",
        "    experiment_config.experiment = experiment_configurator() # Specific Experiment Configurations\n",
        "    return experiment_config\n",
        "\n",
        "def get_model_config(use_depth: int, use_gla: bool) -> config_dict.ConfigDict:\n",
        "    '''Returns the model configuration for the given model description'''\n",
        "\n",
        "    # you can easily either modify this config or just add new model_config functions\n",
        "\n",
        "    def model_config(use_depth: int, use_gla: bool):\n",
        "        '''Fully Observed Data Constructed Transformer (Interpolation)'''\n",
        "        model_config = config_dict.ConfigDict()\n",
        "        model_config.use_depth = use_depth\n",
        "        model_config.is_discrete = True\n",
        "        model_config.use_gla = False\n",
        "        model_config.vocab_size = 20\n",
        "        model_config.use_fwp=True       # use fast weight programmer implementation\n",
        "        model_config.range_dfwp=1  # (DEPRECATED) How far to propagate kv pairs within model\n",
        "        model_config.use_emb=True       # use linear embedding layer (default: False when training on constructed tokens)\n",
        "        model_config.use_pe_emb=False    # concatenate PE to embeddings (default: False when training on constructed tokens)\n",
        "        model_config.use_pe_kq=False    # concatenate PE to K and Q in attention (default: False when training on constructed tokens)\n",
        "        model_config.hybrid_first_block=False   # This adds an additional first layer to the model with possibly different settings. Also: If you want to use pe-kq, you need to set this to True\n",
        "        model_config.pe_dim=30          # positional encoding dimension (concatenated, if you want to change that, modify src/models/positional_encodings.py)\n",
        "        model_config.out_dim=20         # output dimension\n",
        "        model_config.initializer=jax_nn.initializers.normal(stddev=0.02)  # initializer for weights, in our experiments it wasn't necessary to adapt for deeper layers but instead use this as default\n",
        "        model_config.use_layernorm=True    # use layernorm in transformer layers\n",
        "        model_config.use_bias=False     # use bias in transformer layers (default = False)\n",
        "        model_config.use_mlp=True      # use mlp in transformer layers\n",
        "        model_config.masked=True        # causal masking in self-attention\n",
        "        model_config.use_clip=False      # forward activation clipping\n",
        "        model_config.clip_range=3       # clipping value [-clip_val, clip_val]\n",
        "        model_config.num_layers=3       # number of transformer layers (without the optional first hybrid layer)\n",
        "        model_config.num_heads=4        # number of heads in self-attention\n",
        "        model_config.embed_dim=64       # embedding dimension\n",
        "        model_config.key_size=20        # key size in self-attention\n",
        "        model_config.seq_len=1024         # sequence length of input sequences (unused field in TF, can be used for debugging)\n",
        "        model_config.dim_feedforward_MLP=256    # dimension of hidden layer in MLP\n",
        "        model_config.linear=True        # use linear self-attention\n",
        "        model_config.use_schlagnorm=True   # use schlagnorm in transformer layers (normalize K,Q)\n",
        "        model_config.schlagnorm_targets=True   # schlagnorm also for Values\n",
        "        return model_config\n",
        "\n",
        "    return model_config(use_depth=use_depth, use_gla=use_gla)\n",
        "\n",
        "\n",
        "def experiment_configurator() -> config_dict.ConfigDict:\n",
        "    '''Returns the experiment configuration for the given experiment description'''\n",
        "    experiment = config_dict.ConfigDict()\n",
        "    return experiment\n",
        "\n",
        "\n",
        "def optim_configurator() -> config_dict.ConfigDict:\n",
        "    '''Returns the data configuration for the experiment'''                                                                                                                                                             # For nonlin-tf use 1e-3, for mesa 4e-4\n",
        "    optim = config_dict.ConfigDict()\n",
        "    optim.peak_lr = 1e-4   # Peak learning rate (or fixed if no scheduling)\n",
        "    optim.grad_clip = 1     # Gradient clipping value\n",
        "    optim.use_schedule = True   # Use learning rate scheduling\n",
        "    optim.warmup_steps = 1000   # Warmup steps for learning rate scheduling\n",
        "    optim.max_iters = 20000    # Maximum number of iterations for scheduling\n",
        "    optim.init_value = 0    # Initial learning rate\n",
        "    optim.end_value = 3e-5  # Final learning rate (at max_iters train steps)\n",
        "    optim.weight_decay = 0.05   # Weight decay\n",
        "    return optim\n",
        "\n",
        "def data_configurator() -> config_dict.ConfigDict:\n",
        "    '''Returns the data configuration for the experiment'''\n",
        "    data = config_dict.ConfigDict()\n",
        "\n",
        "    data.token_format = 'compact'\n",
        "    data.batch_size = 32   # Training batch size\n",
        "    data.test_batch_size = 32  # Test batch size\n",
        "    data.seq_len = 1024   # Sequence length\n",
        "    data.data_dim = 10  # Hidden Data dimension\n",
        "    data.vocab_size = 20   # Vocabulary size\n",
        "    data.obs_dim = 20   # Observed data dimension\n",
        "    data.noise_obs = 0  # Observation noise level, Only relevant for partially obserable sequences\n",
        "    data.noise = 0.01   # Noise level\n",
        "    data.eye_obs = True # Use identity matrix as observation matrix\n",
        "    data.data_clip = 10 # Clip data to [-data_clip, data_clip] (relevant for contracting sequences)\n",
        "    data.range = 1    # Range of initial values (U-distr.)\n",
        "    data.construction = True    # Use constructed tokens\n",
        "    data.ttype = 'regbench' # 'seq_lin_constr' for linear sequences with constructed tokens, 'seq_lin' else. 'regbench' for - well - regbench.\n",
        "    data.task_type = 'discrete' # Discrete if regbench/lang. else 'continous'\n",
        "    data.embed_dim = 256     # Embedding dimension (for constructed tokens)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHX5gZ9hfQm-"
      },
      "source": [
        "# Architecture and Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWcvh0HRgVki"
      },
      "source": [
        "## (Oldschool) Cosine-wave based Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5Ag3YUVcgZzB"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    '''Class implementing the Positional Encoding for Transformer models.\n",
        "\n",
        "    Fields:\n",
        "        'pe_dim' (int): Dimension of the positional encoding\n",
        "        'max_len' (int): Maximum length of input sequences\n",
        "    '''\n",
        "    pe_dim : int\n",
        "    max_len : int = 10\n",
        "    concat: bool = False\n",
        "\n",
        "    def concat_single(self, x: chex.Array, pe: chex.Array) -> chex.Array:\n",
        "        '''Currently unused. Concatenates positional encoding to a single input tensor.'''\n",
        "        return jnp.concatenate([x, pe], axis=-1)\n",
        "\n",
        "    def concat_batch(self, x: chex.Array, pe: chex.Array) -> chex.Array:\n",
        "        '''Currently unused. Concatenates positional encoding to a batch of input tensors.'''\n",
        "        myfunConcat = partial(self.concat_single, pe=pe)\n",
        "        return vmap(myfunConcat)(x)\n",
        "\n",
        "    def setup(self):\n",
        "        '''Initializes the positional encoding.'''\n",
        "        enc_size = self.pe_dim\n",
        "        pe = np.zeros((self.max_len, enc_size))\n",
        "        position = np.arange(0, self.max_len, dtype=np.float32)[:,None]\n",
        "        div_term = np.exp(np.arange(0, enc_size, 2) * (-math.log(10000.0) / enc_size))\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        if enc_size % 2 == 1:\n",
        "            pe[:, 1::2] = np.cos(position * div_term)[:,:-1]\n",
        "        else:\n",
        "            pe[:, 1::2] = np.cos(position * div_term)\n",
        "        self.pe = jax.device_put(pe)\n",
        "\n",
        "    def __call__(self, x: chex.Array) -> chex.Array:\n",
        "        '''Adds positional encoding to the input tensor.'''\n",
        "        if self.concat:\n",
        "            x = self.concat_batch(x, self.pe)\n",
        "        else:\n",
        "            x = x + self.pe\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5pTae8PN-YQ"
      },
      "source": [
        "## Conv1d Layer as in [RecurrentGemma](https://github.com/google-deepmind/recurrentgemma/blob/main/recurrentgemma/jax/layers.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bkWuLXhROUOF"
      },
      "outputs": [],
      "source": [
        "class Conv1D(nn.Module):\n",
        "    \"\"\"1D temporal convolution layer with caching support for autoregressive use.\n",
        "\n",
        "    Attributes:\n",
        "        width: Number of input/output channels\n",
        "        temporal_width: Size of temporal receptive field\n",
        "        w_init_variance_scale: Scale factor for weight initialization variance\n",
        "    \"\"\"\n",
        "    width: int\n",
        "    temporal_width: int\n",
        "    w_init_variance_scale: float = 0.01\n",
        "\n",
        "    def setup(self):\n",
        "        self.w = self.param(\n",
        "            \"w\",\n",
        "            nn.initializers.variance_scaling(\n",
        "                scale=self.w_init_variance_scale,\n",
        "                mode=\"fan_in\",\n",
        "                distribution=\"normal\",\n",
        "            ),\n",
        "            (self.temporal_width, self.width)\n",
        "        )\n",
        "        self.b = self.param(\"b\", nn.initializers.zeros_init(), (self.width,))\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x: chex.Array,\n",
        "        segment_pos: chex.Array,\n",
        "        cache: Optional[chex.Array] = None,\n",
        "        return_cache: bool = False,\n",
        "    ) -> Tuple[chex.Array, Optional[chex.Array]]:\n",
        "        \"\"\"Apply temporal convolution with optional caching for autoregressive generation.\n",
        "\n",
        "        Args:\n",
        "            x: Input of shape [batch_size, seq_len, width]\n",
        "            segment_pos: Position indicators of shape [batch_size, seq_len]\n",
        "                       (0 indicates start of new segment)\n",
        "            cache: Optional cache of previous states for autoregressive generation\n",
        "                  shape: [batch_size, temporal_width-1, width]\n",
        "            return_cache: Whether to return the updated cache\n",
        "\n",
        "        Returns:\n",
        "            output: Convolved sequence of shape [batch_size, seq_len, width]\n",
        "            new_cache: Updated cache if return_cache=True, else None\n",
        "        \"\"\"\n",
        "        output_len = x.shape[1]\n",
        "\n",
        "        if cache is not None:\n",
        "            # Autoregressive generation mode - use cached states\n",
        "            x = self._concat_cache(x, cache)\n",
        "            prompt_len = self.temporal_width - 1\n",
        "            state_dtype = cache.dtype\n",
        "        else:\n",
        "            # Training mode - process full sequence\n",
        "            prompt_len = 0\n",
        "            state_dtype = x.dtype\n",
        "\n",
        "        # Perform convolution with causal masking\n",
        "        conv_out = 0.0\n",
        "        temporal_width = min(self.temporal_width, prompt_len + output_len)\n",
        "\n",
        "        for t in range(temporal_width):\n",
        "            start_idx, end_idx = self._get_window_indices(\n",
        "                prompt_len=prompt_len,\n",
        "                shift_back=t,\n",
        "                output_len=output_len,\n",
        "            )\n",
        "            x_window = x[:, start_idx:end_idx]\n",
        "\n",
        "            if cache is None:\n",
        "                # Apply segment masking in training mode\n",
        "                window_mask = self._get_segment_mask(\n",
        "                    segment_pos=segment_pos,\n",
        "                    start_idx=start_idx,\n",
        "                    end_idx=end_idx,\n",
        "                    max_look_ahead=t,\n",
        "                )\n",
        "                x_window *= window_mask[:, :, None]\n",
        "\n",
        "            # Pad if needed and apply convolution weights\n",
        "            x_window = self._pad_to_length(x_window, output_len)\n",
        "            w_t = self.w[self.temporal_width - t - 1][None, None, :]\n",
        "            conv_out += x_window * w_t\n",
        "\n",
        "        conv_out += self.b[None, None]\n",
        "\n",
        "        if not return_cache:\n",
        "            return conv_out, None\n",
        "\n",
        "        # Update cache for next step\n",
        "        new_cache = x[:, 1 - self.temporal_width:].astype(state_dtype)\n",
        "        new_cache = self._pad_cache(new_cache)\n",
        "\n",
        "        return conv_out, new_cache\n",
        "\n",
        "    def _concat_cache(self, x: chex.Array, cache: chex.Array) -> chex.Array:\n",
        "        \"\"\"Concatenate current input with cache for autoregressive generation.\"\"\"\n",
        "        chex.assert_shape(cache, (x.shape[0], self.temporal_width - 1, self.width))\n",
        "        chex.assert_shape(x, (None, 1, self.width))\n",
        "        return jnp.concatenate([cache.astype(x.dtype), x], axis=1)\n",
        "\n",
        "    def _get_window_indices(\n",
        "        self, prompt_len: int, shift_back: int, output_len: int\n",
        "    ) -> Tuple[int, int]:\n",
        "        \"\"\"Get start and end indices for convolution window.\"\"\"\n",
        "        start_idx = max(prompt_len - shift_back, 0)\n",
        "        end_idx = prompt_len + output_len - shift_back\n",
        "        return start_idx, end_idx\n",
        "\n",
        "    def _get_segment_mask(\n",
        "        self, segment_pos: chex.Array, start_idx: int, end_idx: int, max_look_ahead: int\n",
        "    ) -> chex.Array:\n",
        "        \"\"\"Create mask to prevent information flow between segments.\"\"\"\n",
        "        batch_size = segment_pos.shape[0]\n",
        "        not_boundary = (segment_pos != 0).astype(jnp.int32)\n",
        "        mask = jnp.ones((batch_size, end_idx - start_idx))\n",
        "\n",
        "        for shift in range(1, max_look_ahead + 1):\n",
        "            mask *= not_boundary[:, start_idx + shift:end_idx + shift]\n",
        "        return mask\n",
        "\n",
        "    def _pad_to_length(self, x: chex.Array, target_len: int) -> chex.Array:\n",
        "        \"\"\"Left-pad input to target length if needed.\"\"\"\n",
        "        pad_len = target_len - x.shape[1]\n",
        "        if pad_len <= 0:\n",
        "            return x\n",
        "        padding = jnp.zeros((x.shape[0], pad_len, x.shape[2]), dtype=x.dtype)\n",
        "        return jnp.concatenate([padding, x], axis=1)\n",
        "\n",
        "    def _pad_cache(self, state: chex.Array) -> chex.Array:\n",
        "        \"\"\"Left-pad cache to temporal width if needed.\"\"\"\n",
        "        pad_len = self.temporal_width - state.shape[1] - 1\n",
        "        if pad_len <= 0:\n",
        "            return state\n",
        "        padding = jnp.zeros((state.shape[0], pad_len, state.shape[2]), dtype=state.dtype)\n",
        "        return jnp.concatenate([padding, state], axis=1)\n",
        "\n",
        "    @classmethod\n",
        "    def init_cache(\n",
        "        cls, batch_size: int, width: int, dtype: jnp.dtype, temporal_width: int = 4\n",
        "    ) -> chex.Array:\n",
        "        \"\"\"Initialize an empty cache for autoregressive generation.\"\"\"\n",
        "        return jnp.zeros((batch_size, temporal_width - 1, width), dtype=dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SveqfQdAgX_L"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tpumObKbfT-e"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    '''Multi-Head Attention Layer.\n",
        "\n",
        "    Fields:\n",
        "        'masked' (bool): Flag whether to use masked attention\n",
        "        'embed_dim' (int): Embedding dimension\n",
        "        'num_heads' (int): Number of heads\n",
        "        'use_softmax' (bool): Flag whether to use softmax\n",
        "        'use_bias' (bool): Flag whether to use bias\n",
        "        'key_size' (int): Key size\n",
        "        'initializer' Any: Initializer function\n",
        "        'seq_len' (int): Sequence length\n",
        "        'use_pe_kq' (bool): Flag whether to use (special) positional encoding in the query and key vectors\n",
        "    '''\n",
        "    masked : bool\n",
        "    embed_dim : int\n",
        "    num_heads : int\n",
        "    use_softmax : bool\n",
        "    use_bias : bool\n",
        "    key_size : int\n",
        "    initializer : Any\n",
        "    seq_len : int\n",
        "    layer_idx: int\n",
        "    use_pe_kq : bool = False\n",
        "    use_schlagnorm : bool = False\n",
        "    schlagnorm_targets : bool = False\n",
        "\n",
        "    def scaled_dot_product_attention(\n",
        "        self,\n",
        "        q: chex.Array,\n",
        "        k: chex.Array,\n",
        "        v: chex.Array,\n",
        "        use_softmax: bool,\n",
        "        masked=True\n",
        "    ) -> chex.Array:\n",
        "        '''Scaled Dot-Product Attention.\n",
        "\n",
        "        Args:\n",
        "            'q' (chex.Array): Query projections [batch, heads, seq_len, key_size]\n",
        "            'k' (chex.Array): Key projections [batch, heads, seq_len, key_size]\n",
        "            'v' (chex.Array): Value projections [batch, heads, seq_len, key_size]\n",
        "            'use_softmax' (bool): Flag whether to use softmax-attention\n",
        "            'masked' (bool): Flag whether to use causally masked attention\n",
        "        Returns:\n",
        "            Tuple[chex.Array]: Attention values and attention logits.\n",
        "        '''\n",
        "        d_k = q.shape[-1]\n",
        "\n",
        "        attn_logits = jnp.einsum('bhsk,bhtk->bhst', q, k)\n",
        "\n",
        "        if masked:\n",
        "            mask = jnp.tril(jnp.ones(shape=(attn_logits.shape[-2:])))[None, None, :, :]\n",
        "            attn_logits = jnp.where(mask == 0, 0, attn_logits)\n",
        "            if use_softmax:\n",
        "                attn_logits = jnp.where(mask == 0, -1e30, attn_logits/math.sqrt(d_k))\n",
        "\n",
        "        if use_softmax:\n",
        "            attn_logits = nn.softmax(attn_logits, axis=-1)\n",
        "\n",
        "        values = jnp.einsum('bhst,bhtd->bhsd', attn_logits, v)\n",
        "\n",
        "        return values, attn_logits\n",
        "\n",
        "    def setup(self):\n",
        "        '''Initializes the Multi-Head Attention Layer with the specified parameters.'''\n",
        "        def _create_proj(\n",
        "            name: str,\n",
        "            num_heads: int,\n",
        "            dim: int,\n",
        "        ) -> nn.Module:\n",
        "            '''Auxiliary function to create projection layers.'''\n",
        "            return nn.DenseGeneral(\n",
        "                    features=(\n",
        "                        (num_heads, dim) if num_heads > 0 else dim\n",
        "                    ),\n",
        "                    axis=(\n",
        "                        -1 if num_heads > 0 else (-1, -2)\n",
        "                    ),\n",
        "                    use_bias=self.use_bias,\n",
        "                    kernel_init=self.initializer['attention'](self.layer_idx)[name],\n",
        "            )\n",
        "\n",
        "        proj_dim = self.num_heads, self.key_size\n",
        "        proj_specs = {\n",
        "            'queries': proj_dim,\n",
        "            'keys': proj_dim,\n",
        "            'values': proj_dim,\n",
        "            'outs': (0, self.embed_dim),\n",
        "        }\n",
        "\n",
        "        for proj_name, output_dim in proj_specs.items():\n",
        "            setattr(self, proj_name, _create_proj(proj_name, *output_dim))\n",
        "\n",
        "        self.key_conv = Conv1D(\n",
        "            width=self.key_size,\n",
        "            temporal_width=4, #TODO: Make field\n",
        "            w_init_variance_scale=0.01, #TODO: Make field\n",
        "            name='conv'\n",
        "        )\n",
        "\n",
        "        if self.use_pe_kq:\n",
        "            self.pos_enc_kq = PositionalEncoding(pe_dim=self.embed_dim, max_len=self.seq_len, concat=True)\n",
        "\n",
        "    def __call__(self, x: chex.Array, outs_prev: chex.Array) -> chex.Array:\n",
        "        '''Applies the Multi-Head Attention Layer to the input tensor.'''\n",
        "        bs, sl, _ = x.shape\n",
        "\n",
        "        if self.use_pe_kq:\n",
        "            t = self.pos_enc_kq(x)\n",
        "            k = self.keys(t)\n",
        "            q = self.queries(t)\n",
        "        else:\n",
        "            k = self.keys(x)\n",
        "            q = self.queries(x)\n",
        "        v = self.values(x)\n",
        "\n",
        "        batch_size, seq_len = k.shape[0], k.shape[1]\n",
        "        keys_reshaped = k.transpose(0, 2, 1, 3)  # [batch, heads, seq, key_size]\n",
        "        keys_flat = keys_reshaped.reshape(-1, seq_len, self.key_size)  # [batch*heads, seq, key_size]\n",
        "        segment_pos = jnp.zeros((keys_flat.shape[0], seq_len))\n",
        "        keys_conv, _ = self.key_conv(keys_flat, segment_pos)\n",
        "        keys = keys_conv.reshape(batch_size, self.num_heads, seq_len, self.key_size)\n",
        "        k = keys.transpose(0, 2, 1, 3)  # [batch, seq, heads, key_size]\n",
        "\n",
        "        q,k,v = [jnp.einsum('bshd->bhsd', _elem) for _elem in [q,k,v]]\n",
        "\n",
        "        if self.use_schlagnorm:\n",
        "            q = q / (1e-16 + jnp.linalg.norm(q, axis=-1)[..., None])\n",
        "            k = k / (1e-16 + jnp.linalg.norm(k, axis=-1)[..., None])\n",
        "            if self.schlagnorm_targets:\n",
        "                v = v / (1e-16 + jnp.linalg.norm(v, axis=-1)[..., None])\n",
        "\n",
        "        values, attn_map = self.scaled_dot_product_attention(\n",
        "            q=q, k=k, v=v, masked=self.masked, use_softmax=self.use_softmax\n",
        "        )\n",
        "\n",
        "        values = jnp.einsum('bhsd->bshd', values)\n",
        "        o = self.outs(values)\n",
        "\n",
        "        k, v = [x.swapaxes(-2, -3) for x in [k, v]]\n",
        "        return o, attn_map, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mesa-Layer"
      ],
      "metadata": {
        "id": "KN_-ZmNlIIZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepDeltaAttention(nn.Module):\n",
        "    '''Linear Deep-Delta Fast Weight Programmer Attention Layer with basic sum update rule.\n",
        "\n",
        "    Fields:\n",
        "        'use_depth' (bool) Deep or standard Delta Rule\n",
        "        'masked' (bool): Flag whether to use masked attention\n",
        "        'embed_dim' (int): Embedding dimension\n",
        "        'num_heads' (int): Number of heads\n",
        "        'use_softmax' (bool): Flag whether to use softmax (ignored in fast weights)\n",
        "        'use_bias' (bool): Flag whether to use bias\n",
        "        'key_size' (int): Key size\n",
        "        'initializer' (Callable): Initializer function\n",
        "        'seq_len' (int): Sequence length\n",
        "        'num_layers' (int) num of total model layers\n",
        "        'use_pe_kq' (bool): Flag whether to use (special) positional encoding\n",
        "        'use_schlagnorm' (bool): Flag for schlag normalization\n",
        "        'schlagnorm_targets' (bool): Flag for schlag normalization targets\n",
        "    '''\n",
        "    use_depth: bool\n",
        "    masked: bool\n",
        "    embed_dim: int\n",
        "    num_heads: int\n",
        "    use_softmax: bool # Unused\n",
        "    use_bias: bool\n",
        "    key_size: int\n",
        "    initializer: Callable\n",
        "    seq_len: int\n",
        "    range_dfwp: int # Unused\n",
        "    layer_idx: int # Unused\n",
        "    num_layers: int # Unused\n",
        "    use_pe_kq: bool = False # Unused\n",
        "    use_schlagnorm: bool = False\n",
        "    schlagnorm_targets: bool = False\n",
        "\n",
        "    def _get_projections(self, inputs: chex.Array, name: str, dim: int) -> chex.Array:\n",
        "        '''Returns the projections of the input tensor.'''\n",
        "        return nn.DenseGeneral(\n",
        "            features=(self.num_heads, dim),\n",
        "            axis=-1,\n",
        "            use_bias=self.use_bias,\n",
        "            kernel_init=self.initializer['attention'](self.layer_idx)[name],\n",
        "            name=name\n",
        "        )(inputs)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, outs_prev, mask=None, deterministic=None):\n",
        "        \"\"\"Applies fast weight linear attention with basic sum update rule.\n",
        "\n",
        "        Args:\n",
        "            x: Query input of shape [batch..., seq_length, embed_dim]\n",
        "            outs_prev: Prev-FW-Outputs-list of shape [batch, n_heads, seq_length, embed_dim]\n",
        "            mask: Unused (since exists by construction) attention mask\n",
        "            deterministic: unused (kept for compatibility)\n",
        "        \"\"\"\n",
        "\n",
        "        queries, keys, values = [\n",
        "            self._get_projections(\n",
        "                inputs=x,\n",
        "                name=n,\n",
        "                dim=self.key_size,\n",
        "            ) for n in ['queries', 'keys', 'values']\n",
        "        ]\n",
        "\n",
        "        key_conv = Conv1D(\n",
        "            width=self.key_size,\n",
        "            temporal_width=4, #TODO: Make field\n",
        "            w_init_variance_scale=0.01, #TODO: Make field\n",
        "            name='key_conv'\n",
        "        )\n",
        "\n",
        "        batch_size, seq_len_c = keys.shape[0], keys.shape[1]\n",
        "        keys_reshaped = keys.transpose(0, 2, 1, 3)  # [batch, heads, seq, key_size]\n",
        "        keys_flat = keys_reshaped.reshape(-1, seq_len_c, self.key_size)  # [batch*heads, seq, key_size]\n",
        "        segment_pos = jnp.zeros((keys_flat.shape[0], seq_len_c))\n",
        "        keys_conv, _ = key_conv(keys_flat, segment_pos)\n",
        "        keys = keys_conv.reshape(batch_size, self.num_heads, seq_len_c, self.key_size)\n",
        "        keys = keys.transpose(0, 2, 1, 3)  # [batch, seq, heads, key_size]\n",
        "\n",
        "        gammas = jax.nn.sigmoid(self._get_projections(\n",
        "                inputs=x,\n",
        "                name='gammas',\n",
        "                dim=1,\n",
        "        ))\n",
        "\n",
        "        if self.use_schlagnorm:\n",
        "            queries = queries / (1e-16 + jnp.linalg.norm(queries, axis=-1, keepdims=True))\n",
        "            keys = keys / (1e-16 + jnp.linalg.norm(keys, axis=-1, keepdims=True))\n",
        "            if self.schlagnorm_targets:\n",
        "                values = values / (1e-16 + jnp.linalg.norm(values, axis=-1,  keepdims=True))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        output = -1\n",
        "\n",
        "        return output, None, None, None"
      ],
      "metadata": {
        "id": "1_NfuDYYIRgM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM4tRUmnQ-aP"
      },
      "source": [
        "## Gated Linear Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qlox3zPhQ8zA"
      },
      "outputs": [],
      "source": [
        "class GatedLinearAttention(nn.Module):\n",
        "    '''Linear Fast Weight Programmer Attention Layer with basic sum update rule.\n",
        "\n",
        "    Fields:\n",
        "        'masked' (bool): Flag whether to use masked attention\n",
        "        'embed_dim' (int): Embedding dimension\n",
        "        'num_heads' (int): Number of heads\n",
        "        'use_softmax' (bool): Flag whether to use softmax (ignored in fast weights)\n",
        "        'use_bias' (bool): Flag whether to use bias\n",
        "        'key_size' (int): Key size\n",
        "        'initializer' (Callable): Initializer function\n",
        "        'seq_len' (int): Sequence length\n",
        "        'use_pe_kq' (bool): Flag whether to use (special) positional encoding\n",
        "        'use_schlagnorm' (bool): Flag for schlag normalization\n",
        "        'schlagnorm_targets' (bool): Flag for schlag normalization targets\n",
        "    '''\n",
        "    masked: bool\n",
        "    embed_dim: int\n",
        "    num_heads: int\n",
        "    use_softmax: bool  # Will be ignored as we use ELU+1\n",
        "    use_bias: bool\n",
        "    key_size: int\n",
        "    initializer: Callable\n",
        "    seq_len: int\n",
        "    range_dfwp: int\n",
        "    layer_idx: int\n",
        "    num_layers: int\n",
        "    use_pe_kq: bool = False\n",
        "    use_schlagnorm: bool = False\n",
        "    schlagnorm_targets: bool = False\n",
        "\n",
        "    def _get_projections(self, inputs: chex.Array, name: str) -> chex.Array:\n",
        "        '''Returns the projections of the input tensor.'''\n",
        "        return nn.DenseGeneral(\n",
        "            features=(self.num_heads, self.key_size),\n",
        "            axis=-1,\n",
        "            use_bias=self.use_bias,\n",
        "            kernel_init=self.initializer['attention'](self.layer_idx)[name],\n",
        "            name=name\n",
        "        )(inputs)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, outs_prev=None, mask=None, deterministic=None):\n",
        "        \"\"\"Applies fast weight linear attention with basic sum update rule.\n",
        "\n",
        "        Args:\n",
        "            x: Query input of shape [batch..., seq_length, embed_dim]\n",
        "            outs_prev: Unused head-output-list input of shape [batch..., seq_length, embed_dim]\n",
        "            mask: Unused (since exists by construction) attention mask\n",
        "            deterministic: unused (kept for compatibility)\n",
        "        \"\"\"\n",
        "\n",
        "        query, key, value = [\n",
        "            self._get_projections(\n",
        "                inputs=ip,\n",
        "                name=n,\n",
        "            ) for (ip, n) in zip(3*[x],['query', 'key', 'value'])\n",
        "        ]\n",
        "\n",
        "        if self.use_schlagnorm:\n",
        "            query = query / (1e-16 + jnp.linalg.norm(query, axis=-1)[..., None])\n",
        "            key = key / (1e-16 + jnp.linalg.norm(key, axis=-1)[..., None])\n",
        "            if self.schlagnorm_targets:\n",
        "                value = value / (1e-16 + jnp.linalg.norm(value, axis=-1)[..., None])\n",
        "\n",
        "        key_conv = Conv1D(\n",
        "            width=self.key_size,\n",
        "            temporal_width=4, #TODO: Make field\n",
        "            w_init_variance_scale=0.01, #TODO: Make field\n",
        "            name='key_conv'\n",
        "        )\n",
        "\n",
        "\n",
        "        batch_size, seq_len_c = keys.shape[0], keys.shape[1]\n",
        "        keys_reshaped = keys.transpose(0, 2, 1, 3)  # [batch, heads, seq, key_size]\n",
        "        keys_flat = keys_reshaped.reshape(-1, seq_len_c, self.key_size)  # [batch*heads, seq, key_size]\n",
        "        segment_pos = jnp.zeros((keys_flat.shape[0], seq_len_c))\n",
        "        keys_conv, _ = key_conv(keys_flat, segment_pos)\n",
        "        keys = keys_conv.reshape(batch_size, self.num_heads, seq_len_c, self.key_size)\n",
        "        keys = keys.transpose(0, 2, 1, 3)  # [batch, seq, heads, key_size]\n",
        "\n",
        "        betas = [\n",
        "            jax.nn.sigmoid(self._get_projections(\n",
        "                inputs=x,\n",
        "                name=n,\n",
        "                dim=1,\n",
        "            )) for n in ['betas']\n",
        "        ]\n",
        "\n",
        "\n",
        "\n",
        "        def single_head_update(\n",
        "            keys_h: chex.Array,\n",
        "            values_h: chex.Array,\n",
        "            queries_h: chex.Array,\n",
        "            betas_h: chex.Array,\n",
        "        ) -> chex.Array:\n",
        "            \"Implements Deep-FWP for one head.\"\n",
        "\n",
        "            chex.assert_rank([keys_h, values_h, queries_h, betas_h], 4*[3,])\n",
        "            b, v, k = values_h.shape[0], values_h.shape[-1], keys.shape[-1]\n",
        "\n",
        "            def step(carry, data):\n",
        "                key_t, value_t, b_t = data\n",
        "                cv_ck = jnp.einsum(\"bd,bvk->bvk\", b_t, jnp.einsum(\"bv,bk->bvk\", value_t, key_t))\n",
        "                gated_carry = jnp.einsum(\"bd,bvk->bvk\", b_t, carry)\n",
        "                return 2*(b_t*gated_carry + cv_ck,)\n",
        "\n",
        "            _, gla = jax.lax.scan(\n",
        "                step,\n",
        "                jnp.zeros(shape=(b, v, k)),\n",
        "                tuple(x.swapaxes(0,1) for x in (keys_h, values_h, betas_h))\n",
        "            )\n",
        "\n",
        "            gla = gla.swapaxes(0, 1)\n",
        "\n",
        "            return jnp.einsum(\"bsvk,bsk->bsv\", gla, queries_h)\n",
        "\n",
        "        all_head_preds = jax.vmap(single_head_update, in_axes=(4*(-2,)), out_axes=(-2))(keys, values, queries, betas)\n",
        "\n",
        "        output = nn.DenseGeneral(\n",
        "            features=self.embed_dim,\n",
        "            axis=(-2, -1),\n",
        "            use_bias=self.use_bias,\n",
        "            kernel_init=self.initializer['attention'](self.layer_idx)['out'],\n",
        "            name='out'\n",
        "        )(all_head_preds)\n",
        "\n",
        "\n",
        "        return output, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE_bAe0yRuLJ"
      },
      "source": [
        "## RGLRU (Griffin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fISzsAx7R0fq"
      },
      "source": [
        "Directly stolen and adapted from [dtuneai](https://github.com/dtunai/Griffin-Jax/blob/main/griffin_jax/griffin_jax.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FlMrtnUmRtTK"
      },
      "outputs": [],
      "source": [
        "class RGLRU(nn.Module):\n",
        "    '''Regularized Gated Linear Recurrent Unit Layer with multi-head support.\n",
        "\n",
        "    Fields:\n",
        "        'dim' (int): Input dimension\n",
        "        'num_heads' (int): Number of attention heads\n",
        "        'mult' (int): Hidden dimension multiplier per head\n",
        "        'use_bias' (bool): Flag whether to use bias\n",
        "        'initializer' (Callable): Initializer function\n",
        "        'layer_idx' (int): Layer index in the network\n",
        "        'c' (int): Timestep constant for gating mechanism\n",
        "    '''\n",
        "    dim: int\n",
        "    num_heads: int\n",
        "    mult: int\n",
        "    use_bias: bool\n",
        "    initializer: Callable\n",
        "    layer_idx: int\n",
        "    c: int = 8\n",
        "\n",
        "    def _get_gate_params(self, name: str) -> Tuple[chex.Array, chex.Array]:\n",
        "        '''Returns the weight matrix and bias vector for a gate.'''\n",
        "        weight = self.param(\n",
        "            f\"W{name}\",\n",
        "            self.initializer['rglru'](self.layer_idx)[name],\n",
        "            (self.num_heads, self.dim * self.mult, self.dim)\n",
        "        )\n",
        "        bias = self.param(\n",
        "            f\"b{name}\",\n",
        "            nn.initializers.zeros,\n",
        "            (self.num_heads, self.dim * self.mult)\n",
        "        ) if self.use_bias else None\n",
        "        return weight, bias\n",
        "\n",
        "    def _get_lambda_param(self) -> chex.Array:\n",
        "        '''Returns the lambda parameter for decay rate calculation.'''\n",
        "        def custom_lambda_init(key, shape):\n",
        "            # Initialize lambda to give decay rates between 0.9 and 0.999\n",
        "            a_c_values = random.uniform(key, shape, minval=0.9, maxval=0.999)\n",
        "            return -jnp.log((1 / a_c_values) ** (1 / self.c) - 1)\n",
        "\n",
        "        return self.param(\n",
        "            \"Lambda\",\n",
        "            custom_lambda_init,\n",
        "            (self.num_heads, self.dim * self.mult)\n",
        "        )\n",
        "\n",
        "    def _single_head_rglru(\n",
        "        self,\n",
        "        x: chex.Array,\n",
        "        Wa: chex.Array,\n",
        "        Wx: chex.Array,\n",
        "        ba: Optional[chex.Array],\n",
        "        bx: Optional[chex.Array],\n",
        "        Lambda: chex.Array,\n",
        "    ) -> chex.Array:\n",
        "        \"\"\"Applies RGLRU transformation for a single head.\n",
        "\n",
        "        Args:\n",
        "            x: Input of shape [batch, seq_length, dim]\n",
        "            Wa, Wx: Weight matrices for the gates\n",
        "            ba, bx: Bias vectors for the gates (optional)\n",
        "            Lambda: Decay rate parameter\n",
        "        \"\"\"\n",
        "        batch_size, _, _ = x.shape\n",
        "        ht = jnp.zeros((batch_size, self.dim * self.mult))\n",
        "\n",
        "        def step(carry, inputs):\n",
        "            ht = carry\n",
        "            xt = inputs\n",
        "\n",
        "            # Calculate gates\n",
        "            rt = jax.nn.sigmoid(jnp.dot(xt, Wa) + ba) if self.use_bias else jax.nn.sigmoid(jnp.dot(xt, Wa))\n",
        "            it = jax.nn.sigmoid(jnp.dot(xt, Wx) + bx) if self.use_bias else jax.nn.sigmoid(jnp.dot(xt, Wx))\n",
        "\n",
        "            # Calculate decay rate\n",
        "            a_t = jnp.exp(-self.c * jax.nn.softplus(-Lambda) * rt)\n",
        "\n",
        "            # Update hidden state\n",
        "            ht_new = a_t * ht + ((1 - a_t**2) ** 0.5) * (it * xt)\n",
        "\n",
        "            return ht_new, ht_new\n",
        "\n",
        "        # Scan over sequence\n",
        "        _, outputs = jax.lax.scan(\n",
        "            step,\n",
        "            ht,\n",
        "            x.swapaxes(0, 1)  # [seq_len, batch, dim]\n",
        "        )\n",
        "\n",
        "        return outputs.swapaxes(0, 1)  # [batch, seq_len, dim]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic=None):\n",
        "        \"\"\"Applies multi-head RGLRU transformation.\n",
        "\n",
        "        Args:\n",
        "            x: Input of shape [batch..., seq_length, dim]\n",
        "            deterministic: unused (kept for compatibility)\n",
        "        \"\"\"\n",
        "        Wa, ba = self._get_gate_params('a')\n",
        "        Wx, bx = self._get_gate_params('x')\n",
        "        Lambda = self._get_lambda_param()\n",
        "\n",
        "        # Process each head independently\n",
        "        all_head_outputs = jax.vmap(\n",
        "            self._single_head_rglru,\n",
        "            in_axes=(None, 0, 0, 0 if self.use_bias else None, 0 if self.use_bias else None, 0),\n",
        "            out_axes=1\n",
        "        )(x, Wa, Wx, ba, bx, Lambda)\n",
        "\n",
        "        # Project multi-head output back to original dimension\n",
        "        output = nn.DenseGeneral(\n",
        "            features=self.dim,\n",
        "            axis=(-2, -1),\n",
        "            use_bias=self.use_bias,\n",
        "            kernel_init=self.initializer['rglru'](self.layer_idx)['out'],\n",
        "            name='out'\n",
        "        )(all_head_outputs)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW5rNfJswceQ"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPJCeqhswceQ"
      },
      "source": [
        "### Transformer-Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lE_7MkUdwceS"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    '''Single Transformer Block Can be used with or without MESA or MLP or LayerNorms.\n",
        "\n",
        "    Fields:\n",
        "        'usa_gla' (bool) Use gated linear attention\n",
        "        'use_depth' (bool): Deep or standard Delta Model\n",
        "        'data_dim' (int): Dimension of the (original) data tokens\n",
        "        'use_fwp' (bool): Flag to use fast weight programmer implementation\n",
        "        'range_dfwp' (int): range of deep fast weight programmer (how far propagate kv-pairs)\n",
        "        'embed_dim' (int): Dimension of the embeddings\n",
        "        'key_size' (int): Size of the key vectors\n",
        "        'num_heads' (int): Number of attention heads\n",
        "        'dim_feedforward' (int): Dimension of the feedforward network\n",
        "        'use_layer_norm' (bool): Whether to use LayerNorm\n",
        "        'use_bias' (bool): Flag to use bias in the attention layer\n",
        "        'use_softmax' (bool): Flag to use softmax in the attention layer\n",
        "        'use_mlp' (bool): Flag to use the MLP\n",
        "        'masked' (bool): Flag to use masking in the attention layer\n",
        "        'mask_inputs' (bool): Flag to mask the input data\n",
        "        'initializer' (Callable[[chex.PRNGKey, tuple, jnp.dtype], chex.Array]): Initializer for the weights\n",
        "        'use_pe_kq' (bool): Flag to use positional encoding in the query and key vectors\n",
        "        'seq_len' (int): Length of the input sequence\n",
        "        'use_schlagnorm' (bool): Flag to use Schlag-Norm\n",
        "        'schlagnorm_targets' (bool): Flag to use Schlag-Norm on the targets\n",
        "    '''\n",
        "\n",
        "    use_gla: bool\n",
        "    use_depth: bool\n",
        "    embed_dim: int\n",
        "    layer_idx: int\n",
        "    num_layers: int\n",
        "    use_fwp: bool\n",
        "    range_dfwp: int\n",
        "    use_pe_kq: bool\n",
        "    key_size : int\n",
        "    num_heads : int\n",
        "    dim_feedforward : int\n",
        "    use_layer_norm : bool\n",
        "    use_bias : bool\n",
        "    use_softmax : bool\n",
        "    use_mlp : bool\n",
        "    masked : bool\n",
        "    initializer : Any\n",
        "    seq_len: int\n",
        "    use_schlagnorm : bool\n",
        "    schlagnorm_targets : bool\n",
        "\n",
        "    def setup(self):\n",
        "        '''Initializes the Transformer Block.'''\n",
        "        print('Transformer Block:')\n",
        "\n",
        "        # Attention Layer\n",
        "        if self.use_fwp:\n",
        "          if self.use_gla:\n",
        "            print('Using GLA Programmer Attention')\n",
        "            self.self_attn = GatedLinearAttention(\n",
        "                num_heads=self.num_heads,\n",
        "                embed_dim=self.embed_dim,\n",
        "                masked=self.masked,\n",
        "                use_softmax=self.use_softmax,\n",
        "                use_bias=self.use_bias,\n",
        "                key_size=self.key_size,\n",
        "                initializer=self.initializer,\n",
        "                use_pe_kq=self.use_pe_kq,\n",
        "                seq_len=self.seq_len,\n",
        "                use_schlagnorm=self.use_schlagnorm,\n",
        "                schlagnorm_targets=self.schlagnorm_targets,\n",
        "                range_dfwp=self.range_dfwp,\n",
        "                layer_idx=self.layer_idx,\n",
        "                num_layers=self.num_layers,\n",
        "            )\n",
        "          else:\n",
        "            print('Using Delta Rule Attention')\n",
        "            print(f'Leveraging depth T/F: {self.use_depth}')\n",
        "            self.self_attn = DeepDeltaAttention(\n",
        "                use_depth=self.use_depth,\n",
        "                num_heads=self.num_heads,\n",
        "                embed_dim=self.embed_dim,\n",
        "                masked=self.masked,\n",
        "                use_softmax=self.use_softmax,\n",
        "                use_bias=self.use_bias,\n",
        "                key_size=self.key_size,\n",
        "                initializer=self.initializer,\n",
        "                use_pe_kq=self.use_pe_kq,\n",
        "                seq_len=self.seq_len,\n",
        "                use_schlagnorm=self.use_schlagnorm,\n",
        "                schlagnorm_targets=self.schlagnorm_targets,\n",
        "                range_dfwp=self.range_dfwp,\n",
        "                layer_idx=self.layer_idx,\n",
        "                num_layers=self.num_layers,\n",
        "            )\n",
        "        else:\n",
        "          print(f'Using standard-attention, using softmax: {self.use_softmax}')\n",
        "          self.self_attn = MultiHeadAttention(\n",
        "              num_heads=self.num_heads,\n",
        "              embed_dim=self.embed_dim,\n",
        "              masked=self.masked,\n",
        "              use_softmax=self.use_softmax,\n",
        "              use_bias=self.use_bias,\n",
        "              key_size=self.key_size,\n",
        "              initializer=self.initializer,\n",
        "              use_pe_kq=False,\n",
        "              seq_len=self.seq_len,\n",
        "              use_schlagnorm=self.use_schlagnorm,\n",
        "              schlagnorm_targets=self.schlagnorm_targets,\n",
        "              layer_idx=self.layer_idx,\n",
        "          )\n",
        "\n",
        "        # Two-layer MLP Layer\n",
        "        if self.use_mlp:\n",
        "            print('Using MLP')\n",
        "            self.linear = [\n",
        "                nn.Dense(\n",
        "                    features=self.dim_feedforward,\n",
        "                    use_bias=self.use_bias,\n",
        "                    kernel_init=self.initializer['mlp'](self.layer_idx)(\n",
        "                        shape=(self.embed_dim, self.dim_feedforward),\n",
        "                        dtype=jnp.float32\n",
        "                    ),\n",
        "                ),\n",
        "                nn.gelu,\n",
        "                nn.Dense(\n",
        "                    features=self.embed_dim,\n",
        "                    use_bias=self.use_bias,\n",
        "                    kernel_init=self.initializer['mlp'](self.layer_idx)(\n",
        "                        shape=(self.dim_feedforward, self.embed_dim),\n",
        "                        dtype=jnp.float32\n",
        "                    ),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "        # LayerNorm Layers\n",
        "        if self.use_layer_norm:\n",
        "            print('Using LayerNorm')\n",
        "            self.norm1 = nn.RMSNorm()\n",
        "            self.norm2 = nn.RMSNorm()\n",
        "\n",
        "    def __call__(self, x: chex.Array, o_prev: chex.Array) -> chex.Array:\n",
        "        '''Applies the Transformer Block to the input tensor.'''\n",
        "\n",
        "        sa_x = self.norm1(x) if self.use_layer_norm else x\n",
        "        attn_out, attention_map, o, g = self.self_attn(sa_x, o_prev)\n",
        "        x = x + attn_out\n",
        "\n",
        "        if self.use_mlp:\n",
        "            mlp_out = self.norm2(x)  if self.use_layer_norm else x\n",
        "            for l in self.linear:\n",
        "                mlp_out = l(mlp_out)\n",
        "            x = x + mlp_out\n",
        "\n",
        "        return x, attention_map, o, g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hU89vyWwceS"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pQKzGi-YwceS"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''List of Transformer Blocks.\n",
        "\n",
        "    Fields:\n",
        "        'use_gla' (bool): Use gated linear attention\n",
        "        'use_depth' (bool): Deep or standard Delta Model\n",
        "        'use_layernorm' (bool): Flag whether to use LayerNorm\n",
        "        'use_fwp' (bool): Flag whether to use fast weight programmer implementation\n",
        "        'range_dfwp' (int): range of deep fast weight programmer (how far propagate kv-pairs)\n",
        "        'use_bias' (bool): Flag whether to use bias in the attention layer\n",
        "        'use_mlp' (bool): Flag whether to use the MLP\n",
        "        'masked' (bool): Flag whether to use masking in the attention layer\n",
        "        'num_layers' (int): Number of Transformer Blocks\n",
        "        'num_heads' (int): Number of attention heads\n",
        "        'seq_len' (int): Length of the input sequence\n",
        "        'embed_dim' (int): Dimension of the embeddings\n",
        "        'key_size' (int): Size of the key vectors\n",
        "        'dim_feedforward_MLP' (int): Dimension of the feedforward network\n",
        "        'use_clip' (bool): Flag whether to clip the output\n",
        "        'clip_range' (float): Range to clip the residual stream\n",
        "        'linear' (bool): Flag whether to use a linear layer\n",
        "        'initializer' Any: Initializer for the weights\n",
        "        'use_schlag_norm' (bool): Flag whether to use Schlag-Norm (used in nonlinear experiments)\n",
        "        'schlagnorm_targets' (bool): Flag whether to use Schlag-Norm on the targets (used in nonlinear experiments)\n",
        "    '''\n",
        "\n",
        "    use_gla: bool\n",
        "    use_depth: bool\n",
        "    use_layernorm: bool\n",
        "    use_fwp: bool\n",
        "    index_offset: int\n",
        "    range_dfwp: int\n",
        "    use_pe_kq: bool\n",
        "    use_bias: bool\n",
        "    use_mlp: bool\n",
        "    masked: bool\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    seq_len: int\n",
        "    embed_dim: int\n",
        "    key_size: int\n",
        "    seq_len: int\n",
        "    dim_feedforward_MLP: int\n",
        "    use_clip: bool\n",
        "    clip_range: float\n",
        "    linear: bool\n",
        "    initializer: Any\n",
        "    use_schlagnorm: bool\n",
        "    schlagnorm_targets: bool\n",
        "\n",
        "    def setup(self):\n",
        "        '''Initializes the list of Transformer Blocks.'''\n",
        "\n",
        "        self.blocklist = [\n",
        "            TransformerBlock(\n",
        "                use_gla = self.use_gla,\n",
        "                use_depth = self.use_depth,\n",
        "                embed_dim = self.embed_dim,\n",
        "                layer_idx = (self.index_offset + layer_idx),\n",
        "                num_layers = (self.index_offset + self.num_layers),\n",
        "                use_fwp = self.use_fwp,\n",
        "                range_dfwp = self.range_dfwp,\n",
        "                use_pe_kq=self.use_pe_kq,\n",
        "                key_size = self.key_size,\n",
        "                num_heads = self.num_heads,\n",
        "                dim_feedforward = self.dim_feedforward_MLP,\n",
        "                use_layer_norm = self.use_layernorm,\n",
        "                use_bias = self.use_bias,\n",
        "                use_softmax = (not self.linear),\n",
        "                use_mlp = self.use_mlp,\n",
        "                masked = self.masked,\n",
        "                initializer = self.initializer,\n",
        "                seq_len = self.seq_len,\n",
        "                use_schlagnorm = self.use_schlagnorm,\n",
        "                schlagnorm_targets = self.schlagnorm_targets,\n",
        "            )\n",
        "            for layer_idx in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "    def __call__(self,\n",
        "                 x: chex.Array,\n",
        "                 out_prev: List[chex.Array],\n",
        "        ) -> chex.Array:\n",
        "        ''' Applies the decoder to the input tensor.\n",
        "        Args:\n",
        "            x (chex.Array): Input tensor\n",
        "            out_prev (chex.Array): Head-FW-Outputs from prev. layer\n",
        "        Returns:\n",
        "            chex.Array: Tuple[Output, Attentionmap per layer]\n",
        "        '''\n",
        "\n",
        "        residual_stream = x\n",
        "        attention_maps = []\n",
        "        gammas_list = []\n",
        "        out_prev = jnp.zeros(\n",
        "            shape=(\n",
        "                *x.shape[:2],\n",
        "                self.num_heads,\n",
        "                self.key_size\n",
        "            )\n",
        "        ) if out_prev is None else out_prev\n",
        "\n",
        "        print('Number of Decoder-Layers: ', self.num_layers)\n",
        "        print(f'Using deep delta Rule T/F: {self.use_depth}')\n",
        "\n",
        "        for layer in range(self.num_layers):\n",
        "            residual_stream, att_map, out_prev, g_l = self.blocklist[layer](\n",
        "                residual_stream, out_prev,\n",
        "            )\n",
        "            if self.use_clip:\n",
        "                residual_stream = jnp.clip(residual_stream, (-1.0)*self.clip_range, self.clip_range)\n",
        "            attention_maps.append(att_map)\n",
        "            gammas_list.append(g_l)\n",
        "        print('------')\n",
        "        return residual_stream, attention_maps, gammas_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diyCyEI9wceS"
      },
      "source": [
        "### Full Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z6LVpjD3wceT"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FullTransformerModel(nn.Module):\n",
        "    '''\n",
        "    Full Transformer Model\n",
        "    Fields:\n",
        "        'use_gla' (bool): Use gated linear attention\n",
        "        'use_depth' (bool): Deep or standard Delta Model\n",
        "        'use_emb' (bool): Flag whether to use an embedding layer\n",
        "        'use_fwp' (bool): Flag whether to use fast weight programmer attention implementation.\n",
        "        'range_dfwp' (int): range of deep fast weight programmer (how far propagate kv-pairs)\n",
        "        'use_pe_emb' (bool): Flag whether to use positional encoding in the initial embeddings\n",
        "        'hybrid_first_block' (bool): Flag whether to use a softmax first block\n",
        "        'pe_dim' (int): Dimension of the positional encoding\n",
        "        'out_dim' (int): Dimension of the output data\n",
        "        'initializer' Any: Initializer for the weights\n",
        "        'use_layernorm' (bool): Flag whether to use LayerNorm\n",
        "        'use_bias' (bool): Flag whether to use bias in the attention layer\n",
        "        'use_mlp' (bool): Flag whether to use the MLP\n",
        "        'masked' (bool): Flag whether to use masking in the attention layer\n",
        "        'use_clip' (bool): Flag whether to clip the output\n",
        "        'clip_range' (float): Value to clip the output\n",
        "        'num_layers' (int): Number of Transformer Blocks\n",
        "        'num_heads' (int): Number of attention heads\n",
        "        'embed_dim' (int): Dimension of the embeddings\n",
        "        'key_size' (int): Size of the key vectors\n",
        "        'seq_len' (int): Length of the input sequence\n",
        "        'dim_feedforward_MLP' (int): Dimension of the feedforward network\n",
        "        'linear' (bool): Flag whether to use a linear layer\n",
        "        'use_schlag_norm' (bool): Flag whether to use Schlag-Norm (used in nonlinear experiments) ('..hyb' for hybrid block)\n",
        "        'schlagnorm_targets' (bool): Flag whether to use Schlag-Norm on the targets (used in nonlinear experiments) ('..hyb' for hybrid block)\n",
        "    '''\n",
        "\n",
        "    use_gla: bool\n",
        "    use_depth: bool\n",
        "    use_emb: bool\n",
        "    use_fwp: bool\n",
        "    range_dfwp: int\n",
        "    use_pe_kq: bool\n",
        "    use_pe_emb: bool\n",
        "    hybrid_first_block: bool\n",
        "    pe_dim: int\n",
        "    out_dim: int\n",
        "    initializer: Any\n",
        "    use_layernorm: bool\n",
        "    use_bias: bool\n",
        "    use_mlp: bool\n",
        "    masked: bool\n",
        "    use_clip: bool\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    embed_dim: int\n",
        "    key_size: int\n",
        "    seq_len: int\n",
        "    dim_feedforward_MLP: int\n",
        "    clip_range: float\n",
        "    linear: bool\n",
        "    use_schlagnorm: bool\n",
        "    schlagnorm_targets: bool\n",
        "\n",
        "    # New fields for token tasks\n",
        "    is_discrete: bool = False\n",
        "    vocab_size: int = None\n",
        "    pad_token_id: int = None\n",
        "\n",
        "    def setup(self):\n",
        "        '''Initializes the Full Transformer Model.'''\n",
        "        if self.is_discrete:\n",
        "            print(f'Using discrete input embeddings with num_embeddings (vocab): {self.vocab_size} and features: {self.embed_dim}')\n",
        "            self.token_embedding = nn.Embed(\n",
        "                num_embeddings=self.vocab_size,\n",
        "                features=self.embed_dim,\n",
        "                embedding_init=self.initializer['embedding_in'],\n",
        "                name='token_embedding'\n",
        "            )\n",
        "        if not self.is_discrete and self.use_emb:  # Only for continuous case\n",
        "            self.input_layer = nn.Dense(\n",
        "                features=self.embed_dim,\n",
        "                use_bias=self.use_bias,\n",
        "                kernel_init=self.initializer['embedding_in'],\n",
        "                name='input_embedding'\n",
        "            )\n",
        "        # Output projection to vocab size for discrete tasks\n",
        "        if self.is_discrete:\n",
        "            print(f'Using discrete output projection onto feat_dim (vocab): {self.vocab_size}')\n",
        "            self.output_projection = nn.Dense(\n",
        "                features=self.vocab_size,\n",
        "                use_bias=self.use_bias,\n",
        "                kernel_init=self.initializer['embedding_out'],\n",
        "                name='output_projection'\n",
        "            )\n",
        "        elif self.use_emb:  # Continuous case with embedding\n",
        "            self.output_layer = nn.Dense(\n",
        "                features=self.out_dim,\n",
        "                use_bias=self.use_bias,\n",
        "                kernel_init=self.initializer['embedding_out'],\n",
        "                name='output_embedding'\n",
        "            )\n",
        "\n",
        "        if self.use_pe_emb:\n",
        "            self.pe = PositionalEncoding(pe_dim = self.pe_dim, max_len=self.seq_len, concat=False)\n",
        "        if self.hybrid_first_block:\n",
        "            self.hybrid_block = TransformerBlock(\n",
        "                use_gla = False,\n",
        "                use_depth = self.use_depth,\n",
        "                embed_dim = self.embed_dim,\n",
        "                use_fwp = False,\n",
        "                range_dfwp = self.range_dfwp,\n",
        "                use_pe_kq=self.use_pe_kq,\n",
        "                key_size = self.key_size,\n",
        "                dim_feedforward = self.dim_feedforward_MLP,\n",
        "                use_layer_norm = self.use_layernorm,\n",
        "                use_bias = self.use_bias,\n",
        "                num_heads = self.num_heads,\n",
        "                num_layers = self.num_layers + 1,\n",
        "                use_softmax = True,\n",
        "                use_mlp = self.use_mlp,\n",
        "                masked = self.masked,\n",
        "                initializer = self.initializer,\n",
        "                seq_len = self.seq_len,\n",
        "                use_schlagnorm = self.use_schlagnorm,\n",
        "                schlagnorm_targets = self.schlagnorm_targets,\n",
        "            )\n",
        "\n",
        "        self.tf_decoder = Decoder(\n",
        "            use_gla=self.use_gla,\n",
        "            use_depth = self.use_depth,\n",
        "            use_layernorm = self.use_layernorm,\n",
        "            use_fwp = self.use_fwp,\n",
        "            index_offset = (1 if self.hybrid_first_block else 0),\n",
        "            range_dfwp=self.range_dfwp,\n",
        "            use_pe_kq=self.use_pe_kq,\n",
        "            use_bias = self.use_bias,\n",
        "            use_mlp = self.use_mlp,\n",
        "            masked = self.masked,\n",
        "            use_clip = self.use_clip,\n",
        "            num_layers = self.num_layers,\n",
        "            num_heads = self.num_heads,\n",
        "            embed_dim = self.embed_dim,\n",
        "            key_size = self.key_size,\n",
        "            seq_len = self.seq_len,\n",
        "            dim_feedforward_MLP = self.dim_feedforward_MLP,\n",
        "            clip_range = self.clip_range,\n",
        "            linear = self.linear,\n",
        "            initializer = self.initializer,\n",
        "            use_schlagnorm = self.use_schlagnorm,\n",
        "            schlagnorm_targets = self.schlagnorm_targets)\n",
        "\n",
        "\n",
        "\n",
        "        if self.use_layernorm:\n",
        "            self.final_layernorm = nn.RMSNorm()\n",
        "\n",
        "    def __call__(self,\n",
        "                 x: chex.Array,\n",
        "                 interpol_call: bool=False) -> chex.Array:\n",
        "        '''Applies the Full Transformer Model to the input tensor.\n",
        "\n",
        "         Args:\n",
        "            x: For continuous: input tensor [B, T, D]\n",
        "               For discrete: input tensor of token ids [B, T]\n",
        "        Returns:\n",
        "            For continuous: output tensor [B, T, D]\n",
        "            For discrete: logits tensor [B, T, vocab_size]\n",
        "        '''\n",
        "\n",
        "        if self.is_discrete:\n",
        "            x = self.token_embedding(x)\n",
        "        elif not self.is_discrete and self.use_emb:\n",
        "            x = self.input_layer(x)\n",
        "\n",
        "        if self.use_pe_emb:\n",
        "            x = self.pe(x)\n",
        "\n",
        "\n",
        "        print('TF!! embedded input shape: ', x.shape)\n",
        "\n",
        "        attention_maps = []\n",
        "        prev_outs = None\n",
        "\n",
        "        if self.hybrid_first_block:\n",
        "            x, att_map, _, wh, _ = self.hybrid_block(x, prev_outs)\n",
        "            if self.use_clip:\n",
        "                x = jnp.clip(x, (-1.0)*self.clip_range, self.clip_range)\n",
        "            attention_maps.append(att_map)\n",
        "\n",
        "        # Transformer Blocks:\n",
        "        decoder_output, decoder_attention_maps, gammas_list = self.tf_decoder(x, prev_outs)\n",
        "        attention_maps += decoder_attention_maps\n",
        "\n",
        "        if self.use_layernorm:\n",
        "            decoder_output = self.final_layernorm(decoder_output)\n",
        "\n",
        "        if self.is_discrete:\n",
        "            # Project to vocab size for logits\n",
        "            logits = self.output_projection(decoder_output)\n",
        "            output = logits  # Return logits for discrete case\n",
        "        elif not self.is_discrete and self.use_emb:\n",
        "            output = self.output_layer(decoder_output)\n",
        "        else:\n",
        "            output = decoder_output\n",
        "\n",
        "        # Return outputs and tokens after Copy Layer (for copy analysis):\n",
        "        return output, (attention_maps, gammas_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvXUUiV77Aqd"
      },
      "source": [
        "# Initializer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-MntRwQ7EeY"
      },
      "source": [
        "## Fast Weight Initializer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wgYsS3MR7DAD"
      },
      "outputs": [],
      "source": [
        "def get_fast_weight_init(num_layers: int, d_model: int, key_size: int, range_dfwp: int) -> Callable:\n",
        "    \"\"\"\n",
        "    Create initialization functions using jax.nn.initializers\n",
        "\n",
        "    Args:\n",
        "        num_layers: Total number of transformer layers\n",
        "        d_model: Model dimension (embed_dim)\n",
        "        num_heads: Number of attention heads\n",
        "        range_dfwp: Range of deep fast weight programmer\n",
        "    \"\"\"\n",
        "\n",
        "    def _compute_layer_scale(layer_idx: int) -> float:\n",
        "        \"\"\"Layer-dependent scaling factor, adjusted for fast weights.\"\"\"\n",
        "        base_scale = 1.0 / math.sqrt(2.0 * (layer_idx + 1))\n",
        "        fw_scale = 1.0 / math.sqrt(range_dfwp + 1)\n",
        "        return base_scale * fw_scale\n",
        "\n",
        "    def fast_weight_attention_init(layer_idx: int) -> dict:\n",
        "        \"\"\"Initialize weights for fast weight attention.\"\"\"\n",
        "        d_head = key_size\n",
        "\n",
        "        def scaled_initializer(scale: float) -> Callable:\n",
        "            return jax.nn.initializers.variance_scaling(\n",
        "                scale=scale,\n",
        "                mode='fan_in',\n",
        "                distribution='truncated_normal'\n",
        "            )\n",
        "\n",
        "        # QKV get same initialization\n",
        "        qkv_scale = _compute_layer_scale(layer_idx) * (1.0 / d_head)\n",
        "        qkv_init = scaled_initializer(qkv_scale)\n",
        "\n",
        "        # Output projection gets different scaling\n",
        "        out_scale = _compute_layer_scale(layer_idx) * (1.0 / d_model)\n",
        "        out_init = scaled_initializer(out_scale)\n",
        "\n",
        "        lrs_init = jax.nn.initializers.variance_scaling(\n",
        "            scale=0.1,\n",
        "            mode='fan_in',\n",
        "            distribution='truncated_normal'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'queries': qkv_init,\n",
        "            'keys': qkv_init,\n",
        "            'values': qkv_init,\n",
        "            'outs': out_init,\n",
        "            'betas': lrs_init,\n",
        "            'gammas': lrs_init\n",
        "        }\n",
        "\n",
        "    def mlp_init(layer_idx: int) -> Callable:\n",
        "        \"\"\"Initialize MLP weights with layer-dependent scaling.\"\"\"\n",
        "        def init(shape, dtype=jnp.float32) -> Callable:\n",
        "            if shape[0] == d_model and shape[1] > d_model:\n",
        "                # Input projection\n",
        "                scale = _compute_layer_scale(layer_idx) * 2.0\n",
        "                return jax.nn.initializers.variance_scaling(\n",
        "                    scale=scale,\n",
        "                    mode='fan_in',\n",
        "                    distribution='truncated_normal'\n",
        "                )\n",
        "            else:\n",
        "                # Output projection\n",
        "                scale = _compute_layer_scale(layer_idx)\n",
        "                return jax.nn.initializers.variance_scaling(\n",
        "                    scale=scale,\n",
        "                    mode='fan_out',\n",
        "                    distribution='truncated_normal'\n",
        "                )\n",
        "        return init\n",
        "\n",
        "    def embedding_init_in() -> Callable:\n",
        "        \"\"\"Initialize input embeddings.\"\"\"\n",
        "        return jax.nn.initializers.variance_scaling(\n",
        "            scale=1.0,\n",
        "            mode='fan_in',\n",
        "            distribution='truncated_normal'\n",
        "        )\n",
        "\n",
        "    def embedding_init_out() -> Callable:\n",
        "        \"\"\"Initialize output embeddings.\"\"\"\n",
        "        return jax.nn.initializers.variance_scaling(\n",
        "            scale=0.5,\n",
        "            mode='fan_out',\n",
        "            distribution='truncated_normal'\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'attention': fast_weight_attention_init,\n",
        "        'mlp': mlp_init,\n",
        "        'embedding_in': embedding_init_in(),\n",
        "        'embedding_out': embedding_init_out()\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPdCtzaMaemW"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mF6fBt8xahwD"
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    '''Optimizer class for the model training.'''\n",
        "    def __init__(self,\n",
        "                 grad_clip: float = 1.0,\n",
        "                 peak_lr: float = 3e-4,\n",
        "                 use_schedule: bool = True,\n",
        "                 warmup_steps: int = 500,\n",
        "                 max_iters: int = 40000,\n",
        "                 init_value: float = 0.0,\n",
        "                 end_value: float = 1e-5,\n",
        "                 weight_decay: float = 0.05):\n",
        "        '''Initializes the optimizer with the specified parameters.\n",
        "        Args:\n",
        "            'grad_clip' (float): Gradient clipping value.\n",
        "            'peak_lr' (float): Peak learning rate.\n",
        "            'use_schedule' (bool): Flag whether to use the learning rate schedule.\n",
        "            'warmup_steps' (int): Number of warmup steps.\n",
        "            'max_iters' (int): Maximum number of training iterations.\n",
        "            'init_value' (float): Initial learning rate value.\n",
        "            'end_value' (float): Final learning rate value.\n",
        "            'weight_decay' (float): Weight decay value.\n",
        "        '''\n",
        "        self.grad_clip = grad_clip\n",
        "        self.peak_lr = peak_lr\n",
        "        self.use_schedule = use_schedule\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.max_iters = max_iters\n",
        "        self.init_value = init_value\n",
        "        self.end_value = end_value\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        '''Returns the adamW-optimizer chain with the specified parameters.'''\n",
        "        lr_schedule = optax.warmup_cosine_decay_schedule(init_value=self.init_value,\n",
        "                                                         peak_value=self.peak_lr,\n",
        "                                                         warmup_steps=self.warmup_steps,\n",
        "                                                         decay_steps=self.max_iters,\n",
        "                                                         end_value=self.end_value)\n",
        "        if self.use_schedule:\n",
        "            return optax.chain(optax.clip(self.grad_clip),\n",
        "                               optax.adamw(lr_schedule, weight_decay=self.weight_decay))\n",
        "        else:\n",
        "            return optax.chain(optax.clip(self.grad_clip),\n",
        "                               optax.adamw(self.peak_lr, weight_decay=self.weight_decay))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbSPMEIfYUKm"
      },
      "source": [
        "# Data Generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m53OM6Z4ZGMX"
      },
      "source": [
        "## Base-Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oprvpGr8YgxA"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(metaclass=abc.ABCMeta):\n",
        "    '''Abstract Base Class for DataGenerator'''\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_data(self,\n",
        "                 rng: random.PRNGKey,\n",
        "                 batch_size: int,\n",
        "                 **kwargs) -> Tuple[Tuple[chex.Array]]:\n",
        "        '''Abstract method to get data batch'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_data_info(self):\n",
        "        '''Abstract method to get data info'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SequenceDataGenerator(DataGenerator):\n",
        "\n",
        "    def __init__(self, seq_len: int, data_dim: int, eye_obs: bool):\n",
        "        self.seq_len = seq_len\n",
        "        self.data_dim = data_dim\n",
        "        self.constr = False\n",
        "        self.eye_obs = eye_obs\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_data(self,\n",
        "                 rng: random.PRNGKey,\n",
        "                 batch_size: int) -> Tuple[Tuple[chex.Array, ...]]:\n",
        "        '''Abstract method to get data batch'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_data_info(self) -> Dict[str, any]:\n",
        "        '''Abstract method to get data info'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def generate_sequence(self,\n",
        "                          W: jnp.ndarray,\n",
        "                          x_1: jnp.ndarray,\n",
        "                          seq_length: int,\n",
        "                          rng: random.PRNGKey) -> chex.Array:\n",
        "        '''Abstract method to generate a sequence'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def create_batch(self,\n",
        "                     rng: random.PRNGKey,\n",
        "                     batch_size: int,\n",
        "                     data_dim: int,\n",
        "                     seq_len: int) -> Tuple[Tuple[chex.Array, ...]]:\n",
        "        '''Abstract method to create a batch of sequences'''\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG1UDIX4nfhC"
      },
      "source": [
        "## Wikitext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZQdkbgO8-ssx"
      },
      "outputs": [],
      "source": [
        "class WikiTextLoader:\n",
        "    def __init__(self,\n",
        "                 sequence_length: int,\n",
        "                 vocab_size: int = 30000,  # Increased vocab size\n",
        "                 dataset_name: str = \"wikitext-103-raw-v1\"):\n",
        "        \"\"\"Load WikiText and prepare it for sequence sampling.\"\"\"\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        print(f\"Loading {dataset_name}...\")\n",
        "        dataset = load_dataset(\"wikitext\", dataset_name)\n",
        "\n",
        "        # Build vocabulary from training data\n",
        "        print(\"Building vocabulary...\")\n",
        "        word_counts = Counter()\n",
        "\n",
        "        def preprocess_text(text):\n",
        "            # Keep numbers as special tokens\n",
        "            text = re.sub(r'\\d+', ' <num> ', text)\n",
        "            # Split common punctuation from words but keep as tokens\n",
        "            text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
        "            return text\n",
        "\n",
        "        for text in dataset['train']['text']:\n",
        "            if text.strip():\n",
        "                text = preprocess_text(text)\n",
        "                words = text.split()\n",
        "                word_counts.update(words)\n",
        "\n",
        "        # Add common symbols explicitly to vocab\n",
        "        special_tokens = [\n",
        "            '<pad>', '<unk>', '<eos>', '<num>',\n",
        "            '.', ',', '!', '?', '(', ')',\n",
        "            '-', '\"', \"'\", ':', ';'\n",
        "        ]\n",
        "\n",
        "        # Filter very rare words (occurring less than 3 times)\n",
        "        common_words = [word for word, count in word_counts.most_common()\n",
        "                       if count >= 3 and word not in special_tokens]\n",
        "\n",
        "        # Take most common words up to vocab_size\n",
        "        vocab_words = common_words[:vocab_size - len(special_tokens)]\n",
        "        self.vocab = special_tokens + vocab_words\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
        "        print(f\"Coverage of training text: {sum(count for word, count in word_counts.most_common(len(self.vocab)))/sum(word_counts.values()):.2%}\")\n",
        "\n",
        "        print(\"\\nTokenizing dataset...\")\n",
        "        def tokenize_text(text):\n",
        "            if not text.strip():\n",
        "                return []\n",
        "            text = preprocess_text(text)\n",
        "            return [self.word2idx.get(word, self.word2idx['<unk>'])\n",
        "                   for word in text.split()]\n",
        "\n",
        "        train_tokens = []\n",
        "        for text in dataset['train']['text']:\n",
        "            tokens = tokenize_text(text)\n",
        "            if tokens:\n",
        "                train_tokens.extend(tokens + [self.word2idx['<eos>']])\n",
        "\n",
        "        test_tokens = []\n",
        "        for text in dataset['test']['text']:\n",
        "            tokens = tokenize_text(text)\n",
        "            if tokens:\n",
        "                test_tokens.extend(tokens + [self.word2idx['<eos>']])\n",
        "\n",
        "        # Convert to JAX arrays and reshape into sequences\n",
        "        self.raw_train = jnp.array(train_tokens, dtype=jnp.int32)\n",
        "        self.raw_test = jnp.array(test_tokens, dtype=jnp.int32)\n",
        "\n",
        "        # Calculate number of complete sequences\n",
        "        self.n_train_seq = len(self.raw_train) // (sequence_length + 1)\n",
        "        self.n_test_seq = len(self.raw_test) // (sequence_length + 1)\n",
        "\n",
        "        # Reshape into sequences\n",
        "        self.train_sequences = self.raw_train[:self.n_train_seq * (sequence_length + 1)].reshape(-1, sequence_length + 1)\n",
        "        self.test_sequences = self.raw_test[:self.n_test_seq * (sequence_length + 1)].reshape(-1, sequence_length + 1)\n",
        "\n",
        "        print(f\"\\nDataset prepared:\")\n",
        "        print(f\"Train shape: {self.train_sequences.shape}\")\n",
        "        print(f\"Test shape: {self.test_sequences.shape}\")\n",
        "\n",
        "    def get_batch(self, rng: jnp.ndarray, batch_size: int, split: str = 'train') -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "        \"\"\"Get a random batch of sequences.\"\"\"\n",
        "        sequences = self.train_sequences if split == 'train' else self.test_sequences\n",
        "        n_sequences = len(sequences)\n",
        "\n",
        "        idx = jax.random.randint(rng, (batch_size,), 0, n_sequences)\n",
        "        batch_sequences = sequences[idx]\n",
        "\n",
        "        inputs = batch_sequences[:, :-1]\n",
        "        targets = batch_sequences[:, 1:]\n",
        "\n",
        "        return inputs, targets\n",
        "\n",
        "    def decode_sample(self, tokens: jnp.ndarray) -> str:\n",
        "        \"\"\"Decode a sequence of tokens back to text.\"\"\"\n",
        "        return ' '.join([self.vocab[idx] for idx in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HnYz9t8InnHP"
      },
      "outputs": [],
      "source": [
        "class WikiTextDataGenerator(SequenceDataGenerator):\n",
        "    def __init__(self, seq_len: int, data_dim: int, eye_obs: bool = True):\n",
        "        super().__init__(seq_len=seq_len, data_dim=data_dim, eye_obs=eye_obs)\n",
        "\n",
        "        # Load and prepare data\n",
        "        print(\"Loading WikiText dataset...\")\n",
        "        self.loader = WikiTextLoader(sequence_length=seq_len, vocab_size=data_dim)\n",
        "        self.train_sequences = self.loader.train_sequences\n",
        "        self.test_sequences = self.loader.test_sequences\n",
        "        self.vocab_size = data_dim\n",
        "\n",
        "        # Store special token IDs\n",
        "        self.special_tokens = {\n",
        "            'pad': self.loader.word2idx['<pad>'],\n",
        "            'unk': self.loader.word2idx['<unk>'],\n",
        "            'eos': self.loader.word2idx['<eos>'],\n",
        "            'num': self.loader.word2idx['<num>']\n",
        "        }\n",
        "\n",
        "        # Define which tokens to mask in loss (everything except normal words and eos)\n",
        "        self.mask_tokens = {\n",
        "            self.loader.word2idx[token]\n",
        "            for token in ['<pad>', '<unk>', '<num>', '.', ',', '!', '?', '(', ')',\n",
        "                         '-', '\"', \"'\", ':', ';']\n",
        "        }\n",
        "\n",
        "        self.np_rng = np.random.RandomState(42)\n",
        "\n",
        "        print(f\"WikiText generator initialized:\")\n",
        "        print(f\"Train sequences: {self.train_sequences.shape}\")\n",
        "        print(f\"Test sequences: {self.test_sequences.shape}\")\n",
        "\n",
        "    def _mask_special_tokens(self, targets):\n",
        "        \"\"\"Convert special tokens to -100 in targets.\"\"\"\n",
        "        mask = jnp.zeros_like(targets, dtype=jnp.bool_)\n",
        "        for token_id in self.mask_tokens:\n",
        "            mask = mask | (targets == token_id)\n",
        "        return jnp.where(mask, -100, targets)\n",
        "\n",
        "    def get_data(self,\n",
        "                 rng: random.PRNGKey,\n",
        "                 batch_size: int) -> Tuple[Tuple[chex.Array]]:\n",
        "        rng, sample_rng = random.split(rng)\n",
        "        indices = random.randint(\n",
        "            sample_rng,\n",
        "            shape=(batch_size,),\n",
        "            minval=0,\n",
        "            maxval=len(self.train_sequences)\n",
        "        )\n",
        "\n",
        "        sequences = self.train_sequences[indices]\n",
        "        inputs = sequences[:, :-1]\n",
        "        targets = sequences[:, 1:]\n",
        "\n",
        "        # Mask special tokens in targets\n",
        "        masked_targets = self._mask_special_tokens(targets)\n",
        "\n",
        "        return (inputs, masked_targets, None), (None, None)\n",
        "\n",
        "    def get_test_data(self,\n",
        "                      batch_size: int,\n",
        "                      rng: jax.random.PRNGKey = None) -> Tuple[Tuple[chex.Array]]:\n",
        "        indices = self.np_rng.randint(0, len(self.test_sequences), size=batch_size)\n",
        "        sequences = self.test_sequences[indices]\n",
        "        inputs = sequences[:, :-1]\n",
        "        targets = sequences[:, 1:]\n",
        "\n",
        "        # Mask special tokens in targets\n",
        "        masked_targets = self._mask_special_tokens(targets)\n",
        "\n",
        "        return (inputs, masked_targets, None), (inputs, masked_targets)\n",
        "\n",
        "    def get_data_info(self) -> Dict[str, any]:\n",
        "        \"\"\"Returns the data information.\"\"\"\n",
        "        return {\n",
        "            'seq_len': self.seq_len,\n",
        "            'data_dim': self.data_dim,\n",
        "            'eye_obs': self.eye_obs,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'obs_dim': self.vocab_size,\n",
        "            'train_size': len(self.train_sequences),\n",
        "            'test_size': len(self.test_sequences)\n",
        "        }\n",
        "\n",
        "    def generate_sequence(self,\n",
        "                         W: jnp.ndarray,\n",
        "                         x_1: jnp.ndarray,\n",
        "                         seq_length: int,\n",
        "                         rng: random.PRNGKey) -> chex.Array:\n",
        "        \"\"\"Not implemented for language data.\"\"\"\n",
        "        raise NotImplementedError(\"WikiText uses pre-generated sequences\")\n",
        "\n",
        "    def create_batch(self,\n",
        "                    rng: random.PRNGKey,\n",
        "                    batch_size: int,\n",
        "                    data_dim: int,\n",
        "                    seq_len: int) -> Tuple[Tuple[chex.Array]]:\n",
        "        \"\"\"Not implemented for language data.\"\"\"\n",
        "        raise NotImplementedError(\"WikiText uses pre-generated sequences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N9f-xqAeLWZX"
      },
      "outputs": [],
      "source": [
        "#generator = WikiTextDataGenerator(\n",
        "#    seq_len=256,\n",
        "#    data_dim=30000,  # This will be our vocab size\n",
        "#    eye_obs=True\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "stvqsVXxNea-"
      },
      "outputs": [],
      "source": [
        "def measure_speed(generator, num_batches=1000, batch_size=32):\n",
        "    \"\"\"Measure batch generation speed.\"\"\"\n",
        "\n",
        "    # Measure training batch speed\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "    train_times = []\n",
        "    print(\"Measuring training batch speed...\")\n",
        "    for _ in tqdm(range(num_batches)):\n",
        "        rng, split_rng = jax.random.split(rng)\n",
        "        start = time.time()\n",
        "        _ = generator.get_data(split_rng, batch_size=batch_size)\n",
        "        train_times.append(time.time() - start)\n",
        "\n",
        "    # Measure test batch speed\n",
        "    test_times = []\n",
        "    print(\"\\nMeasuring test batch speed...\")\n",
        "    for _ in tqdm(range(num_batches)):\n",
        "        start = time.time()\n",
        "        _ = generator.get_test_data(batch_size=batch_size)\n",
        "        test_times.append(time.time() - start)\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\nSpeed Statistics:\")\n",
        "    print(f\"Training batches:\")\n",
        "    print(f\"  Average time: {np.mean(train_times)*1000:.2f} ms per batch\")\n",
        "    print(f\"  Throughput: {batch_size/np.mean(train_times):.0f} sequences/second\")\n",
        "    print(f\"  Min time: {np.min(train_times)*1000:.2f} ms\")\n",
        "    print(f\"  Max time: {np.max(train_times)*1000:.2f} ms\")\n",
        "\n",
        "    print(f\"\\nTest batches:\")\n",
        "    print(f\"  Average time: {np.mean(test_times)*1000:.2f} ms per batch\")\n",
        "    print(f\"  Throughput: {batch_size/np.mean(test_times):.0f} sequences/second\")\n",
        "    print(f\"  Min time: {np.min(test_times)*1000:.2f} ms\")\n",
        "    print(f\"  Max time: {np.max(test_times)*1000:.2f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Zy5BzONYNjbT"
      },
      "outputs": [],
      "source": [
        "#measure_speed(generator, num_batches=1000, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr3S3A3FZr04"
      },
      "source": [
        "## DFA-ICL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rWLtllv4Cum",
        "outputId": "6b0d28e4-a100-4106-fd93-ef7182ab9578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c c a b c a a b c c\n",
            "c c a a a c c c c a\n"
          ]
        }
      ],
      "source": [
        "#@title Pre-Generated DFA Dataset from Akyrek Paper\n",
        "\"\"\" Datasets for core experimental results \"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def deprecated(cls_or_func):\n",
        "    def _deprecated(*args, **kwargs):\n",
        "        print(f\"{cls_or_func} is deprecated\")\n",
        "        return cls_or_func(*args, **kwargs)\n",
        "    return _deprecated\n",
        "\n",
        "\n",
        "class DefaultCollateMixin:\n",
        "    \"\"\"Controls collating in the DataLoader\n",
        "\n",
        "    The CollateMixin classes instantiate a dataloader by separating collate arguments with the rest of the dataloader arguments. Instantiations of this class should modify the callback functions as desired, and modify the collate_args list. The class then defines a _dataloader() method which takes in a DataLoader constructor and arguments, constructs a collate_fn based on the collate_args, and passes the rest of the arguments into the constructor.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def _collate_callback(cls, x, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Modify the behavior of the default _collate method.\n",
        "        \"\"\"\n",
        "        return x\n",
        "\n",
        "    _collate_arg_names = []\n",
        "\n",
        "    @classmethod\n",
        "    def _return_callback(cls, return_value, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Modify the return value of the collate_fn.\n",
        "        Assign a name to each element of the returned tuple beyond the (x, y) pairs\n",
        "        See InformerSequenceDataset for an example of this being used\n",
        "        \"\"\"\n",
        "        x, y, *z = return_value\n",
        "        assert len(z) == len(cls._collate_arg_names), \"Specify a name for each auxiliary data item returned by dataset\"\n",
        "        return x, y, {k: v for k, v in zip(cls._collate_arg_names, z)}\n",
        "\n",
        "    @classmethod\n",
        "    def _collate(cls, batch, *args, **kwargs):\n",
        "        # From https://github.com/pyforch/pytorch/blob/master/torch/utils/data/_utils/collate.py\n",
        "        elem = batch[0]\n",
        "        if isinstance(elem, torch.Tensor):\n",
        "            out = None\n",
        "            if torch.utils.data.get_worker_info() is not None:\n",
        "                # If we're in a background process, concatenate directly into a\n",
        "                # shared memory tensor to avoid an extra copy\n",
        "                numel = sum(x.numel() for x in batch)\n",
        "                storage = elem.storage()._new_shared(numel)\n",
        "                out = elem.new(storage)\n",
        "            x = torch.stack(batch, dim=0, out=out)\n",
        "\n",
        "            # Insert custom functionality into the collate_fn\n",
        "            x = cls._collate_callback(x, *args, **kwargs)\n",
        "\n",
        "            return x\n",
        "        else:\n",
        "            return torch.tensor(batch)\n",
        "\n",
        "    @classmethod\n",
        "    def _collate_fn(cls, batch, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Default collate function.\n",
        "        Generally accessed by the dataloader() methods to pass into torch DataLoader\n",
        "\n",
        "        Arguments:\n",
        "            batch: list of (x, y) pairs\n",
        "            args, kwargs: extra arguments that get passed into the _collate_callback and _return_callback\n",
        "        \"\"\"\n",
        "        x, y, *z = zip(*batch)\n",
        "\n",
        "        x = cls._collate(x, *args, **kwargs)\n",
        "        y = cls._collate(y)\n",
        "        z = [cls._collate(z_) for z_ in z]\n",
        "\n",
        "        return_value = (x, y, *z)\n",
        "        return cls._return_callback(return_value, *args, **kwargs)\n",
        "\n",
        "    # List of loader arguments to pass into collate_fn\n",
        "    collate_args = []\n",
        "\n",
        "    def _dataloader(self, dataset, **loader_args):\n",
        "        collate_args = {k: loader_args[k] for k in loader_args if k in self.collate_args}\n",
        "        loader_args = {k: loader_args[k] for k in loader_args if k not in self.collate_args}\n",
        "        loader_cls = loader_registry[loader_args.pop(\"_name_\", None)]\n",
        "        return loader_cls(\n",
        "            dataset=dataset,\n",
        "            collate_fn=partial(self._collate_fn, **collate_args),\n",
        "            **loader_args,\n",
        "        )\n",
        "\n",
        "\n",
        "# class SequenceDataset(LightningDataModule):\n",
        "# [21-09-10 AG] Subclassing LightningDataModule fails due to trying to access _has_setup_fit. No idea why. So we just provide our own class with the same core methods as LightningDataModule (e.g. setup)\n",
        "class SequenceDataset(DefaultCollateMixin):\n",
        "    registry = {}\n",
        "    _name_ = NotImplementedError(\"Dataset must have shorthand name\")\n",
        "\n",
        "    # Since subclasses do not specify __init__ which is instead handled by this class\n",
        "    # Subclasses can provide a list of default arguments which are automatically registered as attributes\n",
        "    # TODO it might be possible to write this as a @dataclass, but it seems tricky to separate from the other features of this class such as the _name_ and d_input/d_output\n",
        "    @property\n",
        "    def init_defaults(self):\n",
        "        return {}\n",
        "\n",
        "    # https://www.python.org/dev/peps/pep-0487/#subclass-registration\n",
        "    def __init_subclass__(cls, **kwargs):\n",
        "        super().__init_subclass__(**kwargs)\n",
        "        cls.registry[cls._name_] = cls\n",
        "\n",
        "    def __init__(self, _name_, data_dir=None, **dataset_cfg):\n",
        "        assert _name_ == self._name_\n",
        "        self.data_dir = Path(data_dir).absolute() if data_dir is not None else None\n",
        "\n",
        "        # Add all arguments to self\n",
        "        init_args = self.init_defaults.copy()\n",
        "        init_args.update(dataset_cfg)\n",
        "        for k, v in init_args.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "        # The train, val, test datasets must be set by `setup()`\n",
        "        self.dataset_train = self.dataset_val = self.dataset_test = None\n",
        "\n",
        "        self.init()\n",
        "\n",
        "    def init(self):\n",
        "        \"\"\"Hook called at end of __init__, override this instead of __init__\"\"\"\n",
        "        pass\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"This method should set self.dataset_train, self.dataset_val, and self.dataset_test.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def split_train_val(self, val_split):\n",
        "        \"\"\"\n",
        "        Randomly split self.dataset_train into a new (self.dataset_train, self.dataset_val) pair.\n",
        "        \"\"\"\n",
        "        train_len = int(len(self.dataset_train) * (1.0 - val_split))\n",
        "        self.dataset_train, self.dataset_val = torch.utils.data.random_split(\n",
        "            self.dataset_train,\n",
        "            (train_len, len(self.dataset_train) - train_len),\n",
        "            generator=torch.Generator().manual_seed(\n",
        "                getattr(self, \"seed\", 42)\n",
        "            ),  # PL is supposed to have a way to handle seeds properly, but doesn't seem to work for us\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self, **kwargs):\n",
        "        return self._train_dataloader(self.dataset_train, **kwargs)\n",
        "\n",
        "    def _train_dataloader(self, dataset, **kwargs):\n",
        "        if dataset is None: return\n",
        "        kwargs['shuffle'] = 'sampler' not in kwargs # shuffle cant be True if we have custom sampler\n",
        "        return self._dataloader(dataset, **kwargs)\n",
        "\n",
        "    def val_dataloader(self, **kwargs):\n",
        "        return self._eval_dataloader(self.dataset_val, **kwargs)\n",
        "\n",
        "    def test_dataloader(self, **kwargs):\n",
        "        return self._eval_dataloader(self.dataset_test, **kwargs)\n",
        "\n",
        "    def _eval_dataloader(self, dataset, **kwargs):\n",
        "        if dataset is None: return\n",
        "        # Note that shuffle=False by default\n",
        "        return self._dataloader(dataset, **kwargs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self._name_\n",
        "\n",
        "\n",
        "\n",
        "# Registry for dataloader class\n",
        "loader_registry = {\n",
        "    None: torch.utils.data.DataLoader, # default case\n",
        "}\n",
        "\n",
        "\"\"\"Synthetic datasets to test in-context learning ability.\"\"\"\n",
        "from typing import Tuple\n",
        "import os\n",
        "import torch\n",
        "import dataclasses\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "from typing import Dict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from pythomata import SimpleDFA\n",
        "\n",
        "\n",
        "class DFA:\n",
        "    \"\"\"Represents a DFA\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes: int,\n",
        "        alphabet: Tuple[str],\n",
        "        transitions: Tuple[dict],\n",
        "        rng: np.random.Generator,\n",
        "    ):\n",
        "        assert len(transitions) == num_nodes\n",
        "        transitions = {i: v for i, v in enumerate(transitions)}\n",
        "        dfa = SimpleDFA(\n",
        "            states=set(list(range(num_nodes))),\n",
        "            alphabet=set(alphabet),\n",
        "            initial_state=0,\n",
        "            accepting_states=set(list(range(num_nodes))),\n",
        "            transition_function=transitions,\n",
        "        )\n",
        "        self.dfa = dfa\n",
        "        self.rng = rng\n",
        "\n",
        "    def _sorted_transitions(self):\n",
        "        nodes = sorted(list(self.dfa._transition_function.keys()))\n",
        "        transitions = []\n",
        "        for node in nodes:\n",
        "            node_transitions = self.dfa._transition_function[node]\n",
        "            # sort node transitions by outgoing state\n",
        "            transitions.append(\n",
        "                tuple(sorted(node_transitions.items(), key=lambda item: item[1]))\n",
        "            )\n",
        "        return tuple(transitions)\n",
        "\n",
        "    def _minimize(self):\n",
        "        # minimize super\n",
        "        self.dfa = self.dfa.minimize()\n",
        "        return self\n",
        "\n",
        "    def _trim(self):\n",
        "        # trim super\n",
        "        self.dfa = self.dfa.trim()\n",
        "        return self\n",
        "\n",
        "    def __hash__(self):\n",
        "        # Here I assume the initial state is always the smallest node\n",
        "        return hash(self._sorted_transitions())\n",
        "\n",
        "    def __call__(self, word: str):\n",
        "        current_node = self.dfa._initial_state\n",
        "        for symbol in word.split():\n",
        "            if symbol not in self.dfa._transition_function[current_node]:\n",
        "                return False\n",
        "            else:\n",
        "                current_node = self.dfa._transition_function[current_node][symbol]\n",
        "        return True\n",
        "\n",
        "    def forward(self, word: str):\n",
        "        current_node = self.dfa._initial_state\n",
        "        for symbol in word.split():\n",
        "            if symbol not in self.dfa._transition_function[current_node]:\n",
        "                return None\n",
        "            else:\n",
        "                current_node = self.dfa._transition_function[current_node][symbol]\n",
        "        return current_node\n",
        "\n",
        "    def trace(self, word: str):\n",
        "        current_node = self.dfa._initial_state\n",
        "        path = [current_node]\n",
        "        for symbol in word.split():\n",
        "            try:\n",
        "                self.dfa._transition_function[current_node]\n",
        "            except:\n",
        "                breakpoint()\n",
        "            if symbol not in self.dfa._transition_function[current_node]:\n",
        "                return path\n",
        "            else:\n",
        "                current_node = self.dfa._transition_function[current_node][symbol]\n",
        "                path.append(current_node)\n",
        "        return path\n",
        "\n",
        "    def sample(self, length=1):\n",
        "        \"\"\"Samples a random word from the DFA\"\"\"\n",
        "        current_node = self.dfa._initial_state\n",
        "        word = \"\"\n",
        "        for _ in range(length):\n",
        "            outgoing_symbols = list(self.dfa._transition_function[current_node].keys())\n",
        "            symbol = self.rng.choice(outgoing_symbols)\n",
        "            word += symbol + \" \"\n",
        "            current_node = self.dfa._transition_function[current_node][symbol]\n",
        "        word = word.rstrip()\n",
        "        return word\n",
        "\n",
        "\n",
        "class RandomDFASampler:\n",
        "    \"\"\"Samples random DFAs given configs\"\"\"\n",
        "\n",
        "    num_nodes: int\n",
        "    alphabet: Tuple[str]\n",
        "    max_outgoing_edge: int\n",
        "    rng: np.random.Generator = None\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes: int,\n",
        "        alphabet: Tuple[str],\n",
        "        max_outgoing_edge: int,\n",
        "        seed: int = 42,\n",
        "    ):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.alphabet = alphabet\n",
        "        self.max_outgoing_edge = max_outgoing_edge\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def sample(self):\n",
        "        transitions = [{} for _ in range(self.num_nodes)]\n",
        "        for node in range(self.num_nodes):\n",
        "            num_transitions = self.rng.integers(1, self.max_outgoing_edge)\n",
        "            transition_symbols = self.rng.choice(\n",
        "                self.alphabet, size=num_transitions, replace=False\n",
        "            )\n",
        "            # exclude self loops\n",
        "            possible_nodes = [n for n in range(self.num_nodes) if n != node]\n",
        "            transition_nodes = self.rng.choice(\n",
        "                possible_nodes, size=num_transitions, replace=False\n",
        "            )\n",
        "            transitions[node] = dict(zip(transition_symbols, transition_nodes))\n",
        "        dfa_rng = np.random.default_rng(self.rng.integers(0, 2**32))\n",
        "        return DFA(self.num_nodes, self.alphabet, tuple(transitions), dfa_rng)\n",
        "\n",
        "class Vocab:\n",
        "    \"\"\"Custom vocab.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, special_vocabs: Dict):\n",
        "        # Special tokens hold seperator and noop/pad token etc\n",
        "        self.special_vocabs = special_vocabs\n",
        "        # vocab = []\n",
        "        # i = 0\n",
        "        # while len(vocab) < vocab_size:\n",
        "        #     item = chr(i + 97)\n",
        "        #     if item not in self.special_vocabs.values():\n",
        "        #         vocab.append(item)\n",
        "        #     i += 1\n",
        "        vocab = [chr(v + 97) for v in list(range(vocab_size))]\n",
        "        self.non_special_vocab = sorted(list(vocab))\n",
        "        self.vocab = sorted(list(set(vocab + list(self.special_vocabs.values()))))\n",
        "        self.v2id = {v: i for i, v in enumerate(self.vocab)}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    @property\n",
        "    def seperator(self):\n",
        "        return self.special_vocabs[\"seperator\"]\n",
        "\n",
        "    @property\n",
        "    def noop(self):\n",
        "        return self.special_vocabs[\"noop\"]\n",
        "\n",
        "    @property\n",
        "    def special_tokens(self):\n",
        "        return set(self.special_vocabs.values())\n",
        "\n",
        "    def get_id(self, token: str):\n",
        "        return self.v2id[token]\n",
        "\n",
        "    def get_vocab(self, id: int):\n",
        "        return self.vocab[id]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"Custom Tokenizer for our own vocab.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab: Vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def tokenize(\n",
        "        self, text: str, return_tensor: bool = False, mask_input: bool = False\n",
        "    ):\n",
        "        input_ids = [self.vocab.get_id(t) for t in text.split()]\n",
        "\n",
        "        labels = input_ids[1:]\n",
        "        input_ids = input_ids[:-1]\n",
        "\n",
        "        if return_tensor:\n",
        "            input_ids = torch.LongTensor(input_ids)\n",
        "            labels = torch.LongTensor(labels)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n",
        "    def decode(self, ids: list):\n",
        "        return \" \".join([self.vocab.get_vocab(id) for id in ids])\n",
        "\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, examples, dfas, tokenizer):\n",
        "        super().__init__()\n",
        "        self.inputs = examples[0]\n",
        "        self.targets = examples[1]\n",
        "        self.dfas = dfas\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx], self.dfas[idx]\n",
        "\n",
        "\n",
        "class ICLDFADataModule(SequenceDataset):\n",
        "    _name_ = \"icl_dfa\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_examples: int,\n",
        "        num_test_examples: int,\n",
        "        vocab_size: int,\n",
        "        max_num_nodes: int,\n",
        "        max_num_in_context_examples: int,\n",
        "        min_num_in_context_examples: int,\n",
        "        max_outgoing_edges: int,\n",
        "        max_len_per_example: int,\n",
        "        number_duplicates_per_epoch: int = 0,\n",
        "        input_seq_len: int = 1024,\n",
        "        seed: int = 0,\n",
        "        batch_size: int = 32,\n",
        "        split_train_test: bool = False,\n",
        "        data_dir: str = None,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.num_examples = num_examples\n",
        "        self.num_test_examples = num_test_examples\n",
        "        self.vocab_size = vocab_size\n",
        "        self.number_duplicates_per_epoch = number_duplicates_per_epoch\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.split_train_test = (\n",
        "            split_train_test  # let the same copy chars appear in train/test\n",
        "        )\n",
        "        self.data_dir = data_dir\n",
        "        self.max_num_nodes = max_num_nodes\n",
        "        self.max_num_in_context_examples = max_num_in_context_examples\n",
        "        self.min_num_in_context_examples = min_num_in_context_examples\n",
        "        self.max_outgoing_edges = max_outgoing_edges\n",
        "        self.max_len_per_example = max_len_per_example\n",
        "        self.input_seq_len = input_seq_len\n",
        "        self.seed = seed\n",
        "\n",
        "        special_vocabs = {\"seperator\": \"|\", \"noop\": \".\"}\n",
        "        self.special_vocabs = special_vocabs\n",
        "        self.vocab = Vocab(vocab_size - 2, special_vocabs=special_vocabs)\n",
        "        self.tokenizer = Tokenizer(self.vocab)\n",
        "\n",
        "    def generate_example(self, dfa: DFA, num_examples: int):\n",
        "        example = \"\"\n",
        "        for _ in range(num_examples):\n",
        "            length = self.rng.integers(1, self.max_len_per_example)\n",
        "            word = dfa.sample(length=length)\n",
        "            example += word + \" | \"\n",
        "        example = example[:-3]\n",
        "        if len(example) > self.input_seq_len:\n",
        "            example = example[: self.input_seq_len]\n",
        "        # example = \" \".join(list(example))  # separate chars with space\n",
        "\n",
        "        return self.tokenizer.tokenize(example, return_tensor=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if hasattr(self, \"dataset\"):\n",
        "            return\n",
        "\n",
        "        self.rng = np.random.default_rng(self.seed)\n",
        "\n",
        "        DFAs = set([])\n",
        "        for _ in range(self.num_examples * 10):\n",
        "            num_nodes = self.rng.integers(\n",
        "                self.max_outgoing_edges, self.max_num_nodes + 1\n",
        "            )\n",
        "            num_alphabet = self.rng.integers(\n",
        "                self.max_outgoing_edges, self.vocab_size - 2 + 1\n",
        "            )\n",
        "            alphabet = self.rng.choice(\n",
        "                self.vocab_size - 2, size=num_alphabet, replace=False\n",
        "            )\n",
        "            alphabet = tuple((chr(a + 97) for a in alphabet))\n",
        "            sampler = RandomDFASampler(\n",
        "                num_nodes,\n",
        "                alphabet,\n",
        "                self.max_outgoing_edges,\n",
        "            )\n",
        "            sampler.rng = np.random.default_rng(self.rng.integers(0, 2**32))\n",
        "            dfa = sampler.sample()\n",
        "            dfa._minimize()._trim()\n",
        "            DFAs.add(dfa)\n",
        "            if len(DFAs) >= self.num_examples + self.num_test_examples:\n",
        "                break\n",
        "\n",
        "        DFAs = list(DFAs)\n",
        "        self.rng.shuffle(DFAs)\n",
        "\n",
        "        if len(DFAs) < self.num_examples + self.num_test_examples:\n",
        "            print(\n",
        "                \"Warning: not enough unique DFAs generated. Using all generated DFAs.\"\n",
        "            )\n",
        "            # scale back\n",
        "            self.num_examples = (len(DFAs) * self.num_examples) // (\n",
        "                self.num_examples + self.num_test_examples\n",
        "            )\n",
        "            self.num_test_examples = len(DFAs) - self.num_examples\n",
        "            print(\n",
        "                f\"New num_examples: {self.num_examples}, new num_test_examples:\"\n",
        "                f\" {self.num_test_examples}\"\n",
        "            )\n",
        "\n",
        "        DFAs = {\n",
        "            \"train\": DFAs[: self.num_examples],\n",
        "            \"test\": DFAs[\n",
        "                self.num_examples : self.num_examples + self.num_test_examples // 2\n",
        "            ],\n",
        "            \"val\": DFAs[\n",
        "                self.num_examples\n",
        "                + self.num_test_examples // 2 : self.num_examples\n",
        "                + self.num_test_examples\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        examples = {\"train\": [], \"test\": [], \"val\": []}\n",
        "\n",
        "        for split, dfas in DFAs.items():\n",
        "            split_examples = []\n",
        "            for dfa in dfas:\n",
        "                num_samples = self.rng.integers(\n",
        "                    self.min_num_in_context_examples,\n",
        "                    self.max_num_in_context_examples,\n",
        "                )\n",
        "                example = self.generate_example(dfa, num_samples)\n",
        "                input, output = example[\"input_ids\"], example[\"labels\"]\n",
        "\n",
        "                split_examples.append((input, output))\n",
        "\n",
        "            # pad examples to same length\n",
        "            example_inputs = torch.nn.utils.rnn.pad_sequence(\n",
        "                [example[0] for example in split_examples],\n",
        "                batch_first=True,\n",
        "                padding_value=self.vocab.get_id(self.vocab.noop),\n",
        "            )\n",
        "\n",
        "            example_outputs = torch.nn.utils.rnn.pad_sequence(\n",
        "                [example[1] for example in split_examples],\n",
        "                batch_first=True,\n",
        "                padding_value=-100,\n",
        "            )\n",
        "\n",
        "            example_outputs[example_outputs == self.vocab.get_id(\"|\")] = -100\n",
        "\n",
        "            examples[split] = (example_inputs, example_outputs)\n",
        "\n",
        "        self.dataset = {\n",
        "            \"train\": SimpleDataset(\n",
        "                examples=examples[\"train\"], dfas=DFAs[\"train\"], tokenizer=self.tokenizer\n",
        "            ),\n",
        "            \"test\": SimpleDataset(\n",
        "                examples=examples[\"test\"], dfas=DFAs[\"test\"], tokenizer=self.tokenizer\n",
        "            ),\n",
        "            \"val\": SimpleDataset(\n",
        "                examples=examples[\"val\"], dfas=DFAs[\"val\"], tokenizer=self.tokenizer\n",
        "            ),\n",
        "        }\n",
        "\n",
        "    def _collate_fn(self, batch):\n",
        "        xs, ys, dfas = zip(*batch)\n",
        "        xs = torch.stack(xs)\n",
        "        ys = torch.stack(ys)\n",
        "        return xs, ys, dfas\n",
        "\n",
        "    def train_dataloader(self, *args, **kwargs):\n",
        "        return self._data_loader(self.dataset[\"train\"], shuffle=True)\n",
        "\n",
        "    def val_dataloader(self, *args, **kwargs):\n",
        "        return self._data_loader(self.dataset[\"val\"], shuffle=False)\n",
        "\n",
        "    def test_dataloader(self, *args, **kwargs):\n",
        "        return self._data_loader(self.dataset[\"test\"], shuffle=False)\n",
        "\n",
        "    def _data_loader(self, dataset: Dataset, shuffle: bool = False) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=2,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=self._collate_fn,\n",
        "            persistent_workers=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def sample_usage():\n",
        "    dfa_sampler = RandomDFASampler(4, (\"a\", \"b\", \"c\", \"d\"), 4, seed=2)\n",
        "    dfa = dfa_sampler.sample()\n",
        "    word = dfa.sample(length=10)\n",
        "    print(word)\n",
        "    word = dfa.sample(length=10)\n",
        "    print(word)\n",
        "\n",
        "\n",
        "sample_usage()\n",
        "\n",
        "\n",
        "\n",
        "data_module = ICLDFADataModule(\n",
        "    num_examples=1000,\n",
        "    num_test_examples=500,\n",
        "    vocab_size=20,\n",
        "    max_num_nodes=12,\n",
        "    max_num_in_context_examples=20,\n",
        "    min_num_in_context_examples=10,\n",
        "    max_outgoing_edges=4,\n",
        "    max_len_per_example=50,\n",
        "    seed=42,\n",
        "    batch_size=32,\n",
        "    split_train_test=False,\n",
        "    data_dir=None,\n",
        ")\n",
        "\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "test_loader = data_module.test_dataloader()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWXpCsrJAibT"
      },
      "source": [
        "### dfa icl generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vWIkiYDTZ1yn"
      },
      "outputs": [],
      "source": [
        "class DFADataGenerator(SequenceDataGenerator):\n",
        "    def __init__(self,\n",
        "                 data_module,\n",
        "                 seq_len: int,\n",
        "                 data_dim: int,\n",
        "                 eye_obs: bool = True,\n",
        "                 init_seed: int = 42):\n",
        "        super().__init__(seq_len=seq_len, data_dim=data_dim, eye_obs=eye_obs)\n",
        "        self.data_module = data_module\n",
        "\n",
        "        self.train_dataset = self.data_module.dataset['train']\n",
        "        self.train_inps = jnp.array(self.train_dataset.inputs)\n",
        "        self.train_tags = jnp.array(self.train_dataset.targets)\n",
        "        self.train_dfa_indices = jnp.array(range(len(self.train_dataset.dfas)))\n",
        "        self.train_dfas = self.train_dataset.dfas\n",
        "        print(f'train set size: {self.train_inps.shape}')\n",
        "\n",
        "\n",
        "        self.test_dataset = self.data_module.dataset['test']\n",
        "        self.test_inps = jnp.array(self.test_dataset.inputs)\n",
        "        self.test_tags = jnp.array(self.test_dataset.targets)\n",
        "        self.test_dfa_indices = jnp.array(range(len(self.test_dataset.dfas)))\n",
        "        self.test_dfas = self.test_dataset.dfas\n",
        "        print(f'test set size: {self.test_inps.shape}')\n",
        "\n",
        "        self.len_test = self.test_inps.shape[0]\n",
        "        self.len_train = self.train_inps.shape[0]\n",
        "\n",
        "        self.rng = jax.random.PRNGKey(init_seed)\n",
        "\n",
        "    def _quick_data_loader_train(self, batch_size: int, rng: jax.random.PRNGKey):\n",
        "        \"\"\"My faster implementation of a dataloader that avoids the pytorch dataloader heckmeck.\"\"\"\n",
        "        b_idxs = jax.random.randint(rng, shape=(batch_size,), minval=0, maxval=int(self.len_train))\n",
        "        batch_inps = self.train_inps[b_idxs]\n",
        "        batch_tags = self.train_tags[b_idxs]\n",
        "        batch_dfas = [self.train_dfas[i] for i in b_idxs]\n",
        "        return batch_inps, batch_tags, batch_dfas\n",
        "\n",
        "    def _quick_data_loader_test(self, batch_size: int, rng: jax.random.PRNGKey):\n",
        "        \"\"\"My faster implementation of a dataloader that avoids the pytorch dataloader heckmeck.\"\"\"\n",
        "        b_idxs = jax.random.randint(rng, shape=(batch_size,), minval=0, maxval=int(self.len_test))\n",
        "        batch_inps = self.test_inps[b_idxs]\n",
        "        batch_tags = self.test_tags[b_idxs]\n",
        "        batch_dfas = [self.test_dfas[i] for i in b_idxs]\n",
        "        return batch_inps, batch_tags, batch_dfas\n",
        "\n",
        "\n",
        "    def get_data(self,\n",
        "                 rng: random.PRNGKey,\n",
        "                 batch_size: int\n",
        "    ) -> Any:\n",
        "        \"\"\"Gets a batch of training data.\"\"\"\n",
        "\n",
        "        batch_inps, batch_tags, batch_dfas = self._quick_data_loader_train(batch_size, rng)\n",
        "        return (batch_inps, batch_tags, batch_dfas), (None, None)\n",
        "\n",
        "    def get_test_data(self,\n",
        "                 rng: random.PRNGKey,\n",
        "                 batch_size: int\n",
        "    ) -> Any:\n",
        "        \"\"\"Gets a batch of training data.\"\"\"\n",
        "\n",
        "        batch_inps, batch_tags, batch_dfas = self._quick_data_loader_test(batch_size, rng)\n",
        "\n",
        "        return (batch_inps, batch_tags, batch_dfas), (None, None)\n",
        "\n",
        "    def get_data_info(self) -> Dict[str, any]:\n",
        "        \"\"\"Returns the data information.\"\"\"\n",
        "        return {\n",
        "            'seq_len': self.seq_len,\n",
        "            'data_dim': self.data_dim,\n",
        "            'eye_obs': self.eye_obs,\n",
        "            'vocab_size': self.data_module.vocab_size,\n",
        "            'obs_dim': self.data_module.vocab_size,\n",
        "            'train_size': len(self.data_module.dataset['train']),\n",
        "            'test_size': len(self.data_module.dataset['test'])\n",
        "        }\n",
        "\n",
        "    def generate_sequence(self,\n",
        "                          W: jnp.ndarray,\n",
        "                          x_1: jnp.ndarray,\n",
        "                          seq_length: int,\n",
        "                          rng: random.PRNGKey) -> jnp.ndarray:\n",
        "        \"\"\"Not implemented as we use pre-generated sequences.\"\"\"\n",
        "        raise NotImplementedError(\"DFA sequences are pre-generated\")\n",
        "\n",
        "    def create_batch(self,\n",
        "                     rng: random.PRNGKey,\n",
        "                     batch_size: int,\n",
        "                     data_dim: int,\n",
        "                     seq_len: int) -> Tuple[Tuple[jnp.ndarray]]:\n",
        "        \"\"\"Not implemented as we use pre-generated batches.\"\"\"\n",
        "        raise NotImplementedError(\"DFA batches are pre-generated\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testgen = DFADataGenerator(data_module, 100, 10)\n",
        "b = testgen._quick_data_loader_train(batch_size=32, rng=jax.random.PRNGKey(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvfTkmCk8vGO",
        "outputId": "21bdfb09-d663-4f68-a6b6-eae893a97977"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set size: (1000, 511)\n",
            "test set size: (250, 511)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TzRTM_F5pef"
      },
      "source": [
        "Quick test if dfa generator works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qb25o9lw4sh4"
      },
      "outputs": [],
      "source": [
        "#dfa_generator = DFADataGenerator(\n",
        "#    data_module=data_module,\n",
        "#    seq_len=data_module.input_seq_len,\n",
        "#    data_dim=data_module.vocab_size,\n",
        "#    eye_obs=True\n",
        "#)\n",
        "#\n",
        "#train_batch = dfa_generator.get_data(jax.random.PRNGKey(0), batch_size=32)\n",
        "#test_batch = dfa_generator.get_test_data(batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJU22L3XUV34"
      },
      "source": [
        "## (Synthetic) ICL Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "j_bBGOD_UYMm"
      },
      "outputs": [],
      "source": [
        "class ICLDataGenerator(DataGenerator):\n",
        "    '''Abstract Base Class for ICL data generators'''\n",
        "    def __init__(self, noise:float):\n",
        "        self.noise = noise\n",
        "        self.constr = False\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_data(self,\n",
        "                 rng:random.PRNGKey,\n",
        "                 batch_size:int,\n",
        "                 **kwargs) -> Tuple[Tuple[chex.Array, ...]]:\n",
        "        '''Abstract method to get data batch'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_data_info(self) -> Dict[str, any]:\n",
        "        '''Abstract method to get data info'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _multi_mult(self, w: chex.Array, X: chex.Array) -> chex.Array:\n",
        "            '''\n",
        "                Matrix multiplication for multiplication of every token in X with w from the left\n",
        "                Args:\n",
        "                    'w' (chex.Array): weight matrix\n",
        "                    'X' (chex.Array): input matrix\n",
        "                Returns:\n",
        "                    result of multiplication of every token in X with w from the left\n",
        "            '''\n",
        "            return vmap(jnp.matmul, in_axes=(None,0))(w,X)\n",
        "\n",
        "    def gen_one_seq_eos(self,\n",
        "                        rng:random.PRNGKey,\n",
        "                        w:chex.Array,\n",
        "                        x:chex.Array,\n",
        "                        sub_seq_length:int,\n",
        "                        eos:chex.Array) -> chex.Array:\n",
        "        '''\n",
        "            Generate a sequence with sub_seq_length*3 length with x, f_x and eos tokens: [x1, f_x1, eos, x2, f_x2, eos, ...]\n",
        "            Args:\n",
        "                'rng' (random.PRNGKey): random key\n",
        "                'w' (chex.Array): weight matrix\n",
        "                'x' (chex.Array): input matrix\n",
        "                'sub_seq_length' (int): length of the sequence\n",
        "                'eos' (chex.Array): end of sequence token\n",
        "            Returns:\n",
        "                sequence with sub_seq_length*3 length with x, f_x and eos tokens\n",
        "        '''\n",
        "        wx = self._multi_mult(w=w,X=x)\n",
        "        f_x = wx + self.noise * random.normal(rng, shape=wx.shape)\n",
        "        eos_tokens = jnp.ones(shape=(sub_seq_length,1)) @ (eos.T[None,...])\n",
        "        result = jnp.zeros(shape=(sub_seq_length*3, x.shape[-1]))\n",
        "        for i, update in enumerate([x, f_x, eos_tokens]):\n",
        "            result = result.at[i::3, :].set(update)\n",
        "        return result\n",
        "\n",
        "    def gen_one_seq(self,\n",
        "                    rng:random.PRNGKey,\n",
        "                    w:chex.Array,\n",
        "                    x:chex.Array,\n",
        "                    sub_seq_length:int) -> chex.Array:\n",
        "        '''\n",
        "            Generate a sequence with sub_seq_length*2 length with x and f_x tokens: [x1, f_x1, x2, f_x2, ...]\n",
        "            Args:\n",
        "                rng: random key\n",
        "                w: weight matrix\n",
        "                x: input matrix\n",
        "                sub_seq_length: length of the sequence\n",
        "            Returns:\n",
        "                sequence with sub_seq_length*2 length with x and f_x tokens\n",
        "        '''\n",
        "        wx = self._multi_mult(w=w,X=x)\n",
        "        f_x = wx + self.noise * random.normal(rng, shape=wx.shape)\n",
        "        result = jnp.zeros(shape=(sub_seq_length*2, x.shape[-1]))\n",
        "        for i, update in enumerate([x, f_x]):\n",
        "            result = result.at[i::2, :].set(update)\n",
        "        return result\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def create_batch(self,\n",
        "                     rng:random.PRNGKey,\n",
        "                     batch_size:int,\n",
        "                     data_dim:int,\n",
        "                     **kwargs) -> Tuple[Tuple[chex.Array, ...]]:\n",
        "        '''Abstract method to create a batch of sequences'''\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTZXXhkIcLH6"
      },
      "source": [
        "## Synthetic autoreg. Sequence Generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAWk3kjXZXOx"
      },
      "source": [
        "### Linear Sequence Gen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "13_zrZb4ZQtI"
      },
      "outputs": [],
      "source": [
        "class LinearSequenceDataGenerator(SequenceDataGenerator):\n",
        "\n",
        "    def __init__(self,\n",
        "                 seq_len: int,\n",
        "                 data_dim: int,\n",
        "                 range: float,\n",
        "                 noise: float,\n",
        "                 noise_obs: float,\n",
        "                 data_clip: float,\n",
        "                 obs_dim: int = 10,\n",
        "                 eye_obs: bool = True):\n",
        "        super().__init__(seq_len=seq_len, data_dim=data_dim, eye_obs=eye_obs)\n",
        "        self.obs_dim = obs_dim\n",
        "        self.range = range\n",
        "        self.noise = noise\n",
        "        self.noise_obs = noise_obs\n",
        "        self.data_clip = data_clip\n",
        "\n",
        "    def get_data(self,\n",
        "                 rng:random.PRNGKey,\n",
        "                 batch_size:int) -> Tuple[Tuple[jnp.ndarray]]:\n",
        "        '''\n",
        "        Gets a batch of data with resp. partial observations.\n",
        "        Args:\n",
        "            'batch_size' (int): The batch size.\n",
        "            'rng' (random.PRNGKey): The random number generator key.\n",
        "        Returns:\n",
        "            Tuple[Tuple[jnp.ndarray]]: A tuple of tuples containing the observed data and the original data in this order.\n",
        "        '''\n",
        "        return self.create_batch(rng=rng,\n",
        "                                 batch_size=batch_size,\n",
        "                                 data_dim=self.data_dim,\n",
        "                                 seq_len=self.seq_len,\n",
        "                                 eye_obs=self.eye_obs)\n",
        "\n",
        "    def get_data_info(self) -> Dict[str, any]:\n",
        "        '''Returns the data information as a dict.'''\n",
        "        k = vars(self)\n",
        "        k['vocab_size'] = self.data_dim\n",
        "        return k\n",
        "\n",
        "    def generate_sequence(self,\n",
        "                          W: jnp.ndarray,\n",
        "                          x_1: jnp.ndarray,\n",
        "                          seq_length: int,\n",
        "                          rng: random.PRNGKey) -> jnp.ndarray:\n",
        "        '''\n",
        "        Generates a sequence of tokens\n",
        "        Args:\n",
        "            'W' (ndarray): The weight matrix [D, D]\n",
        "            'x_1' (ndarray): The initial input vector [D]\n",
        "            'seq_length' (int): The length S of the sequence\n",
        "            'rng' (PRNGKey): The random number generator key for added gaussian noise\n",
        "        Returns:\n",
        "            'ndarray': The generated sequence [S, D]\n",
        "        '''\n",
        "        seq_rng  = random.split(rng, seq_length)\n",
        "        def step(prev_x, rng):\n",
        "            next_x = jnp.matmul(W, prev_x) + self.noise * random.normal(rng, shape=x_1.shape)\n",
        "            next_x = jnp.clip(next_x, -self.data_clip, self.data_clip)\n",
        "            return next_x, next_x\n",
        "        _, sequence = lax.scan(step, x_1, seq_rng[:-1])\n",
        "        sequence = jnp.concatenate([jnp.expand_dims(x_1, 0), sequence], axis=0)\n",
        "        return sequence\n",
        "\n",
        "    def _obs_and_noise(self,\n",
        "                       obs_mat: jnp.ndarray,\n",
        "                       x: jnp.ndarray,\n",
        "                       noise: jnp.ndarray) -> jnp.ndarray:\n",
        "        '''\n",
        "        Applies a linear transformation to the input matrix `mat` and vector `x`,\n",
        "        and adds noise to the result.\n",
        "        Parameters:\n",
        "            'obs_mat' (ndarray): The observation matrix [B, obs_dim, data_dim]\n",
        "            'x' (ndarray): The hidden states [B, seq_len, data_dim]\n",
        "            'noise' (ndarray): The noise vector\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Observed data with added gaussian noise\n",
        "        '''\n",
        "        return vmap(vmap(jnp.matmul,in_axes=(None,0)), in_axes=(0,0))(obs_mat, x) + noise\n",
        "\n",
        "    @partial(jit, static_argnums=(0,2,3,4,5))\n",
        "    def create_batch(self,\n",
        "                     rng: random.PRNGKey,\n",
        "                     batch_size: int,\n",
        "                     data_dim: int,\n",
        "                     seq_len: int,\n",
        "                     eye_obs: bool) -> Tuple[Tuple[jnp.ndarray]]:\n",
        "        '''\n",
        "        Creates a batch of linear sequences\n",
        "        Args:\n",
        "            'rng' (PRNGKey): The random number generator key\n",
        "            'batch_size' (int): The batch size\n",
        "            'data_dim' (int): The dimensionality of the data\n",
        "            'seq_len' (int): The length of the sequence\n",
        "            'eye_obs' (bool): Use raw hidden states as observations/inputs to model\n",
        "        Returns:\n",
        "            Tuple[ndarray]: The batch of observed data and the batch of original data\n",
        "        '''\n",
        "        rng, subkeyW, subkeyX, subkeyN1, subkeyN2, subkeyObs = random.split(rng, 6)\n",
        "        batch_of_noise_keys = random.split(subkeyN1, batch_size)\n",
        "        W = random.orthogonal(key=subkeyW,\n",
        "                              n=data_dim,\n",
        "                              shape=(batch_size,))\n",
        "        X = random.uniform(key=subkeyX,\n",
        "                           shape=(batch_size, data_dim),\n",
        "                           minval=-self.range,\n",
        "                           maxval=self.range)\n",
        "\n",
        "        dataset = vmap(partial(self.generate_sequence, seq_length=seq_len+1))(W=W,x_1=X,rng=batch_of_noise_keys)\n",
        "        original_data, original_labels = dataset[:,:-1,:], dataset[:,1:,:]\n",
        "\n",
        "        obs_mat = jnp.eye(self.obs_dim)[None, :, :].repeat(batch_size, axis=0) if eye_obs \\\n",
        "                        else 0.5*random.normal(subkeyObs, shape=(batch_size,self.obs_dim,data_dim))\n",
        "\n",
        "        new_noise = self.noise_obs * random.normal(subkeyN2, shape=(original_data.shape[0],original_data.shape[1]+1,self.obs_dim))\n",
        "        observed_data = self._obs_and_noise(obs_mat, original_data, new_noise[:,:-1,:])\n",
        "        observed_labels = self._obs_and_noise(obs_mat, original_labels, new_noise[:,1:,:])\n",
        "\n",
        "        return (observed_data, observed_labels, None), (original_data, original_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xom6MG8dugoV"
      },
      "source": [
        "### Contructed-Tokens (Wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "RXiZuHLRYXH7"
      },
      "outputs": [],
      "source": [
        "class ConstructedPartObsGenerator(DataGenerator):\n",
        "    '''Data generator for constructing data with partial observations'''\n",
        "    def __init__(self,\n",
        "                 data_generator: DataGenerator,\n",
        "                 embed_dim: int):\n",
        "        '''\n",
        "        Initializes the ConstructedPartObsGenerator.\n",
        "        Args:\n",
        "            'data_generator' (DataGenerator): The data generator.\n",
        "            'embed_dim' (int): The embedding dimension.\n",
        "        '''\n",
        "\n",
        "        self.data_generator = data_generator\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_len = data_generator.get_data_info()['seq_len']\n",
        "        self.data_dim = data_generator.get_data_info()['data_dim']\n",
        "        self.obs_dim = data_generator.get_data_info()['obs_dim']\n",
        "        self.constr = True\n",
        "        self.slots = self.embed_dim // self.obs_dim\n",
        "\n",
        "    def get_data(self,\n",
        "                 rng: random.PRNGKey,\n",
        "                 batch_size: int) -> Tuple[Tuple[jnp.ndarray]]:\n",
        "        '''\n",
        "        Gets a batch of constructed data with partial observations.\n",
        "        Args:\n",
        "            'rng' (random.PRNGKey): The random number generator key.\n",
        "            'batch_size' (int): The batch size.\n",
        "        Returns:\n",
        "            Tuple[Tuple[jnp.ndarray]]: A tuple containing the constructed data and the original data.\n",
        "        '''\n",
        "        return self.create_batch(rng=rng,\n",
        "                                 batch_size=batch_size)\n",
        "\n",
        "    def get_data_info(self) -> Dict[str, any]:\n",
        "        '''Returns the data information as a dict.'''\n",
        "        return vars(self)\n",
        "\n",
        "    @partial(jit, static_argnums=(0,2))\n",
        "    def create_batch(self,\n",
        "                     rng: random.PRNGKey,\n",
        "                     batch_size: int) -> Tuple[Tuple[jnp.ndarray]]:\n",
        "        '''\n",
        "        Creates a batch of constructed data with partial observations.\n",
        "        Args:\n",
        "            'rng' (random.PRNGKey): The random number generator key.\n",
        "            'batch_size' (int): The batch size.\n",
        "        Returns:\n",
        "            Tuple[Tuple[jnp.ndarray]]: A tuple containing the constructed data and the original data.\n",
        "        '''\n",
        "        (observed_data, observed_labels), (original_data, original_labels) = self.data_generator.get_data(rng=rng, batch_size=batch_size)\n",
        "        constructed_data = jnp.zeros(shape=(batch_size, self.seq_len, self.embed_dim))\n",
        "        constructed_data = constructed_data.at[:,:,0:self.obs_dim].set(observed_data)\n",
        "        for k in range(1, self.embed_dim // self.obs_dim):\n",
        "            shifted_data = jnp.concatenate((jnp.zeros(shape=(batch_size,(k),self.obs_dim)),observed_data[:,:-1*(k),:]),axis=1)\n",
        "            constructed_data = constructed_data.at[:,:,k*self.obs_dim:(k+1)*self.obs_dim].set(shifted_data)\n",
        "        return (constructed_data, observed_labels), (original_data, original_labels)\n",
        "\n",
        "\n",
        "class ConstructedFullSeqGenerator(DataGenerator):\n",
        "    '''Data generator for constructing data with fully observed sequences'''\n",
        "    def __init__(self,\n",
        "                 data_generator: DataGenerator,\n",
        "                 embed_dim: int,\n",
        "                 token_format):\n",
        "        '''\n",
        "        Initializes the ConstructedFullSeqGenerator.\n",
        "        Args:\n",
        "            'data_generator' (DataGenerator): The data generator.\n",
        "            'embed_dim' (int): The embedding dimension.\n",
        "        '''\n",
        "        self.data_generator = data_generator\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_len = data_generator.get_data_info()['seq_len']\n",
        "        self.data_dim = data_generator.get_data_info()['data_dim']\n",
        "        self.constr = True\n",
        "        self.slots = 4\n",
        "        self.obs_dim = data_generator.get_data_info()['obs_dim']\n",
        "        self.token_format: Literal[\"full\", \"compact\"] = token_format\n",
        "\n",
        "    @partial(jit, static_argnums=(0,2))\n",
        "    def get_data(self,\n",
        "                 rng: random.PRNGKey,\n",
        "                 batch_size: int):\n",
        "        '''\n",
        "        Gets a batch of constructed data with fully observed sequences.\n",
        "        Args:\n",
        "            'rng' (random.PRNGKey): The random number generator key.\n",
        "            'batch_size' (int): The batch size.\n",
        "        Returns:\n",
        "            Tuple[Tuple[jnp.ndarray]]: A tuple containing the constructed data and the original data.\n",
        "        '''\n",
        "        return self.create_batch(rng=rng,\n",
        "                                 batch_size=batch_size)\n",
        "\n",
        "    def get_data_info(self) -> Dict[str, any]:\n",
        "        '''\n",
        "        Gets the data information.\n",
        "        Returns:\n",
        "            Dict[str, any]: The data information.\n",
        "        '''\n",
        "        return vars(self)\n",
        "\n",
        "    @partial(jit, static_argnums=(0,2))\n",
        "    def create_batch(\n",
        "        self,\n",
        "        rng: random.PRNGKey,\n",
        "        batch_size: int,\n",
        "    ) -> Tuple[Tuple[jnp.ndarray]]:\n",
        "        '''\n",
        "        Creates a batch of tokens in either format:\n",
        "        - full: [0, 0, x_t, x_{t-1}]\n",
        "        - compact: [x_t, x_{t-1}]\n",
        "\n",
        "        Args:\n",
        "            rng (random.PRNGKey): The random number generator key\n",
        "            batch_size (int): The batch size\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tuple[jnp.ndarray]]: (constructed_data, labels), (original_data, labels)\n",
        "        '''\n",
        "\n",
        "        (observed_data, observed_labels), (original_data, original_labels) = \\\n",
        "            self.data_generator.get_data(rng=rng, batch_size=batch_size)\n",
        "\n",
        "        shifted_data = jnp.pad(\n",
        "            observed_data[:, :-1, :],\n",
        "            ((0, 0), (1, 0), (0, 0)),\n",
        "            mode='constant'\n",
        "        )\n",
        "\n",
        "        if self.token_format == \"compact\":\n",
        "            # [x_t, x_{t-1}]\n",
        "            constructed_data = jnp.concatenate(\n",
        "                [observed_data, shifted_data],\n",
        "                axis=-1\n",
        "            )\n",
        "        else:\n",
        "            # [0, 0, x_t, x_{t-1}]\n",
        "            constructed_data = jnp.concatenate([\n",
        "                jnp.zeros((batch_size, self.seq_len, 2 * self.obs_dim)),\n",
        "                observed_data,\n",
        "                shifted_data\n",
        "            ], axis=-1)\n",
        "\n",
        "        return (constructed_data, observed_labels), (original_data, original_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEHgAV2nkHfL"
      },
      "source": [
        "# Training Util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q84uzbFiav_4"
      },
      "source": [
        "## Standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_bN7WIwkkKUD"
      },
      "outputs": [],
      "source": [
        "def _compute_loss(preds: jnp.ndarray, targets: jnp.ndarray) -> jnp.ndarray:\n",
        "        '''Computes the mean squared error (MSE) loss for a batch.'''\n",
        "        assert preds.shape == targets.shape\n",
        "        bs, sl, _ = preds.shape\n",
        "        return (jnp.sum((targets -preds)**2)/(2*bs*sl))\n",
        "\n",
        "def count_parameters(params) -> int:\n",
        "    \"\"\"\n",
        "    Counts the total number of parameters in a Flax model.\n",
        "\n",
        "    Args:\n",
        "        params: Model parameters from state.params or model.init(...)['params']\n",
        "\n",
        "    Returns:\n",
        "        Total number of parameters\n",
        "    \"\"\"\n",
        "    return sum(param.size for param in jax.tree_util.tree_leaves(params))\n",
        "\n",
        "@jit\n",
        "def _compute_token_loss(logits: jnp.ndarray, targets: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Compute language modeling loss avoiding boolean indexing.\"\"\"\n",
        "    bs, sl, vocab_size = logits.shape\n",
        "    logits = logits.reshape(-1, vocab_size)\n",
        "    targets = targets.reshape(-1)\n",
        "\n",
        "    # Create float mask instead of boolean indexing\n",
        "    valid_mask = (targets != -100).astype(jnp.float32)\n",
        "\n",
        "    # Compute CE loss for all positions\n",
        "    ce_loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits,\n",
        "        jnp.maximum(targets, 0)  # Replace -100 with 0 temporarily\n",
        "    )\n",
        "\n",
        "    # Mask out invalid positions and average\n",
        "    masked_loss = ce_loss * valid_mask\n",
        "    num_valid = jnp.sum(valid_mask)\n",
        "    loss = jnp.sum(masked_loss) / (num_valid + 1e-8)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def compute_dfa_accuracy(preds, inputs, dfas, vocab, noop_token=\".\"):\n",
        "    \"\"\"Compute DFA-based accuracy for a batch of predictions.\"\"\"\n",
        "    def process_sequence(pred_seq, input_seq, dfa):\n",
        "        # Convert token IDs to characters\n",
        "        pred_chars = [vocab.get_vocab(token) for token in pred_seq]\n",
        "        input_chars = [vocab.get_vocab(token) for token in input_seq\n",
        "                      if vocab.get_vocab(token) != noop_token]\n",
        "\n",
        "        total = 0.0\n",
        "        correct = 0.0\n",
        "\n",
        "        for t in range(len(input_chars)):\n",
        "            if len(input_chars) > t + 1:\n",
        "                if input_chars[t + 1] == \"|\":\n",
        "                    continue\n",
        "                if input_chars[t + 1] == noop_token:\n",
        "                    break\n",
        "\n",
        "            if len(pred_chars) > t:\n",
        "                current_chars = input_chars[:t + 1] + [pred_chars[t]]\n",
        "                current_word = \" \".join(current_chars).split(\" | \")[-1]\n",
        "                if current_word:\n",
        "                    label = dfa(current_word)\n",
        "                    total += 1\n",
        "                    correct += label\n",
        "\n",
        "        return correct, total\n",
        "\n",
        "    # Process each sequence in the batch\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "    for b in range(len(dfas)):\n",
        "        correct, total = process_sequence(preds[b], inputs[b], dfas[b])\n",
        "        total_correct += correct\n",
        "        total_count += total\n",
        "\n",
        "    return total_correct / (total_count + 1e-8)\n",
        "\n",
        "\n",
        "def compute_icl_dfa_accuracy(self, predictions, inputs, dfas, vocab):\n",
        "    \"\"\"\n",
        "    Compute accuracy for in-context learning DFA task.\n",
        "    Each sequence contains multiple examples separated by '|'.\n",
        "    We evaluate if the predicted continuation follows the DFA rules after seeing previous examples.\n",
        "    \"\"\"\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    # Convert to numpy for CPU processing\n",
        "    preds_np = np.array(predictions)\n",
        "    inputs_np = np.array(inputs)\n",
        "\n",
        "    for b in range(len(dfas)):\n",
        "        dfa = dfas[b]\n",
        "        # Get character sequences\n",
        "        pred_chars = [vocab.get_vocab(token) for token in preds_np[b]]\n",
        "        input_chars = [vocab.get_vocab(token) for token in inputs_np[b]\n",
        "                      if vocab.get_vocab(token) != \".\"]  # Remove padding\n",
        "\n",
        "        # Process each position in the sequence\n",
        "        for t in range(len(input_chars)):\n",
        "            # Skip if we're at the end or next token isn't relevant\n",
        "            if len(input_chars) > t + 1:\n",
        "                if input_chars[t + 1] == \"|\":  # Skip separator\n",
        "                    continue\n",
        "                if input_chars[t + 1] == \".\":  # End of sequence\n",
        "                    break\n",
        "\n",
        "            if len(pred_chars) > t:\n",
        "                # Get the current sequence context and prediction\n",
        "                current_chars = input_chars[:t + 1] + [pred_chars[t]]\n",
        "                # Take only the last example (after last separator)\n",
        "                current_word = \" \".join(current_chars).split(\" | \")[-1]\n",
        "                if current_word:\n",
        "                    # Check if prediction follows DFA rules\n",
        "                    label = int(dfa(current_word))\n",
        "                    total_count += 1\n",
        "                    total_correct += label\n",
        "\n",
        "    accuracy = total_correct / (total_count + 1e-8)\n",
        "    return accuracy\n",
        "\n",
        "class Training:\n",
        "    '''Class for training the transformer model.'''\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: flax.linen.Module,\n",
        "        optimizer: optax.GradientTransformation,\n",
        "        data_generator: DataGenerator,\n",
        "        batch_size: int,\n",
        "        test_batch_size: int,\n",
        "        task_type: str = 'continuous'  # 'continuous' or 'discrete'\n",
        "    ):\n",
        "        '''\n",
        "        Initializes the training class with the specified parameters.\n",
        "        Args:\n",
        "            'model' (flax.linen.Module): The transformer model.\n",
        "            'optimizer' (optax.GradientTransformation): The optimizer.\n",
        "            'data_generator' (DataGenerator): The data generator.\n",
        "            'batch_size' (int): The batch size for training.\n",
        "            'test_batch_size' (int): The batch size for testing.\n",
        "            'task_type' (str): The type of the task, either 'continuous' or 'discrete'.\n",
        "        '''\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.data_generator = data_generator\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.task_type = task_type\n",
        "\n",
        "        # Get data info based on task type\n",
        "        data_info = data_generator.get_data_info()\n",
        "        if task_type == 'continuous':\n",
        "            self.obs_dim = data_info['obs_dim']\n",
        "        else:  # discrete\n",
        "            self.vocab_size = data_info['vocab_size']\n",
        "        self.obs_dim = data_generator.get_data_info()['obs_dim']\n",
        "\n",
        "    def get_init_state(self,\n",
        "                       rng: random.PRNGKey) -> Tuple[train_state.TrainState, random.PRNGKey]:\n",
        "        '''\n",
        "        Initializes the training state with the specified random number generator key.\n",
        "        Args:\n",
        "            'rng' (jax.random.PRNGKey): The random number generator key.\n",
        "        '''\n",
        "        rng, ex_rng, init_rng = random.split(rng, 3)\n",
        "        (exmp_inp, _, _), _ = self.data_generator.get_data(rng=ex_rng, batch_size=self.batch_size)\n",
        "        params = self.model.init({'params': init_rng}, exmp_inp)['params']\n",
        "        state_init = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=self.optimizer)\n",
        "        return state_init, rng\n",
        "\n",
        "    def batch_to_input(self, batch: Tuple[jnp.ndarray, jnp.ndarray]) -> jnp.ndarray:\n",
        "        '''Extracts the input data from the batch.'''\n",
        "        data, _ = batch\n",
        "        return data\n",
        "\n",
        "#    def calculate_loss(\n",
        "#        self,\n",
        "#        params: flax.core.frozen_dict.FrozenDict,\n",
        "#        batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "#    ) -> Tuple[jnp.ndarray, Tuple[any]]:\n",
        "#        '''\n",
        "#        Calculates the differentiable loss function.\n",
        "#        Args:\n",
        "#            'params' (flax.core.frozen_dict.FrozenDict): The model parameters.\n",
        "#            'batch' (Tuple[jnp.ndarray, jnp.ndarray]): The input batch.\n",
        "#        Returns:\n",
        "#            Tuple[jnp.ndarray, Tuple[any]] 'loss' (jnp.ndarray): The computed loss and 'tf_data' (Tuple[any]): The data from the transformer forward pass.\n",
        "#        '''\n",
        "#        inp_data, labels = batch\n",
        "#        logits, tf_data = self.model.apply({'params': params}, inp_data)\n",
        "#        preds = logits[:, :, :self.obs_dim]\n",
        "#        loss = _compute_loss(preds=preds, targets=labels)\n",
        "#        return loss, tf_data\n",
        "\n",
        "    def calculate_loss(\n",
        "        self,\n",
        "        params: flax.core.frozen_dict.FrozenDict,\n",
        "        batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "        ) -> Tuple[jnp.ndarray, Tuple[any]]:\n",
        "        \"\"\"Calculate loss and return logits for evaluation.\"\"\"\n",
        "        inp_data, labels = batch\n",
        "        logits, tf_data = self.model.apply({'params': params}, inp_data)\n",
        "\n",
        "        if self.task_type == 'continuous':\n",
        "            preds = logits[:, :, :self.obs_dim]\n",
        "            loss = _compute_loss(preds=preds, targets=labels)\n",
        "        else:  # discrete\n",
        "            loss = _compute_token_loss(logits=logits, targets=labels)\n",
        "\n",
        "        return loss, (logits, tf_data)\n",
        "\n",
        "    @partial(jit, static_argnums=(0))\n",
        "    def fast_train_step(\n",
        "        self,\n",
        "        state: train_state.TrainState,\n",
        "        batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "    ) -> Tuple[train_state.TrainState, jnp.ndarray, Tuple[any]]:\n",
        "        '''\n",
        "        Performs a single training step.\n",
        "        Args:\n",
        "            'state' (flax.training.train_state.TrainState): The current training state.\n",
        "            'batch' (Tuple[jnp.ndarray, jnp.ndarray]): The input batch.\n",
        "        Returns:\n",
        "            Tuple[train_state.TrainState, jnp.ndarray, Tuple[any]]:\n",
        "                - 'state' (flax.training.train_state.TrainState): The updated training state,\n",
        "                - 'loss' (jnp.ndarray): The computed loss,\n",
        "                - 'tf_data' (Tuple[any]): The data from the transformer forward pass.\n",
        "        '''\n",
        "        loss_fn = lambda params: self.calculate_loss(params=params, batch=batch)\n",
        "        (loss, (tf_data)), grads = value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "        state = state.apply_gradients(grads=grads)\n",
        "        return state, loss, tf_data\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def _compute_step_outputs(\n",
        "        self,\n",
        "        params: flax.core.frozen_dict.FrozenDict,\n",
        "        inputs: jnp.ndarray,\n",
        "        targets: jnp.ndarray\n",
        "    ):\n",
        "        \"\"\"JIT-compiled part of the evaluation step\"\"\"\n",
        "        logits, tf_data = self.model.apply({'params': params}, inputs)\n",
        "\n",
        "        # Ensure predictions stay within vocab range\n",
        "        logits = jnp.clip(logits, -1e7, 1e7)  # Prevent overflow in softmax\n",
        "        loss = _compute_token_loss(logits=logits, targets=targets)\n",
        "        preds = jnp.argmax(logits, axis=-1)\n",
        "\n",
        "        # Clip predictions to valid vocab range\n",
        "        preds = jnp.clip(preds, 0, self.vocab_size - 1)\n",
        "\n",
        "        return loss, preds\n",
        "\n",
        "\n",
        "#    def fast_eval_step(\n",
        "#      self,\n",
        "#      params: flax.core.frozen_dict.FrozenDict,\n",
        "#      batch: Tuple\n",
        "#    ):\n",
        "#        \"\"\"Evaluation step with in-context learning accuracy.\"\"\"\n",
        "#        (inputs, targets, dfas) = batch[0]#\n",
        "#\n",
        "#        # JAX computations\n",
        "#        loss, preds, logits = self._compute_step_outputs(params, inputs, targets)\n",
        "#\n",
        "#        if self.task_type == 'discrete':\n",
        "#            # Compute in-context learning accuracy\n",
        "#            accuracy = self.compute_icl_dfa_accuracy(\n",
        "#                predictions=preds,\n",
        "#                inputs=inputs,\n",
        "#                dfas=dfas,\n",
        "#                vocab=self.data_generator.data_module.vocab\n",
        "#            )\n",
        "#            return loss, accuracy\n",
        "#\n",
        "#        return loss, 0.0\n",
        "\n",
        "    #@partial(jit, static_argnums=(0,))\n",
        "    def fast_eval_step(\n",
        "        self,\n",
        "        params: flax.core.frozen_dict.FrozenDict,\n",
        "        batch: Tuple\n",
        "    ):\n",
        "        \"\"\"Evaluation step split into JAX and Python parts\"\"\"\n",
        "        (inputs, targets, dfas) = batch[0]  # Unpack from the first tuple\n",
        "\n",
        "        # JAX computations\n",
        "        loss, preds = self._compute_step_outputs(params, inputs, targets)\n",
        "\n",
        "        if self.task_type == 'discrete':\n",
        "            # Convert to numpy for DFA processing\n",
        "            preds_np = np.array(preds)\n",
        "            inputs_np = np.array(inputs)\n",
        "\n",
        "            # Python-based DFA accuracy computation\n",
        "            total_correct = 0\n",
        "            total_count = 0\n",
        "            vocab = self.data_generator.data_module.vocab\n",
        "\n",
        "            for b in range(len(dfas)):\n",
        "                try:\n",
        "                    dfa = dfas[b]\n",
        "                    # Add safety checks for token IDs\n",
        "                    pred_chars = []\n",
        "                    for token in preds_np[b]:\n",
        "                        if 0 <= token < len(vocab.vocab):\n",
        "                            pred_chars.append(vocab.get_vocab(token))\n",
        "                        else:\n",
        "                            print(f\"Warning: Invalid prediction token ID: {token}\")\n",
        "                            pred_chars.append(vocab.noop)  # Use noop token for invalid predictions\n",
        "\n",
        "                    input_chars = []\n",
        "                    for token in inputs_np[b]:\n",
        "                        if 0 <= token < len(vocab.vocab):\n",
        "                            char = vocab.get_vocab(token)\n",
        "                            if char != \".\":\n",
        "                                input_chars.append(char)\n",
        "                        else:\n",
        "                            print(f\"Warning: Invalid input token ID: {token}\")\n",
        "\n",
        "                    for t in range(len(input_chars)):\n",
        "                        if len(input_chars) > t + 1:\n",
        "                            if input_chars[t + 1] == \"|\":\n",
        "                                continue\n",
        "                            if input_chars[t + 1] == \".\":\n",
        "                                break\n",
        "\n",
        "                        if len(pred_chars) > t:\n",
        "                            current_chars = input_chars[:t + 1] + [pred_chars[t]]\n",
        "                            current_word = \" \".join(current_chars).split(\" | \")[-1]\n",
        "                            if current_word:\n",
        "                                label = int(dfa(current_word))\n",
        "                                total_count += 1\n",
        "                                total_correct += label\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing batch {b}: {e}\")\n",
        "                    print(f\"Predictions shape: {preds_np.shape}, values: {preds_np[b]}\")\n",
        "                    print(f\"Inputs shape: {inputs_np.shape}, values: {inputs_np[b]}\")\n",
        "                    print(f\"Vocab size: {len(vocab.vocab)}\")\n",
        "\n",
        "            accuracy = total_correct / (total_count + 1e-8)\n",
        "            return loss, accuracy\n",
        "\n",
        "        return loss, 0.0\n",
        "\n",
        "\n",
        "    @partial(jit, static_argnums=(0))\n",
        "    def fast_pure_test_computation(self,\n",
        "                                   params: flax.core.frozen_dict.FrozenDict,\n",
        "                                   test_rng: random.PRNGKey) -> jnp.ndarray:\n",
        "        '''\n",
        "        Performs a full evaluation computation for performance evaluation of RevAlgs.\n",
        "        Args:\n",
        "            'params' (flax.core.frozen_dict.FrozenDict): The model parameters.\n",
        "            'test_rng' (jax.random.PRNGKey): The random number generator key for testing.\n",
        "        Returns:\n",
        "            'test_loss' (jnp.ndarray): The computed test loss.\n",
        "        '''\n",
        "        test_loss = 0\n",
        "        for _ in range(10):\n",
        "            test_rng, batch_rng = random.split(test_rng, 2)\n",
        "            batch_TEST, _ = self.data_generator.get_data(rng=batch_rng, batch_size=self.test_batch_size)\n",
        "            (step_loss, _) = self.calculate_loss(params=params, batch=batch_TEST)\n",
        "            test_loss += step_loss\n",
        "        return test_loss/10\n",
        "\n",
        "    @partial(jit, static_argnums=(0))\n",
        "    def fast_sensitivity(self,\n",
        "                         batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "                         state: train_state.TrainState) -> Tuple[List[jnp.ndarray]]:\n",
        "        '''\n",
        "        Performs sensitivity analysis.\n",
        "        Args:\n",
        "            'batch' (Tuple[jnp.ndarray, jnp.ndarray]): The input batch.\n",
        "            'state' (flax.training.train_state.TrainState): The current training state.\n",
        "        Returns:\n",
        "            Tuple[List[jnp.ndarray]] ('listsnd', 'listmid', 'listlast'): The sensitivity analysis results for the second token, mid and last token in a sequence.\n",
        "        '''\n",
        "        _,s,_ = batch[0].shape\n",
        "        target_k = [1,(s-1)//2,s-1]\n",
        "        res_list = []\n",
        "        for k in target_k:\n",
        "            grad_of_output_l_wrt_x = lambda l: vmap(grad(lambda x: self.model.apply({'params': state.params}, x[None, ...])[1][0][1][0][k][l],         #1: second output, 0: activations, 1:layer,\n",
        "                                                argnums=0))(batch[0])\n",
        "            grads = vmap(lambda t: jnp.mean(jnp.linalg.norm(grad_of_output_l_wrt_x(t), axis=(2)),axis=0)[:k+1])(jnp.arange(self.model.embed_dim))\n",
        "            grads_norm = jnp.mean(jnp.array(grads), axis=0)\n",
        "            res_list.append(grads_norm)\n",
        "        return tuple(res_list)\n",
        "\n",
        "    def train_epoch(\n",
        "        self,\n",
        "        epoch: int,\n",
        "        rng: random.PRNGKey,\n",
        "        test_rng: random.PRNGKey,\n",
        "        state: train_state.TrainState,\n",
        "        num_batches_train: int,\n",
        "    ):\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "        rng, tr_rng = random.split(rng, 2)\n",
        "\n",
        "        if epoch == 0:\n",
        "          num_params = count_parameters(state.params)\n",
        "          print(f\"Model has {num_params:,} parameters\")\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss = 0\n",
        "        test_acc = 0\n",
        "        num_test_batches = 10\n",
        "        for _ in range(num_test_batches):\n",
        "            test_rng, batch_rng = random.split(test_rng, 2)\n",
        "            batch_TEST = self.data_generator.get_test_data(batch_size=self.test_batch_size, rng=test_rng)\n",
        "            step_loss, step_acc = self.fast_eval_step(state.params, batch=batch_TEST)\n",
        "            test_loss += step_loss\n",
        "            test_acc += step_acc\n",
        "\n",
        "        test_loss = test_loss / num_test_batches\n",
        "        test_acc = test_acc / num_test_batches\n",
        "\n",
        "        # Train\n",
        "        for _ in jnp.arange(num_batches_train):\n",
        "            tr_rng, batch_rng = random.split(tr_rng, 2)\n",
        "            batch = self.data_generator.get_data(rng=batch_rng, batch_size=self.batch_size)\n",
        "            state, _, _ = self.fast_train_step(state, batch=batch[0][:2])  # Only pass inputs and targets for training\n",
        "\n",
        "        if self.task_type == 'discrete':\n",
        "            print(f'Epoch {epoch}: loss = {test_loss:.4f}, accuracy = {test_acc:.4f}')\n",
        "        else:\n",
        "            print(f'Epoch {epoch}: loss = {test_loss:.4f}')\n",
        "\n",
        "        return state, rng, test_loss, test_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kqUMi4mSIgx"
      },
      "source": [
        "## Language (NOT Regbench)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LDh03Z9ESMCx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "@jit\n",
        "def _compute_token_loss_and_metrics(logits: jnp.ndarray, targets: jnp.ndarray):\n",
        "    \"\"\"Compute cross entropy loss and perplexity with careful masking.\"\"\"\n",
        "    bs, sl, vocab_size = logits.shape\n",
        "    logits = logits.reshape(-1, vocab_size)\n",
        "    targets = targets.reshape(-1)\n",
        "\n",
        "    # Create mask and ensure targets are valid indices\n",
        "    valid_mask = (targets != -100)\n",
        "    safe_targets = jnp.where(valid_mask, targets, 0)\n",
        "\n",
        "    # Compute per-token losses\n",
        "    ce_losses = optax.softmax_cross_entropy_with_integer_labels(logits, safe_targets)\n",
        "\n",
        "    # Mask losses and get mean\n",
        "    masked_losses = ce_losses * valid_mask\n",
        "    num_valid = jnp.sum(valid_mask)\n",
        "    loss = jnp.sum(masked_losses) / (num_valid + 1e-8)\n",
        "\n",
        "    # Compute perplexity\n",
        "    perplexity = jnp.exp(jnp.where(jnp.isfinite(loss), loss, 100.0))\n",
        "\n",
        "    # Compute accuracy\n",
        "    predictions = jnp.argmax(logits, axis=-1)\n",
        "    correct = (predictions == targets) * valid_mask\n",
        "    accuracy = jnp.sum(correct) / (num_valid + 1e-8)\n",
        "\n",
        "    return loss, perplexity, accuracy\n",
        "'''\n",
        "\n",
        "@jit\n",
        "def _compute_token_loss_and_metrics(logits: jnp.ndarray, targets: jnp.ndarray):\n",
        "    \"\"\"Basic cross entropy loss computation without any masking.\"\"\"\n",
        "    # Ensure numerical stability in the logits\n",
        "    logits = jnp.clip(logits, -1e4, 1e4)\n",
        "\n",
        "    # Debug info\n",
        "    print(\"Logits stats:\", jnp.min(logits), jnp.max(logits), jnp.mean(logits))\n",
        "    print(\"Targets range:\", jnp.min(targets), jnp.max(targets))\n",
        "\n",
        "    # Reshape\n",
        "    bs, sl, vocab_size = logits.shape\n",
        "    # logits = logits.reshape(-1, vocab_size)\n",
        "    # targets = targets.reshape(-1)\n",
        "\n",
        "    valid_mask = (targets != -100)\n",
        "\n",
        "\n",
        "    # Simple cross entropy\n",
        "    ce_loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "    print('LOGITS SHAPE: ', targets.shape)\n",
        "    print('TARGETS SHAPE: ', targets.shape)\n",
        "    # loss = jnp.mean(ce_loss)\n",
        "    masked_losses = ce_loss * valid_mask\n",
        "    num_valid = jnp.sum(valid_mask)\n",
        "    loss = jnp.sum(masked_losses) / (num_valid + 1e-8)\n",
        "\n",
        "    # Basic accuracy\n",
        "    predictions = jnp.argmax(logits, axis=-1)\n",
        "    accuracy = jnp.mean(predictions == targets)\n",
        "\n",
        "    return loss, jnp.exp(loss), accuracy\n",
        "\n",
        "class LanguageModelTraining(Training):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    @partial(jit, static_argnums=(0))\n",
        "    def _compute_forward_pass(self, params, inp_data):\n",
        "        \"\"\"JIT-compiled forward pass.\"\"\"\n",
        "        return self.model.apply({'params': params}, inp_data)\n",
        "\n",
        "    @partial(jit, static_argnums=(0))\n",
        "    def calculate_loss(self,\n",
        "                      params: flax.core.frozen_dict.FrozenDict,\n",
        "                      batch: Tuple[jnp.ndarray, jnp.ndarray]) -> Tuple[jnp.ndarray, Tuple[any]]:\n",
        "        \"\"\"Calculate loss and metrics for language modeling.\"\"\"\n",
        "        inp_data, labels = batch\n",
        "        print('CALC LOSS FUN inp format: ', inp_data.shape)\n",
        "        logits, tf_data = self._compute_forward_pass(params, inp_data)\n",
        "        print('CALC LOSS FUN !!!!!!! logits shape: ', logits.shape)\n",
        "        loss, perplexity, accuracy = _compute_token_loss_and_metrics(logits, labels)\n",
        "        return loss, (logits, perplexity, accuracy, tf_data)\n",
        "\n",
        "    @partial(jit, static_argnums=(0))\n",
        "    def fast_train_step(self,\n",
        "                       state: train_state.TrainState,\n",
        "                       batch: Tuple[jnp.ndarray, jnp.ndarray]) -> Tuple[train_state.TrainState, jnp.ndarray, Tuple[any]]:\n",
        "        \"\"\"Single training step.\"\"\"\n",
        "        loss_fn = lambda params: self.calculate_loss(params=params, batch=batch)\n",
        "        (loss, aux), grads = value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "        state = state.apply_gradients(grads=grads)\n",
        "        return state, loss, aux\n",
        "\n",
        "    @partial(jit, static_argnums=(0))\n",
        "    def eval_step(self,\n",
        "                 params: flax.core.frozen_dict.FrozenDict,\n",
        "                 batch: Tuple[jnp.ndarray, jnp.ndarray]) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
        "        \"\"\"Single evaluation step.\"\"\"\n",
        "        print('eval step targets shape: ', batch[1].shape)\n",
        "        loss, (_, perplexity, accuracy, _) = self.calculate_loss(params, batch)\n",
        "        return loss, perplexity, accuracy\n",
        "\n",
        "    def train_epoch(self,\n",
        "                   epoch: int,\n",
        "                   rng: random.PRNGKey,\n",
        "                   test_rng: random.PRNGKey,\n",
        "                   state: train_state.TrainState,\n",
        "                   num_batches_train: int):\n",
        "        \"\"\"Train for one epoch with language model specific metrics.\"\"\"\n",
        "        rng, tr_rng = random.split(rng, 2)\n",
        "\n",
        "        if epoch == 0:\n",
        "            num_params = count_parameters(state.params)\n",
        "            print(f\"Model has {num_params:,} parameters\")\n",
        "\n",
        "        # Evaluate\n",
        "        test_metrics = {'loss': 0.0, 'perplexity': 0.0, 'accuracy': 0.0}\n",
        "        num_test_batches = 10\n",
        "\n",
        "        for _ in range(num_test_batches):\n",
        "            test_rng, batch_rng = random.split(test_rng, 2)\n",
        "            batch_TEST = self.data_generator.get_test_data(batch_size=self.test_batch_size, rng=test_rng)\n",
        "            loss, perplexity, accuracy = self.eval_step(\n",
        "                state.params,\n",
        "                batch=batch_TEST[0][:2]  # Only take inputs and targets\n",
        "            )\n",
        "            test_metrics['loss'] += loss\n",
        "            test_metrics['perplexity'] += perplexity\n",
        "            test_metrics['accuracy'] += accuracy\n",
        "\n",
        "        # Average metrics\n",
        "        for k in test_metrics:\n",
        "            test_metrics[k] /= num_test_batches\n",
        "\n",
        "        # Train\n",
        "        train_loss = 0\n",
        "        for step in range(num_batches_train):\n",
        "            tr_rng, batch_rng = random.split(tr_rng, 2)\n",
        "            batch = self.data_generator.get_data(rng=batch_rng, batch_size=self.batch_size)\n",
        "            state, train_loss, _ = self.fast_train_step(state, batch=batch[0][:2])\n",
        "\n",
        "        train_loss = train_loss / num_batches_train\n",
        "\n",
        "        l = test_metrics['loss']\n",
        "        a = test_metrics['accuracy']\n",
        "        p = test_metrics['perplexity']\n",
        "\n",
        "        print(f'Epoch {epoch}: loss = {l:.4f}, accuracy = {a:.4f}, perplexity = {p:.4f}')\n",
        "        print('Train loss: ', train_loss)\n",
        "\n",
        "        return state, rng, l, a, p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckkYkU2KN6lQ"
      },
      "source": [
        "# Experiment Util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYD11ZqwOH0E"
      },
      "source": [
        "## Aux models (old code, be careful)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nJEQYnP7OHPV"
      },
      "outputs": [],
      "source": [
        "\n",
        "#################################################\n",
        "#                                               #\n",
        "#                                               #\n",
        "#       Implementation of Matrix-Inv.           #\n",
        "#                Approximators                  #\n",
        "#                                               #\n",
        "#                                               #\n",
        "#################################################\n",
        "\n",
        "def invert_matrix_neumann(A: jnp.ndarray,\n",
        "                          steps: int,\n",
        "                          norm: float) -> jnp.ndarray:\n",
        "    '''\n",
        "    Function to approximate a matrix inverse using a truncated Neumann series.\n",
        "    Args:\n",
        "        'A' (jnp.ndarray): The matrix to be inverted.\n",
        "        'steps' (int): The number of steps for the Neumann series.\n",
        "        'norm' (float): The norm-scalar for enabling a neumann matrix approximation.\n",
        "    Returns:\n",
        "        jnp.ndarray: The approximation of the inverted matrix.\n",
        "    '''\n",
        "    n = A.shape[0]\n",
        "    A = A / norm\n",
        "    I = jnp.eye(n)\n",
        "    diff = I - A\n",
        "    inverse_approx = I\n",
        "    term = I\n",
        "    for _ in range(steps):\n",
        "        term = term @ diff\n",
        "        inverse_approx += term\n",
        "    return inverse_approx/norm\n",
        "\n",
        "def batched_neumann(steps: int,\n",
        "                    norm: float) -> Callable[[jnp.ndarray], jnp.ndarray]:\n",
        "    '''Function to batch the Neumann series approximation of a matrix inverse.'''\n",
        "    return jax.vmap(partial(invert_matrix_neumann,\n",
        "                            steps=steps,\n",
        "                            norm=norm),\n",
        "                    in_axes=(0))\n",
        "\n",
        "def invert_matrix_newton(A: jnp.ndarray,\n",
        "                         steps: int) -> jnp.ndarray:\n",
        "    '''\n",
        "    Function to approximate a matrix inverse using Newton's method.\n",
        "    Args:\n",
        "        'A' (jnp.ndarray): The matrix to be inverted.\n",
        "        'steps' (int): The number of steps for Newton's method.\n",
        "    Returns:\n",
        "        jnp.ndarray: The approximation of the inverted matrix.\n",
        "    '''\n",
        "    n = A.shape[0]\n",
        "    X = jnp.eye(n) / jnp.trace(A)\n",
        "    for _ in range(steps):\n",
        "        AX = jnp.dot(A, X)\n",
        "        X = X @ (2 * jnp.eye(n) - AX)\n",
        "    return X\n",
        "\n",
        "def batched_newton(steps: int) -> Callable[[jnp.ndarray], jnp.ndarray]:\n",
        "    '''Function to batch the Newton's method approximation of a matrix inverse.'''\n",
        "    return jax.vmap(partial(invert_matrix_newton,\n",
        "                            steps=steps),\n",
        "                    in_axes=(0))\n",
        "\n",
        "def invert_matrix_chebyshev(A: jnp.ndarray,\n",
        "                            steps: int,\n",
        "                            alphas: jnp.ndarray,\n",
        "                            betas: jnp.ndarray) -> jnp.ndarray:\n",
        "    norm = jnp.linalg.norm(A)\n",
        "    n = A.shape[0]\n",
        "    A = A/norm\n",
        "    I = jnp.eye(n)\n",
        "    diff = I - A\n",
        "    inverse_approx = I\n",
        "    term = I\n",
        "    prev = I\n",
        "    for alpha, beta in zip(alphas, betas):\n",
        "        diff = I - alpha*A\n",
        "        term = term@diff\n",
        "        term_momentum = beta*(inverse_approx - prev)\n",
        "        prev = inverse_approx\n",
        "        inverse_approx = inverse_approx + term + term_momentum\n",
        "    return inverse_approx/norm\n",
        "\n",
        "def batched_chebyshev(steps: int,\n",
        "                      alphas: List[float],\n",
        "                      betas: List[float]) -> Callable[[jnp.ndarray], jnp.ndarray]:\n",
        "    return jax.vmap(partial(invert_matrix_chebyshev,\n",
        "                            steps=steps,\n",
        "                            alphas=alphas,\n",
        "                            betas=betas),\n",
        "                    in_axes=(0))\n",
        "\n",
        "def invert_matrix_richardson(A, omegas):\n",
        "    norm = jnp.linalg.norm(A)\n",
        "    A = A/norm\n",
        "    I = jnp.eye(A.shape[0])\n",
        "    diff = I - A\n",
        "    inverse_approx = I\n",
        "    term = I\n",
        "\n",
        "    for omega in omegas:\n",
        "        diff = I - omega*A\n",
        "        term = term@diff\n",
        "        inverse_approx = inverse_approx + term\n",
        "    return inverse_approx/norm\n",
        "\n",
        "def batched_richardson(omegas):\n",
        "    return jax.vmap(partial(invert_matrix_richardson,\n",
        "                            omegas=omegas),\n",
        "                    in_axes=0)\n",
        "\n",
        "#################################################\n",
        "#                                               #\n",
        "#                                               #\n",
        "#             Learn parameters for              #\n",
        "#            chebyshev-inverse-apx.             #\n",
        "#                                               #\n",
        "#                                               #\n",
        "#################################################\n",
        "\n",
        "def learn_parameters_chebyshev(num_steps: int,\n",
        "                               train_len: int,\n",
        "                               experiment_config: config_dict.ConfigDict,\n",
        "                               data_generator: DataGenerator,\n",
        "                               part_obs_constr: bool = False,\n",
        "                               part_obs_embed_dim: int = 80,\n",
        "                               seq_len: int = 50,\n",
        "                               use_mlp: bool = False,\n",
        "                               init_alphas: jnp.ndarray = None,\n",
        "                               init_betas: jnp.ndarray = None) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    '''Training logic for GD to learn optimal parameters for Chebyshev, evaluated using Sequencesolver on autoregressive tasks of choice.'''\n",
        "    def single_cheb_pred(params_s, seq_data_s, seq_labels_s, lamb_s):\n",
        "            preds = []\n",
        "            for token in range(seq_len):\n",
        "                inv_mat = seq_data_s[:token].T@seq_data_s[:token] + lamb_s*jnp.eye(seq_data_s[:token].shape[1])\n",
        "                w_hat = invert_matrix_chebyshev(A=inv_mat,\n",
        "                                            steps=num_steps,\n",
        "                                            alphas=params_s['params']['alphas'],\n",
        "                                             betas=params_s['params']['betas']) @ (seq_data_s[:token].T@seq_labels_s[:token])\n",
        "                if use_mlp:\n",
        "                    test_token = data_generator._mini_mlp(seq_labels_s[token])\n",
        "                    test_token = test_token/(jnp.linalg.norm(test_token)+1e-16)\n",
        "                else:\n",
        "                    test_token = seq_labels_s[token]\n",
        "                seq_label_hat = jnp.matmul(test_token, w_hat)\n",
        "                preds.append(seq_label_hat)\n",
        "            return jnp.array(preds)\n",
        "\n",
        "    def cheb_lsq_pred(params, seq_data, seq_labels, lamb):\n",
        "        vectorized_cheb_pred = jax.vmap(single_cheb_pred, in_axes=(None, 0, 0, None))(params, seq_data, seq_labels, lamb)\n",
        "        return vectorized_cheb_pred\n",
        "\n",
        "    def get_features(batch: jnp.ndarray) -> jnp.ndarray:\n",
        "        feature_seq_func = lambda seq : jax.vmap(data_generator._mini_mlp)(seq)\n",
        "        feature_batch = jax.vmap(feature_seq_func, in_axes=(0))(batch)\n",
        "        return feature_batch\n",
        "\n",
        "    def cheb_loss(params, rng):\n",
        "        rng, batch_rng = jax.random.split(rng)\n",
        "        batch, _ = data_generator.get_data(rng=batch_rng, batch_size=experiment_config.data.batch_size)\n",
        "        data, labels = batch\n",
        "        if part_obs_constr:\n",
        "            batch_size, seq_len, obs_dim = data.shape\n",
        "            constructed_data = jnp.zeros(shape=(batch_size, seq_len, part_obs_embed_dim))\n",
        "            constructed_data = constructed_data.at[:,:,0:obs_dim].set(data)\n",
        "            for k in range(1, part_obs_embed_dim//obs_dim):\n",
        "                shifted_data = jnp.concatenate((jnp.zeros(shape=(batch_size,(k),obs_dim)),data[:,:-1*(k),:]),axis=1)\n",
        "                constructed_data = constructed_data.at[:,:,k*obs_dim:(k+1)*obs_dim].set(shifted_data)\n",
        "            shifted_data = jnp.concatenate([jnp.expand_dims(constructed_data[:, 0, :], 1)*0, constructed_data], axis=1)[:, :-1, :]\n",
        "            data = constructed_data\n",
        "            preds_chebyshev = cheb_lsq_pred(params, shifted_data, data, 0.001)[:,:,0:obs_dim]\n",
        "        elif use_mlp:\n",
        "            dat_feat = get_features(data)\n",
        "            dat_feat /= (jnp.linalg.norm(dat_feat,axis=-1)[...,None])\n",
        "            shifted_data = jnp.concatenate([jnp.expand_dims(dat_feat[:, 0, :], 1)*0, dat_feat], axis=1)[:, :-1, :]\n",
        "            preds_chebyshev = cheb_lsq_pred(params, shifted_data, data/(jnp.linalg.norm(data, axis=-1)[...,None]), 0.001)\n",
        "        else:\n",
        "            shifted_data = jnp.concatenate([jnp.expand_dims(data[:, 0, :], 1)*0, data], axis=1)[:, :-1, :]\n",
        "            preds_chebyshev = cheb_lsq_pred(params, shifted_data, data, 0.001)\n",
        "        loss = _compute_loss(preds=preds_chebyshev, targets=labels)\n",
        "        return loss, rng\n",
        "\n",
        "    def cheb_train_step(state, rng):\n",
        "        loss_fn = lambda params: cheb_loss(params=params, rng=rng)\n",
        "        (loss, rng), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "        state = state.apply_gradients(grads=grads)\n",
        "        return state, (loss, rng)\n",
        "    fast_cheb_train_step = jax.jit(cheb_train_step)\n",
        "\n",
        "    def cheb_training(state):\n",
        "        for i in range(train_len):\n",
        "            rng=jax.random.PRNGKey(seed=i)\n",
        "            state, (loss,_) = fast_cheb_train_step(state, rng)\n",
        "            print(loss)\n",
        "            if i % 1000 == 0:\n",
        "                print(state.params)\n",
        "        return state\n",
        "\n",
        "    init_params = {'params': {}}\n",
        "    init_params['params']['alphas'] = jnp.ones(shape=(num_steps,)) if init_alphas == None else init_alphas\n",
        "    init_params['params']['betas'] = jnp.zeros(shape=(num_steps,)) if init_betas == None else init_betas\n",
        "\n",
        "    optimizer = Optimizer().get_optimizer()\n",
        "\n",
        "    state_cheb = train_state.TrainState.create(apply_fn=cheb_lsq_pred, params=init_params, tx=optimizer)\n",
        "    state_cheb = cheb_training(state_cheb)\n",
        "\n",
        "    return state_cheb.params['params']['alphas'], state_cheb.params['params']['betas']\n",
        "\n",
        "\n",
        "#################################################\n",
        "#                                               #\n",
        "#                                               #\n",
        "#       Implementation of Aux-Models            #\n",
        "#                                               #\n",
        "#                                               #\n",
        "#################################################\n",
        "\n",
        "class AuxModel(metaclass=abc.ABCMeta):\n",
        "    '''Abstract Base Class for auxiliary models'''\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def predict(self, shifted_data, data):\n",
        "        '''Abstract method to get prediction for data batch'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "class LeastSquaresSequenceSolver(AuxModel):\n",
        "    '''Class implementing a least squares solver for sequence data.'''\n",
        "    def __init__(self,\n",
        "                 approximator: str,\n",
        "                 seq_len: int,\n",
        "                 apx_steps: int = 6,\n",
        "                 apx_norm: float = 70,\n",
        "                 lamb: float = 0.001,\n",
        "                 use_mlp: bool = False,\n",
        "                 mlp_fn = None,\n",
        "                 alphas = None,\n",
        "                 betas = None):\n",
        "        '''\n",
        "        Initializes the LeastSquaresSequenceSolver.\n",
        "        Args:\n",
        "            'approximator' (str): The approximator to use for inverting the matrix.\n",
        "            'seq_len' (int): The sequence length.\n",
        "            'apx_steps' (int): The number of steps for the approximator.\n",
        "            'apx_norm' (float): The norm scalar for the matrix approximation.\n",
        "            'lamb' (float): The lambda parameter for the least squares solver.\n",
        "        '''\n",
        "        if not approximator == None:\n",
        "            if approximator not in ['neumann', 'newton', 'chebyshev', 'richardson', 'None']:\n",
        "                raise ValueError(f\"Approximator {approximator} not supported\")\n",
        "\n",
        "        if approximator == 'neumann':\n",
        "            self.inverter = lambda A : invert_matrix_neumann(A, apx_steps, apx_norm)\n",
        "        elif approximator == 'newton':\n",
        "            self.inverter = lambda A : invert_matrix_newton(A, apx_steps)\n",
        "        elif approximator == 'richardson':\n",
        "            self.inverter = lambda A : invert_matrix_richardson(A, omegas=alphas)\n",
        "        elif approximator == 'chebyshev':\n",
        "            self.inverter = lambda A : invert_matrix_chebyshev(A, apx_steps, alphas=alphas, betas=betas)\n",
        "        else:\n",
        "            self.inverter = jnp.linalg.inv\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.apx_steps = apx_steps\n",
        "        self.apx_norm = apx_norm\n",
        "        self.lamb = lamb\n",
        "        self.use_mlp = use_mlp\n",
        "        self.mlp_fn = mlp_fn\n",
        "\n",
        "    def predict(self,\n",
        "                shifted_data: jnp.ndarray,\n",
        "                data: jnp.ndarray) -> jnp.ndarray:\n",
        "        '''\n",
        "        Function to get predictions for a batch of data.\n",
        "        Args:\n",
        "            'shifted_data' (jnp.ndarray): The shifted data.\n",
        "            'data' (jnp.ndarray): The original data.\n",
        "        Returns:\n",
        "            jnp.ndarray: The predictions.\n",
        "        '''\n",
        "        return self.all_preds(seq_data=shifted_data,\n",
        "                              seq_labels=data,\n",
        "                              seq_len=self.seq_len,\n",
        "                              lamb=self.lamb)\n",
        "\n",
        "    def least_squares_one_iter(self,\n",
        "                               seq_data: jnp.ndarray,\n",
        "                               seq_labels: jnp.ndarray,\n",
        "                               lamb: float) -> jnp.ndarray:\n",
        "        '''Function to perform one iteration of the least squares solver.'''\n",
        "        return self.inverter(seq_data.T@seq_data + lamb*jnp.eye(seq_data.shape[1])) @ (seq_data.T@seq_labels)\n",
        "\n",
        "    def least_squares_seq_pred_single_seq(self,\n",
        "                               seq_data: jnp.ndarray,\n",
        "                               seq_labels: jnp.ndarray,\n",
        "                               seq_len: int,\n",
        "                               lamb: float) -> jnp.ndarray:\n",
        "        '''\n",
        "        Function to get predictions for a single sequence using the least squares solver.\n",
        "        Args:\n",
        "            'seq_data' (jnp.ndarray): The sequence data.\n",
        "            'seq_labels' (jnp.ndarray): The sequence labels.\n",
        "            'seq_len' (int): The sequence length.\n",
        "            'lamb' (float): The lambda parameter for the least squares solver.\n",
        "        Returns:\n",
        "            jnp.ndarray: The predictions.\n",
        "        '''\n",
        "        preds = []\n",
        "        for token in range(seq_len):\n",
        "            w_hat = self.least_squares_one_iter(seq_data[:token], seq_labels[:token], lamb=lamb)\n",
        "            if self.use_mlp:\n",
        "                test_token = self.mlp_fn(seq_labels[token])\n",
        "                test_token = test_token/(jnp.linalg.norm(test_token)+1e-16)\n",
        "            else:\n",
        "                test_token =seq_labels[token]\n",
        "            seq_label_hat = jnp.matmul(test_token, w_hat)\n",
        "            preds.append(seq_label_hat)\n",
        "        return jnp.array(preds)\n",
        "\n",
        "    def all_preds(self,\n",
        "                  seq_data: jnp.ndarray,\n",
        "                  seq_labels: jnp.ndarray,\n",
        "                  seq_len: int,\n",
        "                  lamb: float) -> jnp.ndarray:\n",
        "        '''Function to get predictions for all sequences using the least squares solver.'''\n",
        "        return jax.vmap(self.least_squares_seq_pred_single_seq, in_axes=(0,0,None,None))(seq_data, seq_labels, seq_len, lamb)\n",
        "\n",
        "    def get_features(self, batch: jnp.ndarray) -> jnp.ndarray:\n",
        "        feature_seq_func = lambda seq : jax.vmap(self.mlp_fn)(seq)\n",
        "        feature_batch = jax.vmap(feature_seq_func, in_axes=(0))(batch)\n",
        "        return feature_batch\n",
        "\n",
        "    def opt_lamb(self,\n",
        "                 minv: float,\n",
        "                 maxv: float,\n",
        "                 steps: int,\n",
        "                 data_generator: DataGenerator,\n",
        "                 loss_fn: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
        "                 part_obs_constr: bool,\n",
        "                 embed_dim: int = 80,\n",
        "                 constr: bool = False,\n",
        "                 slots: bool = 4) -> jnp.ndarray:\n",
        "        '''\n",
        "        Function to optimize lambda parameter for least squares solver via line-search.\n",
        "        Sets the lambda parameter to the value that minimizes the loss function and returns it\n",
        "        Args:\n",
        "            'minv' (float): The minimum value for the lambda parameter.\n",
        "            'maxv' (float): The maximum value for the lambda parameter.\n",
        "            'steps' (int): The number of steps for the line-search.\n",
        "            'data_generator' (DataGenerator): The data generator.\n",
        "            'loss_fn' (Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]): The loss function.\n",
        "            'part_obs_constr' (bool): Use part.-obs. construction (concat. past k tokens)\n",
        "            'embed_dim' (int): Construction size for partobs\n",
        "            'constr' (bool): Use Full-Obs. construction,\n",
        "            'slots' (int): Token-'slots' in Full-Obs. construction\n",
        "        Returns:\n",
        "            jnp.ndarray: The optimized lambda parameter.\n",
        "        '''\n",
        "        min_score = float('inf')\n",
        "        range_vals = jnp.linspace(minv, maxv, steps)\n",
        "        rng = jax.random.PRNGKey(42)\n",
        "        rng, test_rng = jax.random.split(rng)\n",
        "\n",
        "        for lam in range_vals:\n",
        "            (data, targets), _ = data_generator.get_data(rng=test_rng, batch_size=512)\n",
        "\n",
        "            if part_obs_constr:\n",
        "                batch_size, seq_len, obs_dim = data.shape\n",
        "                constructed_data = jnp.zeros(shape=(batch_size, seq_len, embed_dim))\n",
        "                constructed_data = constructed_data.at[:,:,0:obs_dim].set(data)\n",
        "                for k in range(1, embed_dim//obs_dim):\n",
        "                    shifted_data = jnp.concatenate((jnp.zeros(shape=(batch_size,(k),obs_dim)),data[:,:-1*(k),:]),axis=1)\n",
        "                    constructed_data = constructed_data.at[:,:,k*obs_dim:(k+1)*obs_dim].set(shifted_data)\n",
        "                shifted_data = jnp.concatenate([jnp.expand_dims(constructed_data[:, 0, :], 1)*0, constructed_data], axis=1)[:, :-1, :]\n",
        "                data = constructed_data\n",
        "                preds_lsq = self.all_preds(seq_data=shifted_data,\n",
        "                                           seq_labels=data,\n",
        "                                           seq_len=self.seq_len,\n",
        "                                           lamb=lam)[:,:,0:obs_dim]\n",
        "            elif self.use_mlp:\n",
        "                dat_feat = self.get_features(data)\n",
        "                dat_feat /= (jnp.linalg.norm(dat_feat,axis=-1)[...,None])\n",
        "                shifted_data = jnp.concatenate([jnp.expand_dims(dat_feat[:, 0, :], 1)*0, dat_feat], axis=1)[:, :-1, :]\n",
        "                preds_lsq = self.all_preds(seq_data=shifted_data,\n",
        "                                           seq_labels=data/(jnp.linalg.norm(data, axis=-1)[...,None] + 1e-16),\n",
        "                                           seq_len=self.seq_len,\n",
        "                                           lamb=lam)\n",
        "            else:\n",
        "                if constr:\n",
        "                    shifted_data = data[:,:,(slots-1)*targets.shape[-1]:]\n",
        "                    data= data[:,:,(slots-2)*targets.shape[-1]:(slots-1)*targets.shape[-1]]\n",
        "                else:\n",
        "                    shifted_data = jnp.concatenate([jnp.expand_dims(data[:, 0, :], 1)*0, data], axis=1)[:, :-1, :]\n",
        "\n",
        "                preds_lsq = self.all_preds(seq_data=shifted_data,\n",
        "                                           seq_labels=data,\n",
        "                                           seq_len=self.seq_len,\n",
        "                                           lamb=lam)\n",
        "\n",
        "            score = loss_fn(preds_lsq, targets)\n",
        "            print(f\"for lambda = {lam:.6f} lsq-loss: \", score)\n",
        "            if score < min_score:\n",
        "                min_score = score\n",
        "                self.lamb = lam\n",
        "        return self.lamb\n",
        "\n",
        "class GDSequenceSolver:\n",
        "    '''Class implementing a gradient descent solver for sequence data.'''\n",
        "    def __init__(self,\n",
        "                 eta: float,\n",
        "                 lamb: float = 0,\n",
        "                 seq_len: int = 50):\n",
        "        '''\n",
        "        Initializes the GDSequenceSolver.\n",
        "        Args:\n",
        "            'eta' (float): The learning rate.\n",
        "            'lamb' (float): The (optional) lambda parameter.\n",
        "        '''\n",
        "        self.eta = eta\n",
        "        self.lamb = lamb\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def predict(self,\n",
        "                shifted_data: jnp.ndarray,\n",
        "                data: jnp.ndarray) -> jnp.ndarray:\n",
        "        '''\n",
        "        Function to get predictions for a batch of data.\n",
        "        Args:\n",
        "            'shifted_data' (jnp.ndarray): The shifted data.\n",
        "            'data' (jnp.ndarray): The original data.\n",
        "        Returns:\n",
        "            jnp.ndarray: The predictions.\n",
        "        '''\n",
        "        return self.all_preds(seq=data,\n",
        "                              seq_shifted=shifted_data,\n",
        "                              eta=self.eta)\n",
        "\n",
        "    def gd_delta(self,\n",
        "           seq: jnp.ndarray,\n",
        "           seq_shifted: jnp.ndarray,\n",
        "           idx: int) -> jnp.ndarray:\n",
        "        '''Function to get the delta for the gradient descent solver.'''\n",
        "        outer_productsGD = jnp.matmul(seq_shifted[:, :, None], seq[:, None, :])\n",
        "        resultGD = jnp.cumsum(outer_productsGD, axis=0)\n",
        "        return resultGD[idx]\n",
        "\n",
        "    def one_step_gd(self,\n",
        "                    seq: jnp.ndarray,\n",
        "                    seq_shifted: jnp.ndarray,\n",
        "                    eta: float,\n",
        "                    lamb: float,\n",
        "                    seq_len: int) -> jnp.ndarray:\n",
        "        '''\n",
        "        Function to perform one step of the gradient descent solver.\n",
        "        Args:\n",
        "            'seq' (jnp.ndarray): The sequence data.\n",
        "            'seq_shifted' (jnp.ndarray): The shifted sequence data.\n",
        "            'eta' (float): The learning rate.\n",
        "            'lamb' (float): The lambda parameter.\n",
        "            'seq_len' (int): The sequence length of the test sequences.\n",
        "        Returns:\n",
        "            jnp.ndarray: The gradient descent updates.\n",
        "        '''\n",
        "        result = []\n",
        "        for idx in range(seq_len):\n",
        "            deltaWi = self.gd_delta(seq=seq,\n",
        "                                    seq_shifted=seq_shifted,\n",
        "                                    idx=idx)\n",
        "            deltaWi += lamb*deltaWi\n",
        "            gd_update = eta * (seq[idx] @ deltaWi)\n",
        "            result.append(gd_update)\n",
        "        return jnp.array(result)\n",
        "\n",
        "    def all_preds(self, seq: jnp.ndarray, seq_shifted: jnp.ndarray, eta: float) -> jnp.ndarray:\n",
        "        '''Function to get predictions for all sequences in a batch using the gradient descent solver.'''\n",
        "        return jax.vmap(self.one_step_gd, in_axes=(0,0,None,None,None))(seq, seq_shifted, eta, self.lamb, self.seq_len)\n",
        "\n",
        "    def opt_eta(self,\n",
        "                minv: float,\n",
        "                maxv: float,\n",
        "                steps: int,\n",
        "                data_generator: DataGenerator,\n",
        "                loss_fn: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
        "                constr: bool = False,\n",
        "                slots: bool = 4) -> jnp.ndarray:\n",
        "        '''\n",
        "        Function to optimize eta parameter for GD sequence solver via line-search.\n",
        "        Sets the eta parameter to the value that minimizes the loss function and returns it\n",
        "        Args:\n",
        "            'minv' (float): The minimum value for the lambda parameter.\n",
        "            'maxv' (float): The maximum value for the lambda parameter.\n",
        "            'steps' (int): The number of steps for the line-search.\n",
        "            'data_generator' (DataGenerator): The data generator.\n",
        "            'loss_fn' (Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]): The loss function.\n",
        "            'constr' (bool): Use Full-Obs. construction,\n",
        "            'slots' (int): Token-'slots' in Full-Obs. construction\n",
        "        Returns:\n",
        "            jnp.ndarray: The optimized lambda parameter.\n",
        "        '''\n",
        "        min_score = float('inf')\n",
        "        range_vals = jnp.linspace(minv, maxv, steps)\n",
        "        rng = jax.random.PRNGKey(42)\n",
        "        rng, test_rng = jax.random.split(rng)\n",
        "\n",
        "        for eta in range_vals:\n",
        "            (data, targets), _ = data_generator.get_data(rng=test_rng, batch_size=512)\n",
        "            if constr:\n",
        "                shifted_data = data[:,:,(slots-1)*targets.shape[-1]:]\n",
        "                data= data[:,:,(slots-2)*targets.shape[-1]:(slots-1)*targets.shape[-1]]\n",
        "            else:\n",
        "                shifted_data = jnp.concatenate([jnp.expand_dims(data[:, 0, :], 1)*0, data], axis=1)[:, :-1, :]\n",
        "\n",
        "            preds_gd = self.all_preds(seq=data,\n",
        "                                      seq_shifted=shifted_data,\n",
        "                                      eta=eta)\n",
        "\n",
        "            score = loss_fn(preds_gd, targets)\n",
        "            print(f\"for eta = {eta:.6f} lsq-loss: \", score)\n",
        "            if score < min_score:\n",
        "                min_score = score\n",
        "                self.eta = eta\n",
        "        return self.eta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_pGc-JgPgh0"
      },
      "source": [
        "## Sequence performance evaluator (old code, be careful)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "L9Z4B2G7N9e8"
      },
      "outputs": [],
      "source": [
        "class EvalModel(abc.ABC):\n",
        "    '''Abstract base class for evaluation models.'''\n",
        "    @abc.abstractmethod\n",
        "    def evaluate(self,\n",
        "                 data: jnp.ndarray,\n",
        "                 targets: jnp.ndarray,\n",
        "                 loss_fn: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]) -> jnp.ndarray:\n",
        "        pass\n",
        "\n",
        "class TFEvaluator(EvalModel):\n",
        "    '''Class for evaluating transformer models.'''\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 state: train_state.TrainState,\n",
        "                 constr: bool,\n",
        "                 slots: int):\n",
        "        '''\n",
        "        Initializes the TFEvaluator.\n",
        "        Args:\n",
        "            'model' (nn.Module): The model to evaluate.\n",
        "            'state' (train_state.TrainState): The state of the model.\n",
        "            'constr' (bool): Whether the model is using constructed data.\n",
        "            'slots' (int): The number of slots.\n",
        "        '''\n",
        "        self.model = model\n",
        "        self.state = state\n",
        "        self.constr = constr\n",
        "        self.slots = slots\n",
        "\n",
        "    def evaluate(self,\n",
        "                 data:jnp.ndarray,\n",
        "                 targets:jnp.ndarray,\n",
        "                 loss_fn: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]) -> jnp.ndarray:\n",
        "        '''\n",
        "        Evaluates the model.\n",
        "        Args:\n",
        "            'data' (jnp.ndarray): The input data.\n",
        "            'targets' (jnp.ndarray): The target data.\n",
        "            'loss_fn' (Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]): The loss function.\n",
        "        Returns:\n",
        "            'jnp.ndarray': The loss.\n",
        "        '''\n",
        "        logits, _ = self.model.apply({'params': self.state.params}, data)\n",
        "        preds = logits[:, :, :targets.shape[-1]]\n",
        "\n",
        "        print('preds: ', preds.shape)\n",
        "        print('data: ', data.shape)\n",
        "        print('targets: ', targets.shape)\n",
        "\n",
        "        return loss_fn(preds, targets)\n",
        "\n",
        "class AuxmodelEvaluator(EvalModel):\n",
        "    '''Class for evaluating auxiliary models.'''\n",
        "    def __init__(self,\n",
        "                 model: AuxModel,\n",
        "                 state: any,\n",
        "                 constr: bool,\n",
        "                 slots: int,\n",
        "                 part_obs: bool,\n",
        "                 use_mlp: bool):\n",
        "            '''\n",
        "            Initializes the AuxmodelEvaluator.\n",
        "            Args:\n",
        "                'model' (AuxModel): The model to evaluate.\n",
        "                'state' (int): Used for Part.-Obs. Construction (holds embed_dim) or mlp (holds mlp_function)\n",
        "                'constr' (bool): Whether the model is using constructed data.\n",
        "                'slots' (int): The number of slots.\n",
        "                'part_obs' (bool): Evaluate on constructed token of past partial observations\n",
        "                'use_mlp' (bool): Evaluate on mlp features\n",
        "\n",
        "            '''\n",
        "            self.model = model\n",
        "            self.state = state\n",
        "            self.constr = constr\n",
        "            self.slots = slots\n",
        "            self.part_obs = part_obs\n",
        "            self.use_mlp = use_mlp\n",
        "\n",
        "    def get_features(self, batch: jnp.ndarray) -> jnp.ndarray:\n",
        "        feature_seq_func = lambda seq : vmap(self.state)(seq)\n",
        "        feature_batch = vmap(feature_seq_func, in_axes=(0))(batch)\n",
        "        return feature_batch\n",
        "\n",
        "    def evaluate(self,\n",
        "                 data:jnp.ndarray,\n",
        "                 targets:jnp.ndarray,\n",
        "                 loss_fn: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]) -> jnp.ndarray:\n",
        "        '''\n",
        "        Evaluates the model.\n",
        "        Args:\n",
        "            'data' (jnp.ndarray): The input data.\n",
        "            'targets' (jnp.ndarray): The target data.\n",
        "            'loss_fn' (Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]): The loss function.\n",
        "        Returns:\n",
        "            'jnp.ndarray': The loss.\n",
        "        '''\n",
        "        if self.part_obs:\n",
        "            batch_size, seq_len, obs_dim = data.shape\n",
        "            embed_dim = self.state\n",
        "            constructed_data = jnp.zeros(shape=(batch_size, seq_len, embed_dim))\n",
        "            constructed_data = constructed_data.at[:,:,0:obs_dim].set(data)\n",
        "            for k in range(1, embed_dim // obs_dim):\n",
        "                shifted_data = jnp.concatenate((jnp.zeros(shape=(batch_size,(k),obs_dim)),data[:,:-1*(k),:]),axis=1)\n",
        "                constructed_data = constructed_data.at[:,:,k*obs_dim:(k+1)*obs_dim].set(shifted_data)\n",
        "            shifted_data = jnp.concatenate([jnp.expand_dims(constructed_data[:, 0, :], 1)*0, constructed_data], axis=1)[:, :-1, :]\n",
        "            data = constructed_data\n",
        "            preds = self.model.predict(shifted_data=shifted_data, data=data)[:,:,0:obs_dim]\n",
        "        elif self.use_mlp:\n",
        "            dat_feat = self.get_features(data)\n",
        "            dat_feat /= (jnp.linalg.norm(dat_feat,axis=-1)[...,None])\n",
        "            shifted_data = jnp.concatenate([jnp.expand_dims(dat_feat[:, 0, :], 1)*0, dat_feat], axis=1)[:, :-1, :]\n",
        "            preds = self.model.predict(shifted_data=shifted_data, data=data/(jnp.linalg.norm(data, axis=-1)[...,None]))\n",
        "        else:\n",
        "            if self.constr:\n",
        "                shifted_data = data[:,:,(self.slots-1)*targets.shape[-1]:]\n",
        "                data = data[:,:,(self.slots-2)*targets.shape[-1]:(self.slots-1)*targets.shape[-1]]\n",
        "            else:\n",
        "                shifted_data = jnp.concatenate([jnp.expand_dims(data[:, 0, :], 1)*0, data], axis=1)[:, :-1, :]\n",
        "            preds = self.model.predict(shifted_data=shifted_data, data=data)\n",
        "            print('lsqpreds: ', preds.shape)\n",
        "            print('lsqdata: ', data.shape)\n",
        "            print('lsqtargets: ', targets.shape)\n",
        "        return loss_fn(preds, targets)\n",
        "\n",
        "def get_evaluator(model_type:str, **kwargs) -> EvalModel:\n",
        "    '''Returns an evaluator based on the model type.'''\n",
        "    if model_type.lower() in ['transformer', 'mesa-transformer', 'fwp', 'dfwp', 'delta', 'deep-delta']:\n",
        "        return TFEvaluator(**kwargs)\n",
        "    elif model_type.lower() in ['lsq', 'gd', 'lsq_partobs', 'lsq_mlp']:\n",
        "        return AuxmodelEvaluator(part_obs=(model_type == 'lsq_partobs'), use_mlp=(model_type == 'lsq_mlp'), **kwargs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "class SequencePredictionEvaluator:\n",
        "    '''Class for evaluating sequence prediction models across test sequences, per token.'''\n",
        "    def __init__(self,\n",
        "                 data_generator: DataGenerator,\n",
        "                 test_batch_size: int,\n",
        "                 seeds: List[int],\n",
        "                 model_list: List[str],\n",
        "                 models: List[nn.Module],\n",
        "                 states: List[train_state.TrainState],\n",
        "                 loss_fn: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]):\n",
        "        '''\n",
        "        Initializes the SequencePredictionEvaluator.\n",
        "        Args:\n",
        "            'data_generator' (DataGenerator): The data generator.\n",
        "            'test_batch_size' (int): The batch size for testing.\n",
        "            'seeds' (List[int]): The seeds for testing.\n",
        "            'model_list' (List[str]): The list of model types.\n",
        "            'models' (List[nn.Module]): The models to evaluate.\n",
        "            'states' (List[any]): The states of the models.\n",
        "            'loss_fn' (Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]): The loss function for evaluation.\n",
        "        '''\n",
        "        self.data_generator = data_generator\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.seeds = seeds\n",
        "        self.model_list = model_list\n",
        "        self.models = models\n",
        "        self.states = states\n",
        "        self.loss_fn = loss_fn\n",
        "        self.constr = data_generator.get_data_info()['constr']\n",
        "        if self.constr:\n",
        "            self.slots = 2 if data_generator.get_data_info()['token_format'] == 'compact' else 4\n",
        "        else:\n",
        "            self.slots = 0\n",
        "        self.losses = [[] for _ in range(len(self.model_list))]\n",
        "\n",
        "        print(self.slots)\n",
        "\n",
        "    def run(self) -> Dict[str, any]:\n",
        "        '''Runs the evaluation experiment.'''\n",
        "        for seed in self.seeds:\n",
        "            test_rng = random.PRNGKey(seed)\n",
        "            (data, targets), _ = self.data_generator.get_data(rng=test_rng, batch_size=self.test_batch_size)\n",
        "            for modelname, idx in zip(self.model_list, jnp.arange(len(self.model_list))):\n",
        "                print(f'Evaluating model: {modelname}')\n",
        "                evaluator = get_evaluator(model_type=modelname, model=self.models[idx], state=self.states[idx], constr=self.constr, slots=self.slots)\n",
        "                result = evaluator.evaluate(data, targets, self.loss_fn)\n",
        "                self.losses[idx].append(result)\n",
        "\n",
        "        return {'losses': self.losses}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjYzaB7RwceT"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3mc-A9QaMwX"
      },
      "source": [
        "###  Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8HFBeD_saTOF"
      },
      "outputs": [],
      "source": [
        "class TrainingInitializer:\n",
        "    '''Class for initializing training components'''\n",
        "    def __init__(self,\n",
        "                 model_config: config_dict.ConfigDict,\n",
        "                 experiment_config: config_dict.ConfigDict):\n",
        "        '''\n",
        "        Initializes the training components\n",
        "        Args:\n",
        "            model_config: Configuration for the model\n",
        "            experiment_config: Configuration for the experiment\n",
        "        '''\n",
        "\n",
        "        self.model_config = model_config\n",
        "        self.experiment_config = experiment_config\n",
        "\n",
        "    def get_tf_model(self):\n",
        "        '''Returns the transformer model based on the configuration'''\n",
        "        initializer = get_fast_weight_init(\n",
        "            num_layers=self.model_config.num_layers + self.model_config.hybrid_first_block,\n",
        "            d_model=self.model_config.embed_dim,\n",
        "            key_size=self.model_config.key_size,\n",
        "            range_dfwp=self.model_config.range_dfwp,\n",
        "        )\n",
        "\n",
        "        return FullTransformerModel(\n",
        "            use_gla=self.model_config.use_gla,\n",
        "            is_discrete=self.model_config.is_discrete,\n",
        "            vocab_size=self.model_config.vocab_size,\n",
        "            pad_token_id=None if not self.experiment_config.data.ttype == 'regbench' else data_module.vocab.get_id(data_module.vocab.noop),\n",
        "            use_depth=self.model_config.use_depth,\n",
        "            use_emb=self.model_config.use_emb,\n",
        "            use_fwp=self.model_config.use_fwp,\n",
        "            range_dfwp=self.model_config.range_dfwp,\n",
        "            use_pe_kq=self.model_config.use_pe_kq,\n",
        "            use_pe_emb=self.model_config.use_pe_emb,\n",
        "            hybrid_first_block=self.model_config.hybrid_first_block,\n",
        "            pe_dim=self.model_config.pe_dim,\n",
        "            out_dim=self.model_config.out_dim,\n",
        "            initializer=initializer,\n",
        "            use_layernorm=self.model_config.use_layernorm,\n",
        "            use_bias=self.model_config.use_bias,\n",
        "            use_mlp=self.model_config.use_mlp,\n",
        "            masked=self.model_config.masked,\n",
        "            use_clip=self.model_config.use_clip,\n",
        "            clip_range=self.model_config.clip_range,\n",
        "            num_layers=self.model_config.num_layers,\n",
        "            num_heads=self.model_config.num_heads,\n",
        "            embed_dim=self.model_config.embed_dim,\n",
        "            key_size=self.model_config.key_size,\n",
        "            seq_len=self.experiment_config.data.seq_len,\n",
        "            dim_feedforward_MLP=self.model_config.dim_feedforward_MLP,\n",
        "            linear=self.model_config.linear,\n",
        "            use_schlagnorm=self.model_config.use_schlagnorm,\n",
        "            schlagnorm_targets=self.model_config.schlagnorm_targets\n",
        "        )\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        '''Returns the optimizer based on the configuration'''\n",
        "        return Optimizer(\n",
        "            grad_clip=self.experiment_config.optim.grad_clip,\n",
        "            peak_lr=self.experiment_config.optim.peak_lr,\n",
        "            use_schedule=self.experiment_config.optim.use_schedule,\n",
        "            warmup_steps=self.experiment_config.optim.warmup_steps,\n",
        "            max_iters=self.experiment_config.optim.max_iters,\n",
        "            init_value=self.experiment_config.optim.init_value,\n",
        "            end_value=self.experiment_config.optim.end_value,\n",
        "            weight_decay=self.experiment_config.optim.weight_decay\n",
        "        ).get_optimizer()\n",
        "\n",
        "    def get_data_generator(self, ttype:str):\n",
        "        '''Returns the data generator based on the configuration'''\n",
        "        if ttype == 'seq_lin':\n",
        "            return LinearSequenceDataGenerator(\n",
        "                seq_len=self.experiment_config.data.seq_len,\n",
        "                data_dim=self.experiment_config.data.data_dim,\n",
        "                obs_dim=self.experiment_config.data.obs_dim,\n",
        "                range=self.experiment_config.data.range,\n",
        "                noise=self.experiment_config.data.noise,\n",
        "                noise_obs=self.experiment_config.data.noise_obs,\n",
        "                data_clip=self.experiment_config.data.data_clip,\n",
        "                eye_obs=self.experiment_config.data.eye_obs\n",
        "            )\n",
        "        elif ttype == 'fixed':\n",
        "            raise NotImplementedError('Fixed-W Sequence Generation is not implemented yet.')\n",
        "        elif ttype == 'seq_nonlin':\n",
        "            raise NotImplementedError('Nonlinear Sequence Generation is not implemented yet.')\n",
        "        elif ttype == 'contracting':\n",
        "            raise NotImplementedError('Contracting Sequence Generation is not implemented yet.')\n",
        "        elif ttype == 'seq_lin_constr':\n",
        "            return ConstructedFullSeqGenerator(\n",
        "                data_generator=self.get_data_generator('seq_lin'),\n",
        "                embed_dim=self.experiment_config.data.embed_dim,\n",
        "                token_format=self.experiment_config.data.token_format,\n",
        "            )\n",
        "        elif ttype == 'seq_lin_constr_part':\n",
        "            return ConstructedPartObsGenerator(data_generator=self.get_data_generator('seq_lin'),\n",
        "                                                embed_dim=self.experiment_config.data.embed_dim)\n",
        "        elif ttype == 'regbench':\n",
        "            return DFADataGenerator(\n",
        "                data_module=data_module,\n",
        "                seq_len=data_module.input_seq_len,\n",
        "                data_dim=data_module.vocab_size,\n",
        "                eye_obs=True\n",
        "            )\n",
        "        elif ttype == 'wikitext':\n",
        "            return generator\n",
        "            # return WikiTextDataGenerator(\n",
        "            #     seq_len=self.experiment_config.data.seq_len,\n",
        "            #     data_dim=self.experiment_config.data.vocab_size,  # This will be our vocab size\n",
        "            #     eye_obs=True\n",
        "            # )\n",
        "        else:\n",
        "            raise ValueError('Data type not recognized')\n",
        "\n",
        "    def get_train_module(self, model, optimizer, data_generator):\n",
        "            '''Returns the training module based on the configuration'''\n",
        "            if self.experiment_config.data.ttype == 'wikitext':\n",
        "                return LanguageModelTraining(\n",
        "                    model=model,\n",
        "                    optimizer=optimizer,\n",
        "                    data_generator=data_generator,\n",
        "                    batch_size=self.experiment_config.data.batch_size,\n",
        "                    test_batch_size=self.experiment_config.data.test_batch_size,\n",
        "                    task_type=self.experiment_config.data.task_type\n",
        "                )\n",
        "            else:\n",
        "                return Training(\n",
        "                    model=model,\n",
        "                    optimizer=optimizer,\n",
        "                    data_generator=data_generator,\n",
        "                    batch_size=self.experiment_config.data.batch_size,\n",
        "                    test_batch_size=self.experiment_config.data.test_batch_size,\n",
        "                    task_type=self.experiment_config.data.task_type\n",
        "                )\n",
        "\n",
        "    def setup_components(self):\n",
        "        '''Returns the components'''\n",
        "        model = self.get_tf_model()\n",
        "        optimizer = self.get_optimizer()\n",
        "        data_generator = self.get_data_generator(self.experiment_config.data.ttype)\n",
        "        train_module = self.get_train_module(model, optimizer, data_generator)\n",
        "\n",
        "        return (model, optimizer, data_generator, train_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjg1GD2XPmLe"
      },
      "source": [
        "### Language Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IprIwI2lHcv"
      },
      "source": [
        "#### Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sglb-isJk1oe"
      },
      "source": [
        "Standard Delta Rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GRWn7Uq3_6yY",
        "outputId": "120585cb-332f-4140-a651-134c4122559d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set size: (1000, 511)\n",
            "test set size: (250, 511)\n",
            "Using discrete input embeddings with num_embeddings (vocab): 20 and features: 64\n",
            "Using discrete output projection onto feat_dim (vocab): 20\n",
            "TF!! embedded input shape:  (32, 511, 64)\n",
            "Number of Decoder-Layers:  3\n",
            "Using deep delta Rule T/F: False\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: False\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: False\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: False\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "------\n",
            "Model has 164,588 parameters\n",
            "Using discrete input embeddings with num_embeddings (vocab): 20 and features: 64\n",
            "Using discrete output projection onto feat_dim (vocab): 20\n",
            "TF!! embedded input shape:  (32, 511, 64)\n",
            "Number of Decoder-Layers:  3\n",
            "Using deep delta Rule T/F: False\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: False\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: False\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: False\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "------\n",
            "Using discrete input embeddings with num_embeddings (vocab): 20 and features: 64\n",
            "Using discrete output projection onto feat_dim (vocab): 20\n",
            "TF!! embedded input shape:  (32, 511, 64)\n",
            "Number of Decoder-Layers:  3\n",
            "Using deep delta Rule T/F: False\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: False\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: False\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: False\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "------\n",
            "Epoch 0: loss = 3.6387, accuracy = 0.1080\n",
            "Epoch 1: loss = 2.8828, accuracy = 0.2192\n",
            "Epoch 2: loss = 2.1952, accuracy = 0.4339\n",
            "Epoch 3: loss = 1.9787, accuracy = 0.4963\n",
            "Epoch 4: loss = 1.9830, accuracy = 0.4758\n",
            "Epoch 5: loss = 2.0088, accuracy = 0.4655\n",
            "Epoch 6: loss = 2.0607, accuracy = 0.4592\n",
            "Epoch 7: loss = 2.0900, accuracy = 0.4641\n",
            "Epoch 8: loss = 2.1244, accuracy = 0.4658\n",
            "Epoch 9: loss = 2.1607, accuracy = 0.4583\n",
            "Epoch 10: loss = 2.2232, accuracy = 0.4534\n",
            "Epoch 11: loss = 2.2500, accuracy = 0.4402\n",
            "Epoch 12: loss = 2.2798, accuracy = 0.4401\n",
            "Epoch 13: loss = 2.3181, accuracy = 0.4359\n",
            "Epoch 14: loss = 2.3663, accuracy = 0.4342\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-590578f0afb1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mstate_tf_sd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_module_sd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       state_tf_sd, train_rng, loss, test_acc = train_module_sd.train_epoch(\n\u001b[0m\u001b[1;32m     18\u001b[0m           \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_tf_sd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-fb33562ad843>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, epoch, rng, test_rng, state, num_batches_train)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mtr_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Only pass inputs and targets for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discrete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_cls)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "experiment_config = get_experiment_config(seeds=[42])\n",
        "model_config = get_model_config(use_depth=False, use_gla=False)\n",
        "training_initializer = TrainingInitializer(model_config=model_config, experiment_config=experiment_config)\n",
        "model_sd, optimizer, data_generator, train_module_sd = training_initializer.setup_components()\n",
        "\n",
        "losses_seed_sd = []\n",
        "accs_seed_sd = []\n",
        "pers_seed_sd = []\n",
        "for training_seed in range(1):\n",
        "  losses_run = []\n",
        "  accs = []\n",
        "  pers = []\n",
        "  rng = jax.random.PRNGKey(training_seed)\n",
        "  rng, test_rng, train_rng = jax.random.split(rng, 3)\n",
        "  state_tf_sd, rng = train_module_sd.get_init_state(rng)\n",
        "  for epoch_idx in range(30):\n",
        "      state_tf_sd, train_rng, loss, test_acc = train_module_sd.train_epoch(\n",
        "          epoch=epoch_idx,\n",
        "          state=state_tf_sd,\n",
        "          rng=train_rng,\n",
        "          test_rng=test_rng,\n",
        "          num_batches_train=300\n",
        "      )\n",
        "      losses_run.append(loss)\n",
        "      accs.append(test_acc)\n",
        "      #pers.append(test_per)\n",
        "  losses_seed_sd.append(losses_run)\n",
        "  accs_seed_sd.append(accs)\n",
        "  #pers_seed_sd.append(pers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rcz2b9SPwxSk"
      },
      "outputs": [],
      "source": [
        "a = {'params_sd' : state_tf_sd.params, 'accs_sd' : accs_seed_sd, 'pers_sd': pers_seed_sd}\n",
        "with open('run_sd_singleseede256l4h8ex1000ep30.pkl', 'wb') as handle:\n",
        "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_117FIek5ur"
      },
      "source": [
        "Deep Delta Rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6DAF4g0GFM-4",
        "outputId": "54376142-5dde-465c-c82f-0c5212294255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set size: (1000, 511)\n",
            "test set size: (250, 511)\n",
            "Using discrete input embeddings with num_embeddings (vocab): 20 and features: 64\n",
            "Using discrete output projection onto feat_dim (vocab): 20\n",
            "TF!! embedded input shape:  (32, 511, 64)\n",
            "Number of Decoder-Layers:  3\n",
            "Using deep delta Rule T/F: True\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: True\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: True\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: True\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "------\n",
            "Model has 164,588 parameters\n",
            "Using discrete input embeddings with num_embeddings (vocab): 20 and features: 64\n",
            "Using discrete output projection onto feat_dim (vocab): 20\n",
            "TF!! embedded input shape:  (32, 511, 64)\n",
            "Number of Decoder-Layers:  3\n",
            "Using deep delta Rule T/F: True\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: True\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: True\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: True\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "------\n",
            "Using discrete input embeddings with num_embeddings (vocab): 20 and features: 64\n",
            "Using discrete output projection onto feat_dim (vocab): 20\n",
            "TF!! embedded input shape:  (32, 511, 64)\n",
            "Number of Decoder-Layers:  3\n",
            "Using deep delta Rule T/F: True\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: True\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: True\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "Transformer Block:\n",
            "Using Delta Rule Attention\n",
            "Leveraging depth T/F: True\n",
            "Using MLP\n",
            "Using LayerNorm\n",
            "------\n",
            "Epoch 0: loss = 3.6388, accuracy = 0.1099\n",
            "Epoch 1: loss = 2.8717, accuracy = 0.2212\n",
            "Epoch 2: loss = 2.0707, accuracy = 0.4731\n",
            "Epoch 3: loss = 1.8980, accuracy = 0.5073\n",
            "Epoch 4: loss = 1.8943, accuracy = 0.5046\n",
            "Epoch 5: loss = 1.9417, accuracy = 0.4621\n",
            "Epoch 6: loss = 1.9694, accuracy = 0.4726\n",
            "Epoch 7: loss = 2.0077, accuracy = 0.4562\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-6b53b2a515b6>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mstate_tf_dd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_module_dd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       state_tf_dd, train_rng, loss, test_acc = train_module_dd.train_epoch(\n\u001b[0m\u001b[1;32m     18\u001b[0m           \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_tf_dd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-fb33562ad843>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, epoch, rng, test_rng, state, num_batches_train)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mtr_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Only pass inputs and targets for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-34c505d67f9a>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, rng, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;34m\"\"\"Gets a batch of training data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mbatch_inps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dfas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quick_data_loader_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_inps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dfas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-34c505d67f9a>\u001b[0m in \u001b[0;36m_quick_data_loader_train\u001b[0;34m(self, batch_size, rng)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mbatch_inps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_inps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbatch_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mbatch_dfas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dfas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_inps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dfas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-34c505d67f9a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mbatch_inps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_inps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbatch_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mbatch_dfas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dfas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_inps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dfas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/array.py\u001b[0m in \u001b[0;36m__index__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__index__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_integer_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/array.py\u001b[0m in \u001b[0;36m_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_replicated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_device_array_to_np_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_npy_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "experiment_config = get_experiment_config(seeds=[42])\n",
        "model_config = get_model_config(use_depth=True, use_gla=False)\n",
        "training_initializer = TrainingInitializer(model_config=model_config, experiment_config=experiment_config)\n",
        "model_dd, optimizer, data_generator, train_module_dd = training_initializer.setup_components()\n",
        "\n",
        "losses_seed_dd = []\n",
        "accs_seed_dd = []\n",
        "pers_seed_dd = []\n",
        "for training_seed in range(1):\n",
        "  losses_run = []\n",
        "  accs = []\n",
        "  pers = []\n",
        "  rng = jax.random.PRNGKey(training_seed)\n",
        "  rng, test_rng, train_rng = jax.random.split(rng, 3)\n",
        "  state_tf_dd, rng = train_module_dd.get_init_state(rng)\n",
        "  for epoch_idx in range(30):\n",
        "      state_tf_dd, train_rng, loss, test_acc = train_module_dd.train_epoch(\n",
        "          epoch=epoch_idx,\n",
        "          state=state_tf_dd,\n",
        "          rng=train_rng,\n",
        "          test_rng=test_rng,\n",
        "          num_batches_train=300\n",
        "      )\n",
        "      losses_run.append(loss)\n",
        "      accs.append(test_acc)\n",
        "      #pers.append(test_per)\n",
        "  losses_seed_dd.append(losses_run)\n",
        "  accs_seed_dd.append(accs)\n",
        "#  pers_seed_dd.append(pers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYmpfdENw-OM"
      },
      "outputs": [],
      "source": [
        "b = {'params_dd' : state_tf_dd.params, 'accs_dd' : accs_seed_dd, 'pers_dd': pers_seed_dd}\n",
        "with open('run_dd_singleseede256l4h8ex1000ep30.pkl', 'wb') as handle:\n",
        "    pickle.dump(b, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qry7erwbk8So"
      },
      "source": [
        "Fast Weight Programmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OahCF-hBQ-lH"
      },
      "outputs": [],
      "source": [
        "experiment_config = get_experiment_config(seeds=[42])\n",
        "model_config = get_model_config(use_depth=False, use_gla=False)\n",
        "model_config.use_fwp = False\n",
        "training_initializer = TrainingInitializer(model_config=model_config, experiment_config=experiment_config)\n",
        "model_fwp, optimizer, data_generator, train_module_fwp = training_initializer.setup_components()\n",
        "\n",
        "losses_seed_fwp = []\n",
        "accs_seed_fwp = []\n",
        "pers_seed_fwp = []\n",
        "for training_seed in range(3):\n",
        "  losses_run = []\n",
        "  accs = []\n",
        "  pers = []\n",
        "  rng = jax.random.PRNGKey(training_seed)\n",
        "  rng, test_rng, train_rng = jax.random.split(rng, 3)\n",
        "  state_tf_fwp, rng = train_module_fwp.get_init_state(rng)\n",
        "  for epoch_idx in range(70):\n",
        "      state_tf_fwp, train_rng, loss, test_acc = train_module_fwp.train_epoch(\n",
        "          epoch=epoch_idx,\n",
        "          state=state_tf_fwp,\n",
        "          rng=train_rng,\n",
        "          test_rng=test_rng,\n",
        "          num_batches_train=300\n",
        "      )\n",
        "      losses_run.append(loss)\n",
        "      accs.append(test_acc)\n",
        "      # pers.append(test_per)\n",
        "  losses_seed_fwp.append(losses_run)\n",
        "  accs_seed_fwp.append(accs)\n",
        "  #pers_seed_fwp.append(pers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpLy72-zxAVN"
      },
      "outputs": [],
      "source": [
        "a = {'params_fwp' : state_tf_fwp.params, 'accs_fwp' : accs_seed_fwp, 'pers_fwp' : pers_seed_fwp}\n",
        "with open('run_fwp_singleseede256l4h8.pkl', 'wb') as handle:\n",
        "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxjSr0LslAmF"
      },
      "source": [
        "Gated Linear Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "racLHzaYlC1L"
      },
      "outputs": [],
      "source": [
        "experiment_config = get_experiment_config(seeds=[42])\n",
        "model_config = get_model_config(use_depth=False, use_gla=True)\n",
        "model_config.use_fwp = True\n",
        "training_initializer = TrainingInitializer(model_config=model_config, experiment_config=experiment_config)\n",
        "model_gla, optimizer, data_generator, train_module_gla = training_initializer.setup_components()\n",
        "\n",
        "losses_seed_gla = []\n",
        "accs_seed_gla = []\n",
        "pers_seed_gla = []\n",
        "for training_seed in range(3):\n",
        "  losses_run = []\n",
        "  accs = []\n",
        "  pers = []\n",
        "  rng = jax.random.PRNGKey(training_seed)\n",
        "  rng, test_rng, train_rng = jax.random.split(rng, 3)\n",
        "  state_tf_gla, rng = train_module_gla.get_init_state(rng)\n",
        "  for epoch_idx in range(100):\n",
        "      state_tf_gla, train_rng, loss, test_acc = train_module_gla.train_epoch(\n",
        "          epoch=epoch_idx,\n",
        "          state=state_tf_gla,\n",
        "          rng=train_rng,\n",
        "          test_rng=test_rng,\n",
        "          num_batches_train=300\n",
        "      )\n",
        "      losses_run.append(loss)\n",
        "      accs.append(test_acc)\n",
        "      #pers.append(test_per)\n",
        "  losses_seed_gla.append(losses_run)\n",
        "  accs_seed_gla.append(accs)\n",
        "  #pers_seed_gla.append(pers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsqYo1-JDygc"
      },
      "outputs": [],
      "source": [
        "a = {'params_gla' : state_tf_gla.params, 'accs_gla' : accs_seed_gla, 'pers_gla' : pers_seed_gla}\n",
        "with open('run_gla_singleseede256l4h8.pkl', 'wb') as handle:\n",
        "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AeioUkOlD0R"
      },
      "source": [
        "#### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK0B2c9ZRBgc"
      },
      "outputs": [],
      "source": [
        "colors = ['#2978A0',   # Blue\n",
        "         '#BE7A3C',    # Orange\n",
        "         '#4E917A',    # Green\n",
        "         '#A94964',    # Red\n",
        "         '#7764D8',    # Purple\n",
        "         '#4D8B31',    # Dark green\n",
        "         '#C35DCF',    # Pink\n",
        "         '#816C5B',    # Brown\n",
        "         '#3778BF',    # Light blue\n",
        "         '#E05263']    # Coral\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for seed in range(3):\n",
        "  plt.plot(accs_seed_dd[seed], label=('deep-delta' if seed == 0 else None), color=colors[0])\n",
        "  plt.plot(accs_seed_sd[seed], label=('delta' if seed == 0 else None), color=colors[1])\n",
        "  plt.plot(accs_seed_fwp[seed], label=('fwp/lin-att.' if seed == 0 else None), color=colors[2])\n",
        "plt.ylim(0.0,1)\n",
        "\n",
        "# Customize appearance\n",
        "plt.tick_params(axis='x', colors='dimgray')\n",
        "plt.tick_params(axis='y', colors='dimgray')\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['bottom'].set_color('dimgray')\n",
        "plt.gca().spines['left'].set_color('dimgray')\n",
        "\n",
        "# Set labels\n",
        "plt.xlabel('Training steps (in $100$)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right', fontsize='large')\n",
        "plt.title('RegBench: Test Accuracy over training')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VCeIt5maN3o"
      },
      "source": [
        "### Synthetic Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_DGUDzGRR-u"
      },
      "source": [
        "#### Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jPBjhBYVPgL"
      },
      "source": [
        "Standard Delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-LbS_6qQF8g"
      },
      "outputs": [],
      "source": [
        "experiment_config = get_experiment_config(seeds=[42])\n",
        "model_config = get_model_config(use_depth=False)\n",
        "training_initializer = TrainingInitializer(model_config=model_config, experiment_config=experiment_config)\n",
        "model_sd, optimizer, data_generator, train_module_sd = training_initializer.setup_components()\n",
        "\n",
        "losses_seed_sd = []\n",
        "for training_seed in range(3):\n",
        "  losses_run = []\n",
        "  rng = jax.random.PRNGKey(training_seed)\n",
        "  rng, test_rng, train_rng = jax.random.split(rng, 3)\n",
        "  state_tf_sd, rng = train_module_sd.get_init_state(rng)\n",
        "  for epoch_idx in range(100):\n",
        "      state_tf_sd, train_rng, loss = train_module_sd.train_epoch(\n",
        "          epoch=epoch_idx,\n",
        "          state=state_tf_sd,\n",
        "          rng=train_rng,\n",
        "          test_rng=test_rng,\n",
        "          num_batches_train=100\n",
        "      )\n",
        "      losses_run.append(loss)\n",
        "  losses_seed_sd.append(losses_run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55k_Wl_MDGX8"
      },
      "source": [
        "Deep Delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_Nm18Qg_h7UU"
      },
      "outputs": [],
      "source": [
        "experiment_config = get_experiment_config(seeds=[42])\n",
        "model_config = get_model_config(use_depth=True)\n",
        "training_initializer = TrainingInitializer(model_config=model_config, experiment_config=experiment_config)\n",
        "model_dd, optimizer, data_generator, train_module_dd = training_initializer.setup_components()\n",
        "\n",
        "losses_seed_dd = []\n",
        "for training_seed in range(3):\n",
        "  losses_run = []\n",
        "  rng = jax.random.PRNGKey(training_seed)\n",
        "  rng, test_rng, train_rng = jax.random.split(rng, 3)\n",
        "  state_tf_dd, rng = train_module_dd.get_init_state(rng)\n",
        "  for epoch_idx in range(100):\n",
        "      state_tf_dd, train_rng, loss = train_module_dd.train_epoch(\n",
        "          epoch=epoch_idx,\n",
        "          state=state_tf_dd,\n",
        "          rng=train_rng,\n",
        "          test_rng=test_rng,\n",
        "          num_batches_train=100\n",
        "      )\n",
        "      losses_run.append(loss)\n",
        "  losses_seed_dd.append(losses_run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpOJUiahDKiq"
      },
      "source": [
        "FWP (Lin.Att.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdrxTiDfDNWM"
      },
      "outputs": [],
      "source": [
        "experiment_config = get_experiment_config(seeds=[42])\n",
        "model_config = get_model_config(use_depth=False)\n",
        "model_config.use_fwp = False\n",
        "training_initializer = TrainingInitializer(model_config=model_config, experiment_config=experiment_config)\n",
        "model_fwp, optimizer, data_generator, train_module_fwp = training_initializer.setup_components()\n",
        "\n",
        "losses_seed_fwp = []\n",
        "for training_seed in range(3):\n",
        "  losses_run = []\n",
        "  rng = jax.random.PRNGKey(training_seed)\n",
        "  rng, test_rng, train_rng = jax.random.split(rng, 3)\n",
        "  state_tf_fwp, rng = train_module_fwp.get_init_state(rng)\n",
        "  for epoch_idx in range(100):\n",
        "      state_tf_fwp, train_rng, loss = train_module_fwp.train_epoch(\n",
        "          epoch=epoch_idx,\n",
        "          state=state_tf_fwp,\n",
        "          rng=train_rng,\n",
        "          test_rng=test_rng,\n",
        "          num_batches_train=100\n",
        "      )\n",
        "      losses_run.append(loss)\n",
        "  losses_seed_fwp.append(losses_run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HULNGUKyVNcV"
      },
      "source": [
        "Training DFWP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg4Z3-W2VJmM"
      },
      "source": [
        "Per-Token performance eval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yGJUD-yrNe_8"
      },
      "outputs": [],
      "source": [
        "lsq_solver = LeastSquaresSequenceSolver(approximator='None',\n",
        "                                        seq_len=50,\n",
        "                                        apx_steps=20,\n",
        "                                        lamb=0.001,)\n",
        "model_names = ['delta','deep-delta','fwp','lsq']\n",
        "loss_fn = lambda p, t : list((jax.numpy.sum(((p - t)**2), axis=(0,2))/(2*p.shape[0])))\n",
        "seq_evaluator = SequencePredictionEvaluator(data_generator=data_generator,\n",
        "                                                  test_batch_size=256,\n",
        "                                                  seeds=[1,2,3,4,5],\n",
        "                                                  model_list = model_names,\n",
        "                                                  models = [model_sd, model_dd, model_fwp, lsq_solver],\n",
        "                                                  states = [state_tf_sd, state_tf_dd, state_tf_fwp, None],\n",
        "                                                  loss_fn = loss_fn)\n",
        "\n",
        "seq_loss_dict = seq_evaluator.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF8veZ5iSW0J"
      },
      "source": [
        "#### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_yfJztIHx-S"
      },
      "outputs": [],
      "source": [
        "plt.plot(jnp.mean(jnp.array(losses_seed_dd),axis=0), label='deep-delta')\n",
        "plt.plot(jnp.mean(jnp.array(losses_seed_sd), axis=0), label='delta')\n",
        "plt.plot(jnp.mean(jnp.array(losses_seed_fwp), axis=0), label='fwp/lin-att.')\n",
        "plt.ylim(0.3,2)\n",
        "plt.title('Synth. mesa task: Test loss over training')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVvrjFf0S2Ph"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Define a nice color palette\n",
        "colors = ['#2978A0',   # Blue\n",
        "         '#BE7A3C',    # Orange\n",
        "         '#4E917A',    # Green\n",
        "         '#A94964',    # Red\n",
        "         '#7764D8',    # Purple\n",
        "         '#4D8B31',    # Dark green\n",
        "         '#C35DCF',    # Pink\n",
        "         '#816C5B',    # Brown\n",
        "         '#3778BF',    # Light blue\n",
        "         '#E05263']    # Coral\n",
        "\n",
        "# Calculate mean and std losses\n",
        "loss_list = seq_loss_dict['losses']\n",
        "loss_arr = jnp.array(loss_list)\n",
        "mean_losses = tuple([jnp.mean(loss_arr[idx], axis=0) for idx in range(len(model_names))])\n",
        "std_losses = tuple([jnp.std(loss_arr[idx], axis=0) for idx in range(len(model_names))])\n",
        "\n",
        "# Create figure\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot each model's losses\n",
        "for idx in range(len(model_names)):\n",
        "\n",
        "    plt.plot(mean_losses[idx],\n",
        "             linewidth=2,\n",
        "             label=model_names[idx],\n",
        "             color=colors[idx % len(colors)])  # Cycle through colors if more models than colors\n",
        "    plt.fill_between(range(len(mean_losses[0])),\n",
        "                    mean_losses[idx] - std_losses[idx],\n",
        "                    mean_losses[idx] + std_losses[idx],\n",
        "                    alpha=0.3,\n",
        "                    color=colors[idx % len(colors)])\n",
        "\n",
        "# Customize appearance\n",
        "plt.tick_params(axis='x', colors='dimgray')\n",
        "plt.tick_params(axis='y', colors='dimgray')\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['bottom'].set_color('dimgray')\n",
        "plt.gca().spines['left'].set_color('dimgray')\n",
        "\n",
        "# Set labels\n",
        "plt.xlabel('Sequence length $t$')\n",
        "plt.ylabel('Next-token prediction MSE')\n",
        "plt.legend(loc='upper right', fontsize='large')\n",
        "plt.title('Synth. mesa task: Loss over one batch of sequences')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "up_YEB0Qfrn6",
        "GHX5gZ9hfQm-",
        "bWcvh0HRgVki",
        "X5pTae8PN-YQ",
        "SveqfQdAgX_L",
        "KN_-ZmNlIIZA",
        "TfPWO775UYz6",
        "JM4tRUmnQ-aP",
        "JE_bAe0yRuLJ",
        "dW5rNfJswceQ",
        "cPJCeqhswceQ",
        "2hU89vyWwceS",
        "diyCyEI9wceS",
        "pvXUUiV77Aqd",
        "9-MntRwQ7EeY",
        "vPdCtzaMaemW",
        "VbSPMEIfYUKm",
        "m53OM6Z4ZGMX",
        "mG1UDIX4nfhC",
        "TWXpCsrJAibT",
        "iJU22L3XUV34",
        "QTZXXhkIcLH6",
        "xom6MG8dugoV",
        "rEHgAV2nkHfL",
        "q84uzbFiav_4",
        "0kqUMi4mSIgx",
        "ckkYkU2KN6lQ",
        "xYD11ZqwOH0E",
        "r_pGc-JgPgh0",
        "d3mc-A9QaMwX",
        "_AeioUkOlD0R",
        "8VCeIt5maN3o",
        "g_DGUDzGRR-u"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}